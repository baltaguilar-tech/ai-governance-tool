// =============================================================================
// ACHIEVER BANK — 60 questions
// For orgs with mature, board-level AI governance and external audit readiness.
// Focus: continuous improvement, regulatory excellence, program effectiveness.
// =============================================================================

import { AssessmentQuestion } from '@/types/assessment';

export const ACHIEVER_QUESTIONS: AssessmentQuestion[] = [
  // ---------------------------------------------------------------------------
  // SHADOW AI — shadow-a-1 to shadow-a-10
  // ---------------------------------------------------------------------------
  {
    id: 'shadow-a-1',
    dimension: 'shadowAI',
    text: 'How is your AI asset inventory integrated with enterprise CMDB and ITSM systems, and how is completeness validated?',
    helpText: 'ISO 42001 clause 8.4 and NIST AI RMF MAP 1.1 expect AI asset registers to be systematically maintained and linked to broader IT asset management to ensure coverage and accountability.',
    type: 'radio',
    options: [
      { label: 'No integration with CMDB/ITSM; AI inventory completeness unverified', value: 100, riskLevel: 'critical' },
      { label: 'AI inventory exists independently with no CMDB/ITSM integration; reconciliation is ad hoc', value: 75, riskLevel: 'high' },
      { label: 'Partial integration; AI inventory maintained separately with manual reconciliation to CMDB performed periodically', value: 50, riskLevel: 'high' },
      { label: 'Integrated with CMDB/ITSM; completeness validated by IT governance team with documented methodology', value: 25, riskLevel: 'medium' },
      { label: 'Fully integrated with CMDB/ITSM via automated connectors; completeness independently attested by internal audit or third party annually', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'shadow-a-2',
    dimension: 'shadowAI',
    text: 'What shadow AI metrics are reported to the board, and at what frequency?',
    helpText: 'Board-level reporting on shadow AI signals governance maturity and is expected by institutional investors applying ESG AI governance criteria and by auditors reviewing ISO 42001 compliance.',
    type: 'radio',
    options: [
      { label: 'No shadow AI metrics reported to board or senior leadership', value: 100, riskLevel: 'critical' },
      { label: 'Shadow AI metrics tracked internally but no structured board reporting cadence', value: 75, riskLevel: 'high' },
      { label: 'Shadow AI incidents and high-risk discoveries reported to board on exception basis only', value: 50, riskLevel: 'high' },
      { label: 'Key shadow AI metrics reported to board semi-annually; C-suite receives quarterly summary', value: 25, riskLevel: 'medium' },
      { label: 'Comprehensive shadow AI dashboard (detection rate, policy exceptions, remediation SLAs, trend lines) reported to board quarterly', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
  },
  {
    id: 'shadow-a-3',
    dimension: 'shadowAI',
    text: 'How does your organization enforce AI usage policies in real time, and what exception workflows govern approved deviations?',
    helpText: 'Automated policy enforcement with documented exception workflows is a control maturity indicator under NIST CSF and aligns with EU AI Act Article 9 risk management system requirements for high-risk AI.',
    type: 'radio',
    options: [
      { label: 'No automated enforcement; policy violations handled reactively', value: 100, riskLevel: 'critical' },
      { label: 'Policy exists but enforcement is manual and inconsistent; no formal exception workflow', value: 75, riskLevel: 'high' },
      { label: 'Enforcement relies on periodic monitoring and spot-checks; exceptions handled informally', value: 50, riskLevel: 'high' },
      { label: 'Policy enforcement automated for most channels; exception process defined and tracked with senior approval required', value: 25, riskLevel: 'medium' },
      { label: 'Automated technical controls enforce AI usage policies; all exceptions logged, risk-assessed, and approved via formal workflow with time-bound remediation', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
  },
  {
    id: 'shadow-a-4',
    dimension: 'shadowAI',
    text: 'What formal SLAs govern AI tool sunset and decommissioning, and how are they monitored?',
    helpText: 'Lifecycle management of AI tools — including end-of-life protocols — is addressed in ISO 42001 clause 8.6 and reduces residual risk from abandoned or unsupported AI deployments.',
    type: 'radio',
    options: [
      { label: 'No formal AI tool sunset or decommissioning process', value: 100, riskLevel: 'critical' },
      { label: 'Decommissioning handled on a case-by-case basis with no standardized lifecycle management', value: 75, riskLevel: 'high' },
      { label: 'Decommissioning process exists but SLAs not formally defined or tracked against metrics', value: 50, riskLevel: 'high' },
      { label: 'Decommissioning procedures documented with assigned owners; adherence reviewed by IT governance quarterly', value: 25, riskLevel: 'medium' },
      { label: 'Formal decommissioning SLAs defined, tracked in ITSM, and compliance reported to governance committee; data deletion and handoff verified', value: 0, riskLevel: 'low' },
      ],
    weight: 0.9,
  },
  {
    id: 'shadow-a-5',
    dimension: 'shadowAI',
    text: 'How has your organization independently verified the completeness and accuracy of your AI tool inventory?',
    helpText: 'Independent verification — whether through internal audit or external third-party attestation — strengthens audit defensibility. Third-party attestation is the gold standard for organizations with significant regulatory exposure and is increasingly expected by regulators and institutional investors conducting AI governance due diligence.',
    type: 'radio',
    options: [
      { label: 'No independent review of AI inventory completeness has been conducted', value: 100, riskLevel: 'critical' },
      { label: 'Inventory completeness reviewed informally; no structured internal or external assessment', value: 75, riskLevel: 'high' },
      { label: 'Internal audit has assessed inventory completeness but no external third-party attestation', value: 50, riskLevel: 'high' },
      { label: 'Third-party assessment completed; findings addressed; next attestation scheduled', value: 25, riskLevel: 'medium' },
      { label: 'Third-party attestation completed within the last 12 months with formal findings and management response', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'shadow-a-6',
    dimension: 'shadowAI',
    text: 'How do you maintain visibility into AI tool usage across subsidiaries, business units, and joint ventures?',
    helpText: 'Group-wide AI governance coverage is expected under EU AI Act Article 27 for deployers operating across legal entities, and failures at subsidiary level can create parent-company liability.',
    type: 'radio',
    options: [
      { label: 'No cross-entity AI visibility; each subsidiary manages independently', value: 100, riskLevel: 'critical' },
      { label: 'Group policy exists but subsidiaries operate independently with limited reporting to center', value: 75, riskLevel: 'high' },
      { label: 'Group policy applies to subsidiaries but monitoring is inconsistent; visibility dependent on local teams self-reporting', value: 50, riskLevel: 'high' },
      { label: 'Subsidiaries follow group AI governance standards with regular reporting to group function; gaps tracked and remediated', value: 25, riskLevel: 'medium' },
      { label: 'Centralized AI governance platform with subsidiary-level reporting; subsidiaries required to attest to inventory completeness; consolidated view presented to group board', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'shadow-a-7',
    dimension: 'shadowAI',
    text: 'What AI discovery and governance integration protocols are activated during mergers, acquisitions, or divestitures?',
    helpText: 'M&A activity can introduce unvetted AI systems into the governance perimeter; a formal AI due diligence protocol reduces inherited risk and supports post-close integration timelines.',
    type: 'radio',
    options: [
      { label: 'No AI consideration in M&A processes', value: 100, riskLevel: 'critical' },
      { label: 'AI systems identified post-close reactively; no formal M&A AI protocol', value: 75, riskLevel: 'high' },
      { label: 'AI discovery included in general IT due diligence but without a dedicated AI-specific protocol', value: 50, riskLevel: 'high' },
      { label: 'AI discovery conducted during M&A due diligence; integration plan includes AI governance but not always with formal milestones', value: 25, riskLevel: 'medium' },
      { label: 'Formal AI due diligence checklist in M&A playbook; AI risk findings presented to deal committee; integration plan includes AI governance milestones with defined timelines', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'shadow-a-8',
    dimension: 'shadowAI',
    text: 'How does AI usage analytics inform your governance strategy and policy updates?',
    helpText: 'Data-driven governance — using usage patterns to identify policy gaps and emerging risks — is a characteristic of Level 4-5 AI governance maturity under models such as CMMI for AI and the NIST AI RMF.',
    type: 'radio',
    options: [
      { label: 'No AI usage analytics informing governance strategy', value: 100, riskLevel: 'critical' },
      { label: 'Limited analytics capability; governance decisions rely primarily on stakeholder feedback rather than usage data', value: 75, riskLevel: 'high' },
      { label: 'Usage data collected but analysis is informal; policy updates driven by incidents rather than proactive trend analysis', value: 50, riskLevel: 'high' },
      { label: 'Usage data reviewed by governance team periodically; informs policy decisions on an ad hoc basis with some documented outcomes', value: 25, riskLevel: 'medium' },
      { label: 'AI usage analytics platform feeds structured insights into a governance review cycle; policy updates triggered by usage trend thresholds with documented rationale', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'shadow-a-9',
    dimension: 'shadowAI',
    text: 'How does your organization produce regulatory reports on its AI asset landscape when required by regulators or in response to supervisory requests?',
    helpText: 'Proactive readiness for regulatory reporting on AI assets is expected under the EU AI Act Article 49 and sector-specific AI guidance from bodies such as the EBA, FCA, and SEC.',
    type: 'radio',
    options: [
      { label: 'Not prepared to respond to regulatory requests for AI asset information', value: 100, riskLevel: 'critical' },
      { label: 'Regulatory reporting would require significant effort to compile; no defined process', value: 75, riskLevel: 'high' },
      { label: 'Data exists to support regulatory reporting but requires manual assembly; no pre-built templates', value: 50, riskLevel: 'high' },
      { label: 'Reporting capability exists with documented process; data availability confirmed; not yet tested end-to-end', value: 25, riskLevel: 'medium' },
      { label: 'Pre-built regulatory reporting templates with current data feeds; dry-run exercises conducted; response capability tested against defined SLAs', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'shadow-a-10',
    dimension: 'shadowAI',
    text: 'How do you monitor your organization\'s continuous AI compliance posture, and what triggers an out-of-cycle governance review?',
    helpText: 'Continuous compliance monitoring — rather than point-in-time assessments — is the standard expected by ISO 42001 and is increasingly required by financial sector AI supervisory guidance globally.',
    type: 'radio',
    options: [
      { label: 'No continuous compliance monitoring; posture assessed only during formal audit cycles', value: 100, riskLevel: 'critical' },
      { label: 'Compliance assessed periodically; no continuous monitoring capability', value: 75, riskLevel: 'high' },
      { label: 'Compliance monitoring conducted on scheduled cycle; out-of-cycle reviews triggered by incidents but not by drift detection', value: 50, riskLevel: 'high' },
      { label: 'Compliance posture monitored through regular automated scans; significant changes trigger a governance review through a defined escalation path', value: 25, riskLevel: 'medium' },
      { label: 'Real-time or near-real-time compliance posture dashboard with automated alerting; defined trigger conditions initiate out-of-cycle board or governance committee review', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },

  // ---------------------------------------------------------------------------
  // VENDOR RISK — vendor-a-1 to vendor-a-10
  // ---------------------------------------------------------------------------
  {
    id: 'vendor-a-1',
    dimension: 'vendorRisk',
    text: 'How is AI vendor concentration risk identified, quantified, and reported to the board?',
    helpText: 'Board-level visibility into AI vendor concentration risk is a governance maturity expectation for organizations operating at Achiever level. Regulatory frameworks such as DORA (financial sector, EU/UK) require formal concentration risk quantification; for non-financial organizations, the same discipline is increasingly expected by institutional investors and enterprise customers conducting AI governance due diligence.',
    type: 'radio',
    options: [
      { label: 'No formal AI vendor concentration risk assessment', value: 100, riskLevel: 'critical' },
      { label: 'Concentration risk acknowledged but not formally measured or reported at board level', value: 75, riskLevel: 'high' },
      { label: 'Concentration risk identified qualitatively; reported to risk committee without formal quantification or scenario analysis', value: 50, riskLevel: 'high' },
      { label: 'Concentration risk assessed and reported to board annually; senior leadership receives semi-annual update; mitigation strategies documented', value: 25, riskLevel: 'medium' },
      { label: 'Concentration risk formally quantified (revenue dependency, functional criticality, substitutability index) and reported to board quarterly with scenario analysis for top-concentration scenarios', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'vendor-a-2',
    dimension: 'vendorRisk',
    text: 'How do you assess and manage fourth-party AI risk — that is, the AI systems used by your AI vendors?',
    helpText: 'Fourth-party risk from AI-in-AI supply chains is an emerging regulatory expectation under DORA Article 28 and the EU AI Act, where downstream liability can flow through the provider chain.',
    type: 'radio',
    options: [
      { label: 'Fourth-party AI risk not assessed', value: 100, riskLevel: 'critical' },
      { label: 'Fourth-party AI risk discussed but not operationalized in vendor management process', value: 75, riskLevel: 'high' },
      { label: 'Awareness of fourth-party AI risk exists; addressed through questionnaires but no systematic tracking or assessment', value: 50, riskLevel: 'high' },
      { label: 'Key vendors required to disclose AI subprocessors; high-risk fourth parties assessed; tracking in place but not comprehensive', value: 25, riskLevel: 'medium' },
      { label: 'Contractual requirement for vendors to disclose their AI subprocessors and sub-vendors; assessments of material fourth parties conducted; findings tracked in vendor risk register', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'vendor-a-3',
    dimension: 'vendorRisk',
    text: 'Have your contractual audit rights over AI vendors been exercised in practice within the last 24 months, and what did you find?',
    helpText: 'Audit rights that exist only on paper fail the control effectiveness test applied by ISO 27001, DORA, and external auditors; regulators increasingly expect evidence of exercised rights, not just contractual provisions.',
    type: 'radio',
    options: [
      { label: 'Audit rights not included in AI vendor contracts or never exercised', value: 100, riskLevel: 'critical' },
      { label: 'Audit rights included in contracts but not yet exercised in practice', value: 75, riskLevel: 'high' },
      { label: 'Audit rights exist in contracts; one or more vendor assessments conducted but not yet a systematic program', value: 50, riskLevel: 'high' },
      { label: 'Audit rights exercised for highest-risk vendors; findings tracked and remediated; not yet extended to all material vendors', value: 25, riskLevel: 'medium' },
      { label: 'Audit rights exercised for all critical AI vendors on a scheduled cycle; findings documented with vendor response and remediation tracking; results shared with board risk committee', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
  },
  {
    id: 'vendor-a-4',
    dimension: 'vendorRisk',
    text: 'Do you publish or share AI vendor performance scorecards, and how are underperforming vendors managed?',
    helpText: 'Vendor performance scorecards with transparent metrics and escalation paths are a governance maturity indicator under ISO 42001 and demonstrate accountability expected by regulators and institutional investors.',
    type: 'radio',
    options: [
      { label: 'No formal AI vendor performance measurement', value: 100, riskLevel: 'critical' },
      { label: 'Vendor performance monitored reactively; no structured scorecards', value: 75, riskLevel: 'high' },
      { label: 'Vendor performance tracked but scoring is informal; underperformance handled through bilateral vendor discussions without formal escalation', value: 50, riskLevel: 'high' },
      { label: 'Performance scorecards maintained for key vendors; shared with vendors; underperformance escalated through defined process', value: 25, riskLevel: 'medium' },
      { label: 'Structured scorecards with SLA, risk, and ethics KPIs for all material AI vendors; published internally and shared with vendors quarterly; underperformance triggers formal remediation plan with escalation to governance committee', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'vendor-a-5',
    dimension: 'vendorRisk',
    text: 'What regulatory notification obligations arise when an AI vendor makes material changes to their system, and how are these obligations operationalized?',
    helpText: 'EU AI Act Article 14 creates explicit notification obligations when AI system changes materially affect risk profiles of high-risk AI deployments. Failure to track vendor changes can create unmanaged compliance gaps when regulators investigate AI incidents.',
    type: 'radio',
    options: [
      { label: 'Regulatory notification requirements for vendor AI changes not assessed', value: 100, riskLevel: 'critical' },
      { label: 'Regulatory notification obligations acknowledged but not operationalized in vendor management', value: 75, riskLevel: 'high' },
      { label: 'Aware of notification obligations; vendor change management process exists but regulatory impact assessment is ad hoc', value: 50, riskLevel: 'high' },
      { label: 'Notification obligations understood for critical vendors; contractual change notices required; process defined to assess materiality and notify regulators as needed', value: 25, riskLevel: 'medium' },
      { label: 'Regulatory notification obligations mapped and documented by vendor and jurisdiction; contractual change notification clauses in place; automated alerting triggers compliance review when vendor change notice received', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'vendor-a-6',
    dimension: 'vendorRisk',
    text: 'How mature is your AI vendor exit strategy, and have data portability requirements been tested or validated?',
    helpText: 'Vendor lock-in and data portability gaps create operational and regulatory risk; DORA Article 28 and EU AI Act require exit plans for critical AI dependencies with demonstrable portability of AI outputs and training data.',
    type: 'radio',
    options: [
      { label: 'No formal vendor exit strategy or data portability assessment', value: 100, riskLevel: 'critical' },
      { label: 'Exit strategy exists at a high level without detailed plans or portability verification', value: 75, riskLevel: 'high' },
      { label: 'Exit strategy considerations documented but portability not validated; alternatives identified but not qualified', value: 50, riskLevel: 'high' },
      { label: 'Exit strategies documented for critical vendors; data portability contractually assured; not yet tested in practice', value: 25, riskLevel: 'medium' },
      { label: 'Documented exit strategy for all critical AI vendors including data portability validation, transition timeline, and alternative sourcing; exit plans tested through tabletop or live exercises', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'vendor-a-7',
    dimension: 'vendorRisk',
    text: 'Are independent third-party assessments conducted on your highest-risk AI vendors, and how are findings integrated into your risk register?',
    helpText: 'Relying solely on vendor self-assessments is insufficient for mature governance programs; third-party assessments are increasingly expected by financial regulators and institutional investors conducting AI supply chain due diligence.',
    type: 'radio',
    options: [
      { label: 'Relying on vendor self-assessment only; no third-party assessment of AI vendors', value: 100, riskLevel: 'critical' },
      { label: 'Relying on vendor-provided SOC 2 or equivalent; independent AI-specific assessments not yet commissioned', value: 75, riskLevel: 'high' },
      { label: 'Third-party assessments conducted for one or more vendors but not systematically across the critical vendor population', value: 50, riskLevel: 'high' },
      { label: 'Third-party assessments conducted for highest-risk vendors; findings tracked and managed; program being extended to broader vendor population', value: 25, riskLevel: 'medium' },
      { label: 'Third-party assessments commissioned for all critical AI vendors on a defined cycle; findings integrated into vendor risk register; material findings escalated to board risk committee', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'vendor-a-8',
    dimension: 'vendorRisk',
    text: 'How does your AI vendor contract framework address liability, indemnification, and financial remedies for AI-specific harms?',
    helpText: 'AI-specific contractual protections — covering model failures, bias-related harms, and regulatory penalties — are not standard in legacy vendor contracts and must be explicitly negotiated, as contemplated under EU AI Act liability provisions.',
    type: 'radio',
    options: [
      { label: 'No review of AI-specific contractual liability conducted', value: 100, riskLevel: 'critical' },
      { label: 'Standard IT vendor contracts used without AI-specific liability provisions', value: 75, riskLevel: 'high' },
      { label: 'General liability provisions in vendor contracts; AI-specific additions negotiated on a case-by-case basis without a standard clause library', value: 50, riskLevel: 'high' },
      { label: 'AI-specific contractual protections in place for critical vendors; legal review completed; not yet extended to all material vendors', value: 25, riskLevel: 'medium' },
      { label: 'AI-specific liability, indemnification, and financial remedy clauses in all material vendor contracts; reviewed and updated annually by legal and risk; tested in at least one claim or dispute scenario', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'vendor-a-9',
    dimension: 'vendorRisk',
    text: 'How is your AI vendor incident response integrated with your own incident response plan, and when was this integration last tested?',
    helpText: 'Integrated incident response across the AI supply chain is required by DORA Article 17 and is a maturity indicator under NIST CSF RESPOND function; untested integration creates dangerous gaps during actual AI incidents.',
    type: 'radio',
    options: [
      { label: 'AI vendor IR not integrated with own incident response', value: 100, riskLevel: 'critical' },
      { label: 'Vendor incident notification covered in contracts but not integrated into own IR plan operationally', value: 75, riskLevel: 'high' },
      { label: 'Vendor escalation contacts maintained in IR plan; integration relies on manual coordination without pre-agreed procedures', value: 50, riskLevel: 'high' },
      { label: 'IR integration documented and communication channels established with critical vendors; tested informally; formal exercise planned', value: 25, riskLevel: 'medium' },
      { label: 'Vendor IR integration documented in own IR plan with defined escalation paths, communication protocols, and joint response procedures; tested via tabletop or simulation within last 12 months', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'vendor-a-10',
    dimension: 'vendorRisk',
    text: 'Is your AI vendor due diligence framework published externally, and does it signal your governance standards to prospective vendors?',
    helpText: 'Publishing due diligence standards signals governance leadership, sets expectations in vendor pre-qualification, and supports ISO 42001 supply chain transparency requirements and investor ESG disclosures.',
    type: 'radio',
    options: [
      { label: 'No formal AI vendor due diligence framework', value: 100, riskLevel: 'critical' },
      { label: 'Due diligence criteria exist but not standardized or communicated proactively to vendors', value: 75, riskLevel: 'high' },
      { label: 'Due diligence criteria standardized internally; communicated to vendors informally during onboarding rather than pre-qualification', value: 50, riskLevel: 'high' },
      { label: 'Framework documented and shared with prospective vendors during procurement; not publicly published but available on request', value: 25, riskLevel: 'medium' },
      { label: 'AI vendor due diligence framework published on company website or in governance disclosures; referenced in vendor RFP/RFI processes; updated annually with versioning', value: 0, riskLevel: 'low' },
      ],
    weight: 0.9,
  },

  // ---------------------------------------------------------------------------
  // DATA GOVERNANCE — data-a-1 to data-a-10
  // ---------------------------------------------------------------------------
  {
    id: 'data-a-1',
    dimension: 'dataGovernance',
    text: 'How is automated data lineage maintained for all AI models in production, and how is lineage accuracy validated?',
    helpText: 'Automated data lineage is foundational to explainability obligations under GDPR Article 22 and EU AI Act Annex IV, enabling regulators to trace AI output back to source data and understand model behavior.',
    type: 'radio',
    options: [
      { label: 'No systematic data lineage for AI models in production', value: 100, riskLevel: 'critical' },
      { label: 'Lineage documentation exists for some models but coverage is incomplete and not systematically validated', value: 75, riskLevel: 'high' },
      { label: 'Lineage documentation maintained manually or through partially automated tooling; gaps known and being addressed', value: 50, riskLevel: 'high' },
      { label: 'Automated lineage in place for critical models; manual lineage maintained for lower-risk systems; validation process defined', value: 25, riskLevel: 'medium' },
      { label: 'Automated lineage tracking across all production AI models with full upstream-downstream visibility; accuracy validated by data governance team or third party; lineage accessible for regulatory inquiry', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'data-a-2',
    dimension: 'dataGovernance',
    text: 'How are Data Protection Impact Assessments (DPIAs) or AI Impact Assessments integrated into the AI development lifecycle, and what governance gate enforces this?',
    helpText: 'GDPR Article 35 requires DPIAs for high-risk processing; EU AI Act Article 9 creates parallel obligations; integrating these into development gates prevents post-launch compliance remediation which is costly and reputationally damaging.',
    type: 'radio',
    options: [
      { label: 'No DPIA or AI impact assessment integrated into development lifecycle', value: 100, riskLevel: 'critical' },
      { label: 'DPIA process exists for data processing broadly but AI-specific impact assessment not formalized', value: 75, riskLevel: 'high' },
      { label: 'Assessment process defined but not consistently enforced as a development gate; some assessments completed post-launch', value: 50, riskLevel: 'high' },
      { label: 'DPIA/AI impact assessment required for high-risk AI projects; process integrated into project approval; most assessments completed before launch', value: 25, riskLevel: 'medium' },
      { label: 'DPIA/AI impact assessment is a mandatory gate in the AI development lifecycle; completion required for sign-off; DPO or privacy function reviews all assessments; results tracked in a register', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'data-a-3',
    dimension: 'dataGovernance',
    text: 'How do you maintain a cross-border AI data transfer register, and what controls ensure ongoing legal adequacy of transfers?',
    helpText: 'Cross-border AI data transfers require legal bases under GDPR Chapter V; post-Schrems II, standard contractual clauses must be supplemented by transfer impact assessments, and AI model training across borders adds complexity.',
    type: 'radio',
    options: [
      { label: 'Cross-border AI data transfers not systematically tracked or assessed', value: 100, riskLevel: 'critical' },
      { label: 'Cross-border transfers acknowledged; legal basis assumed from general data processing agreements without AI-specific review', value: 75, riskLevel: 'high' },
      { label: 'Cross-border transfers identified and covered by SCCs or other mechanisms; register not fully comprehensive or regularly maintained', value: 50, riskLevel: 'high' },
      { label: 'Transfer register maintained for known AI data flows; SCCs or adequacy decisions in place; reviewed annually or when material changes occur', value: 25, riskLevel: 'medium' },
      { label: 'Comprehensive cross-border AI data transfer register maintained; legal basis documented per transfer; transfer impact assessments completed; register reviewed quarterly and updated when vendor or model changes occur', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'data-a-4',
    dimension: 'dataGovernance',
    text: 'How is your consent management platform integrated with AI inference systems to ensure consent status is honored at the point of inference?',
    helpText: 'Real-time consent enforcement at the AI inference layer is required under GDPR Article 7 and CCPA/CPRA; failure to honor consent withdrawals in AI systems is a high-risk compliance gap in supervisory reviews.',
    type: 'radio',
    options: [
      { label: 'No integration between consent management and AI inference systems', value: 100, riskLevel: 'critical' },
      { label: 'Consent management exists for data collection but not specifically integrated with AI inference or recommendation systems', value: 75, riskLevel: 'high' },
      { label: 'Consent honored through periodic batch processes; real-time enforcement at AI inference not yet implemented', value: 50, riskLevel: 'high' },
      { label: 'CMP connected to AI systems for critical use cases; consent withdrawal triggers AI inference suppression within defined timeframe; not yet comprehensive across all AI applications', value: 25, riskLevel: 'medium' },
      { label: 'Consent management platform (CMP) integrated with AI inference systems via API; consent withdrawal propagates automatically to AI inference within defined SLA; integration tested and validated', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'data-a-5',
    dimension: 'dataGovernance',
    text: 'What governance and ethics review process applies to the use of synthetic data in AI training, and how is synthetic data quality assured?',
    helpText: 'Synthetic data introduces risks of amplifying biases from original datasets, creating privacy loopholes, and misleading model performance metrics; governance of synthetic data is an emerging expectation in AI ethics frameworks including the NIST AI RMF.',
    type: 'radio',
    options: [
      { label: 'No governance applied to synthetic data use in AI training', value: 100, riskLevel: 'critical' },
      { label: 'Synthetic data used in AI training without a specific governance or ethics review framework', value: 75, riskLevel: 'high' },
      { label: 'Synthetic data use acknowledged in data management policy; quality standards defined but ethics review is informal', value: 50, riskLevel: 'high' },
      { label: 'Synthetic data use cases reviewed by data governance or ethics function; quality assurance requirements defined and applied for critical uses', value: 25, riskLevel: 'medium' },
      { label: 'Formal synthetic data governance policy with ethics review board sign-off for novel uses; quality assurance protocol with bias and distributional shift testing; use cases and outcomes tracked', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'data-a-6',
    dimension: 'dataGovernance',
    text: 'Are formal data quality SLAs defined and monitored for AI systems in production, and how are breaches escalated?',
    helpText: 'Data quality degradation in AI production systems can cause silent model drift and biased outcomes; formalizing SLAs and breach escalation aligns with ISO 8000 data quality standards and ISO 42001 AI data requirements.',
    type: 'radio',
    options: [
      { label: 'No data quality SLAs or monitoring specific to AI production systems', value: 100, riskLevel: 'critical' },
      { label: 'Data quality monitored through general data management processes without AI-specific SLAs', value: 75, riskLevel: 'high' },
      { label: 'Data quality monitoring in place but SLAs informally defined; escalation process ad hoc', value: 50, riskLevel: 'high' },
      { label: 'Data quality thresholds defined and monitored for critical AI systems; breach escalation process defined and tested', value: 25, riskLevel: 'medium' },
      { label: 'Data quality SLAs defined per AI system with automated monitoring; breaches trigger automated alert and formal escalation path to data owner and AI governance function; SLA performance reported quarterly', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
  },
  {
    id: 'data-a-7',
    dimension: 'dataGovernance',
    text: 'Have your data governance controls for AI been independently validated by an external party, and how were findings addressed?',
    helpText: 'External validation of data governance controls provides assurance beyond internal self-assessment and is increasingly expected by regulators, auditors, and institutional investors as part of AI governance due diligence.',
    type: 'radio',
    options: [
      { label: 'No independent external validation of AI data governance controls', value: 100, riskLevel: 'critical' },
      { label: 'Data governance assessed as part of a broader audit; AI-specific controls not independently validated', value: 75, riskLevel: 'high' },
      { label: 'Internal audit review of data governance conducted; external review has not yet been commissioned', value: 50, riskLevel: 'high' },
      { label: 'External assessment completed; findings addressed; remediation tracked; follow-up review planned', value: 25, riskLevel: 'medium' },
      { label: 'External validation completed within last 18 months; formal findings report with management response; all critical findings remediated; next validation scheduled', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'data-a-8',
    dimension: 'dataGovernance',
    text: 'Does your AI training data ethics review board evaluate datasets for bias, provenance, rights clearance, and ethical sourcing before use?',
    helpText: 'Ethics review of training datasets is a core recommendation of the EU AI Act Annex IV and the NIST AI RMF; undisclosed use of scraped, biased, or rights-encumbered data creates legal and reputational exposure.',
    type: 'radio',
    options: [
      { label: 'No ethics review of AI training datasets', value: 100, riskLevel: 'critical' },
      { label: 'Training data provenance and rights checked informally; no formal ethics review process', value: 75, riskLevel: 'high' },
      { label: 'Training data reviewed for legal rights and obvious biases; structured ethics review process not formalized', value: 50, riskLevel: 'high' },
      { label: 'Ethics review conducted for training datasets by data governance and legal functions; criteria defined; review documented for material datasets', value: 25, riskLevel: 'medium' },
      { label: 'Formal AI training data ethics review board with documented terms of reference; reviews all new training datasets against bias, provenance, rights, and ethical sourcing criteria; decisions logged with rationale', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'data-a-9',
    dimension: 'dataGovernance',
    text: 'How do you handle data subject rights requests (erasure, portability, correction) for data used in AI model training and inference outputs?',
    helpText: 'GDPR Articles 17 and 20 apply to AI-derived data; honoring erasure and portability requests for data embedded in trained model weights is technically complex and a growing area of regulatory scrutiny and enforcement.',
    type: 'radio',
    options: [
      { label: 'Data subject rights processes do not address AI training data or inference outputs', value: 100, riskLevel: 'critical' },
      { label: 'Data subject rights honored for raw data; impact on AI models and derived outputs not systematically assessed', value: 75, riskLevel: 'high' },
      { label: 'Data subject rights process covers primary data systems; AI model training data and derived outputs addressed on a case-by-case basis without consistent methodology', value: 50, riskLevel: 'high' },
      { label: 'Process defined for data subject rights requests in AI context; technical limitations documented and communicated; DPO involved in complex cases', value: 25, riskLevel: 'medium' },
      { label: 'Documented process for handling data subject rights requests covering training data and AI-derived outputs; technical feasibility assessed per model type; unresolvable cases escalated to DPO with documented rationale; response SLAs met consistently', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'data-a-10',
    dimension: 'dataGovernance',
    text: 'How is AI data governance performance reported to the board, and what metrics are used to track program effectiveness?',
    helpText: 'Board-level data governance reporting for AI is expected under ISO 42001 and signals to investors, auditors, and regulators that accountability for AI data risk is embedded at the highest level of the organization.',
    type: 'radio',
    options: [
      { label: 'No board-level reporting on AI data governance performance', value: 100, riskLevel: 'critical' },
      { label: 'Data governance reporting to board is ad hoc; AI-specific data governance performance not measured', value: 75, riskLevel: 'high' },
      { label: 'Data governance metrics reported as part of broader data risk reporting; AI-specific metrics not yet disaggregated', value: 50, riskLevel: 'high' },
      { label: 'Key AI data governance metrics reported to board or audit committee semi-annually; senior leadership receives quarterly update', value: 25, riskLevel: 'medium' },
      { label: 'Structured AI data governance dashboard with defined KPIs (lineage coverage, DPIA completion rate, consent compliance, rights request fulfillment) reported to board quarterly; trends and remediation tracked', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
  },

  // ---------------------------------------------------------------------------
  // SECURITY & COMPLIANCE — security-a-1 to security-a-10
  // ---------------------------------------------------------------------------
  {
    id: 'security-a-1',
    dimension: 'securityCompliance',
    text: 'What is the status of your ISO 42001 certification or formal certification roadmap, and which AI systems are in scope?',
    helpText: 'ISO 42001 is becoming the de facto enterprise standard for AI management systems; certification or a formal roadmap signals governance leadership and is increasingly requested by enterprise customers and regulators.',
    type: 'radio',
    options: [
      { label: 'No ISO 42001 certification or roadmap', value: 100, riskLevel: 'critical' },
      { label: 'ISO 42001 awareness high; gap assessment initiated or planned but implementation roadmap not approved', value: 75, riskLevel: 'high' },
      { label: 'ISO 42001 gap assessment completed; roadmap approved; implementation underway but certification not yet imminent', value: 50, riskLevel: 'high' },
      { label: 'ISO 42001 certification in progress with Stage 1 audit completed or imminent; scope defined; target date confirmed', value: 25, riskLevel: 'medium' },
      { label: 'ISO 42001 certified (or certification audit completed); scope covers all material AI systems; surveillance audits scheduled; certification referenced in governance disclosures', value: 0, riskLevel: 'low' },
      ],
    weight: 1.4,
  },
  {
    id: 'security-a-2',
    dimension: 'securityCompliance',
    text: 'How is your EU AI Act high-risk AI classification register maintained, and what controls are documented against each classified system?',
    helpText: 'EU AI Act Article 9 requires a risk management system for each high-risk AI system; a classification register with mapped controls is the operational foundation for compliance and the primary artifact requested in regulatory examinations.',
    type: 'radio',
    options: [
      { label: 'No EU AI Act high-risk classification register', value: 100, riskLevel: 'critical' },
      { label: 'Classification work initiated; register in draft form; not yet operationalized', value: 75, riskLevel: 'high' },
      { label: 'High-risk AI systems identified; classification partially documented; control mapping incomplete', value: 50, riskLevel: 'high' },
      { label: 'Classification register maintained for known high-risk AI systems; controls documented; updated when material changes occur', value: 25, riskLevel: 'medium' },
      { label: 'Comprehensive classification register with system inventory, risk tier, regulatory obligations, and mapped controls; updated when new AI systems deployed; reviewed by legal and compliance quarterly', value: 0, riskLevel: 'low' },
      ],
    weight: 1.4,
    jurisdictions: ['eu'],
  },
  {
    id: 'security-a-3',
    dimension: 'securityCompliance',
    text: 'How frequently is independent penetration testing of AI systems conducted, and are AI-specific attack vectors (prompt injection, model inversion, adversarial inputs) included in scope?',
    helpText: 'AI-specific attack vectors are not covered by standard network penetration testing; NIST AI RMF GOVERN 1.2 and emerging AI security frameworks require AI-specific adversarial testing to identify model-layer vulnerabilities.',
    type: 'radio',
    options: [
      { label: 'No penetration testing of AI systems', value: 100, riskLevel: 'critical' },
      { label: 'Penetration testing conducted for IT infrastructure; AI applications not specifically included in scope', value: 75, riskLevel: 'high' },
      { label: 'Standard penetration testing covers AI infrastructure; AI-specific attack vectors not yet systematically in scope', value: 50, riskLevel: 'high' },
      { label: 'Penetration testing conducted for critical AI systems; AI-specific vectors included; findings tracked and remediated; reporting to CISO and risk committee', value: 25, riskLevel: 'medium' },
      { label: 'Annual independent penetration testing of production AI systems with AI-specific attack vectors in scope; findings remediated on defined SLA; results reported to board risk committee', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
  },
  {
    id: 'security-a-4',
    dimension: 'securityCompliance',
    text: 'How prepared are you to respond to a regulatory examination of your AI systems, and are evidence packages pre-built and maintained?',
    helpText: 'Regulatory examination readiness — including pre-built evidence packages — is a maturity differentiator; organizations unprepared for AI-specific supervisory inquiries face extended examination timelines and higher risk of adverse findings.',
    type: 'radio',
    options: [
      { label: 'Not specifically prepared for AI regulatory examination', value: 100, riskLevel: 'critical' },
      { label: 'Compliance documentation maintained but examination readiness process not formally defined', value: 75, riskLevel: 'high' },
      { label: 'Evidence gathered when needed; key documentation maintained but not pre-packaged for examination; no examination readiness exercises conducted', value: 50, riskLevel: 'high' },
      { label: 'Evidence packages prepared for major regulatory obligations; updated annually; examination readiness team identified but not drilled', value: 25, riskLevel: 'medium' },
      { label: 'Pre-built evidence packages for all regulatory obligations; updated on defined cycle; examination readiness drills conducted; designated examination response team with defined roles', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'security-a-5',
    dimension: 'securityCompliance',
    text: 'How do you map and assure AI compliance controls across multiple regulatory frameworks simultaneously, and how are conflicts resolved?',
    helpText: 'Multi-framework compliance (EU AI Act, ISO 42001, GDPR, NIST AI RMF, sector-specific requirements) requires a unified control mapping to avoid duplication, gaps, and conflicting obligations — a capability expected of organizations at regulatory maturity Level 4+.',
    type: 'radio',
    options: [
      { label: 'Regulatory frameworks addressed in silos without cross-framework mapping', value: 100, riskLevel: 'critical' },
      { label: 'Primary regulatory frameworks addressed; cross-framework mapping and conflict resolution not yet operationalized', value: 75, riskLevel: 'high' },
      { label: 'Frameworks addressed separately with manual reconciliation; gaps and conflicts identified but not systematically resolved', value: 50, riskLevel: 'high' },
      { label: 'Compliance control mapping completed for material frameworks; cross-framework conflicts identified and managed; consolidated reporting in place', value: 25, riskLevel: 'medium' },
      { label: 'Unified AI compliance control framework mapped across all applicable regulations; automated control assurance testing where possible; conflicts documented and resolved with legal review; cross-framework view presented to board', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'security-a-6',
    dimension: 'securityCompliance',
    text: 'Does your organization operate a formal AI security red team exercise program, and how are findings fed back into governance?',
    helpText: 'AI red team exercises — simulating adversarial attacks, manipulation, and misuse — are recommended by NIST AI RMF MANAGE 4.1 and are increasingly mandated by regulators for critical AI systems in financial services and other sectors.',
    type: 'radio',
    options: [
      { label: 'No AI security red team exercise program', value: 100, riskLevel: 'critical' },
      { label: 'Standard security red team exercises cover some AI infrastructure; AI-specific adversarial testing not in scope', value: 75, riskLevel: 'high' },
      { label: 'AI red teaming conducted informally or on an ad hoc basis; findings managed through security team without structured governance escalation', value: 50, riskLevel: 'high' },
      { label: 'AI red team exercises conducted; findings tracked and remediated; board-level reporting on material findings; program being formalized', value: 25, riskLevel: 'medium' },
      { label: 'Formal AI red team program with dedicated team or retained external capability; exercises conducted at least annually for critical AI systems; findings presented to board risk committee; remediation tracked', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'security-a-7',
    dimension: 'securityCompliance',
    text: 'Does your organization proactively engage with regulators on AI compliance, and what is the nature and frequency of that engagement?',
    helpText: 'Proactive regulatory engagement — through supervisory dialogue, regulatory sandboxes, or industry working groups — signals governance confidence and reduces the risk of adversarial regulator relationships that elevate enforcement likelihood.',
    type: 'radio',
    options: [
      { label: 'No proactive regulatory engagement on AI matters', value: 100, riskLevel: 'critical' },
      { label: 'Regulatory communication managed by legal/compliance; no proactive AI-specific engagement strategy', value: 75, riskLevel: 'high' },
      { label: 'Regulatory engagement primarily through mandatory filings and responses to inquiries; no proactive outreach', value: 50, riskLevel: 'high' },
      { label: 'Regular dialogue with primary regulators on AI matters; occasional participation in industry consultations; engagement mostly reactive to regulatory inquiries', value: 25, riskLevel: 'medium' },
      { label: 'Proactive and documented engagement with relevant regulators; participation in regulatory AI working groups or sandboxes; outreach not limited to mandated filings; engagement tracked and reported to board', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'security-a-8',
    dimension: 'securityCompliance',
    text: 'How is your AI compliance scorecard structured, what KPIs does it include, and how often is it presented to the board?',
    helpText: 'Quarterly board-level compliance scorecards for AI are expected under ISO 42001 clause 9 (performance evaluation) and by institutional investors who assess AI governance in their proxy voting and ESG due diligence processes.',
    type: 'radio',
    options: [
      { label: 'No board-level AI compliance scorecard', value: 100, riskLevel: 'critical' },
      { label: 'Compliance status reported to board ad hoc; no defined scorecard structure or KPIs', value: 75, riskLevel: 'high' },
      { label: 'Compliance metrics reported to board annually as part of broader risk report; AI-specific scorecard not yet separate', value: 50, riskLevel: 'high' },
      { label: 'AI compliance scorecard presented to board or audit committee semi-annually; KPIs defined and measured; trend data available', value: 25, riskLevel: 'medium' },
      { label: 'Structured AI compliance scorecard with quantified KPIs (control effectiveness, open findings aging, framework coverage) presented to board quarterly; trend analysis included; scorecard independently reviewed annually', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
  },
  {
    id: 'security-a-9',
    dimension: 'securityCompliance',
    text: 'Are third-party AI compliance assessment results tracked over time, and are improvement trends used to demonstrate continuous improvement to regulators?',
    helpText: 'Longitudinal tracking of third-party assessment results demonstrates continuous improvement — a core principle of ISO 42001 clause 10 and a key indicator regulators use to differentiate proactive governance from reactive compliance.',
    type: 'radio',
    options: [
      { label: 'No longitudinal tracking of third-party AI compliance assessment results', value: 100, riskLevel: 'critical' },
      { label: 'Third-party assessments conducted but results not systematically tracked over time', value: 75, riskLevel: 'high' },
      { label: 'Third-party assessments conducted; findings remediated; longitudinal tracking and trend analysis not yet implemented', value: 50, riskLevel: 'high' },
      { label: 'Third-party assessment results tracked across cycles; improvements quantified; trend data available for internal reporting', value: 25, riskLevel: 'medium' },
      { label: 'Third-party assessment results tracked longitudinally with trend analysis; improvement trajectory documented and shared with board and regulators when relevant; benchmark against prior periods included in annual governance report', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'security-a-10',
    dimension: 'securityCompliance',
    text: 'How does your organization harmonize AI compliance obligations across multiple jurisdictions, and who owns the cross-jurisdictional strategy?',
    helpText: 'Cross-jurisdictional AI compliance (EU AI Act, UK AI framework, US state AI laws, CCPA, APPI) requires a centralized harmonization strategy to avoid duplicative controls and conflicting obligations — a Level 5 governance maturity capability.',
    type: 'radio',
    options: [
      { label: 'No cross-jurisdictional AI compliance harmonization strategy', value: 100, riskLevel: 'critical' },
      { label: 'Cross-jurisdictional compliance awareness exists; addressed reactively as new obligations arise', value: 75, riskLevel: 'high' },
      { label: 'Jurisdictional requirements addressed separately; harmonization recognized as needed but not yet operationalized', value: 50, riskLevel: 'high' },
      { label: 'Cross-jurisdictional mapping in place for major AI regulatory requirements; harmonized control set being built; ownership assigned', value: 25, riskLevel: 'medium' },
      { label: 'Documented cross-jurisdictional AI compliance strategy with designated owner (e.g., Global AI Compliance Lead); jurisdiction-specific obligations mapped to common control set; conflicts resolved and documented; strategy reviewed annually', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us', 'ap'],
  },

  // ---------------------------------------------------------------------------
  // AI RISKS — airisk-a-1 to airisk-a-10
  // ---------------------------------------------------------------------------
  {
    id: 'airisk-a-1',
    dimension: 'aiSpecificRisks',
    text: 'Are independent third-party bias audits conducted for your AI systems, and are results published internally or externally?',
    helpText: 'Independent bias audits with published results are required by New York City Local Law 144 for automated employment decision tools and are expected by the EU AI Act for high-risk AI systems; published results demonstrate accountability and build stakeholder trust.',
    type: 'radio',
    options: [
      { label: 'No independent third-party bias audits', value: 100, riskLevel: 'critical' },
      { label: 'Bias testing conducted internally; independent third-party audits not yet commissioned', value: 75, riskLevel: 'high' },
      { label: 'Third-party bias assessments conducted but not on a formal annual cycle; results not published', value: 50, riskLevel: 'high' },
      { label: 'Independent bias audits conducted for highest-risk AI systems; results published internally; external publication in progress', value: 25, riskLevel: 'medium' },
      { label: 'Independent third-party bias audits conducted for all high-risk AI systems on annual cycle; audit methodology and summary results published externally; audit reports available to regulators on request', value: 0, riskLevel: 'low' },
      ],
    weight: 1.4,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'airisk-a-2',
    dimension: 'aiSpecificRisks',
    text: 'Does your AI ethics board have a documented charter with defined decision authority, escalation paths, and accountability for AI ethics outcomes?',
    helpText: 'An AI ethics board with formal charter and documented decision authority — not merely advisory status — is a governance maturity marker under ISO 42001 clause 5 and signals to external stakeholders that AI ethics commitments have structural accountability.',
    type: 'radio',
    options: [
      { label: 'No AI ethics board or equivalent governance body', value: 100, riskLevel: 'critical' },
      { label: 'AI ethics function exists (individual or informal group) but no formal board, charter, or documented authority', value: 75, riskLevel: 'high' },
      { label: 'AI ethics board or committee exists with defined scope; charter in development or partially documented; authority is advisory rather than binding', value: 50, riskLevel: 'high' },
      { label: 'AI ethics board with charter and defined decision authority; C-suite represented; effectiveness reviewed periodically', value: 25, riskLevel: 'medium' },
      { label: 'AI ethics board with documented charter, defined decision authority (including veto rights on high-risk AI deployments), C-suite representation, external member(s), and annual effectiveness review', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
  },
  {
    id: 'airisk-a-3',
    dimension: 'aiSpecificRisks',
    text: 'Are AI model cards and system cards published for your AI systems, and what level of transparency do they provide to internal and external audiences?',
    helpText: 'Model cards (Google, 2019) and system cards are recommended transparency artifacts under EU AI Act Annex IV and NIST AI RMF; publication signals accountability and enables downstream deployers and affected parties to make informed decisions.',
    type: 'radio',
    options: [
      { label: 'No model cards or system cards published', value: 100, riskLevel: 'critical' },
      { label: 'Model documentation exists but not in model card format; not published to external audiences', value: 75, riskLevel: 'high' },
      { label: 'Model card templates defined; published for some systems; coverage not yet complete across the AI portfolio', value: 50, riskLevel: 'high' },
      { label: 'Model cards published internally for production AI systems; external publication for highest-risk systems; templates standardized', value: 25, riskLevel: 'medium' },
      { label: 'Model cards and/or system cards published for all production AI systems; externally available for high-risk systems; include intended use, performance metrics by demographic group, known limitations, and bias testing results', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'airisk-a-4',
    dimension: 'aiSpecificRisks',
    text: 'How is your AI incident register maintained, and does it include root cause analysis, trend reporting, and systematic learning?',
    helpText: 'A mature AI incident register with root cause analysis and trend reporting supports the continuous improvement obligations of ISO 42001 clause 10 and enables evidence-based risk management rather than reactive incident handling.',
    type: 'radio',
    options: [
      { label: 'No formal AI incident register', value: 100, riskLevel: 'critical' },
      { label: 'AI incidents tracked informally; no structured root cause analysis or trend reporting', value: 75, riskLevel: 'high' },
      { label: 'AI incidents logged; some root cause analysis conducted; trend reporting not yet systematized', value: 50, riskLevel: 'high' },
      { label: 'AI incident register maintained; root cause analysis conducted for significant incidents; trend data reviewed by governance function periodically', value: 25, riskLevel: 'medium' },
      { label: 'AI incident register with all incidents logged, classified by type and severity, root cause analysis completed for all significant incidents, and quarterly trend report presented to AI governance committee or board', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'airisk-a-5',
    dimension: 'aiSpecificRisks',
    text: 'Has the board approved a formal AI risk appetite statement, and how is this operationalized across the AI portfolio?',
    helpText: 'A board-approved AI risk appetite statement is expected under ISO 42001 clause 6.1 and enterprise risk frameworks; without it, AI risk decisions are made without explicit governance boundaries, undermining accountability and auditability.',
    type: 'radio',
    options: [
      { label: 'No AI risk appetite statement', value: 100, riskLevel: 'critical' },
      { label: 'AI risk appetite captured within broader enterprise risk appetite but not AI-specific; not operationalized through AI deployment decisions', value: 75, riskLevel: 'high' },
      { label: 'AI risk appetite under development; partially operationalized through governance processes; board awareness but not yet formal approval', value: 50, riskLevel: 'high' },
      { label: 'AI risk appetite statement approved by board or risk committee; operationalized through governance thresholds; reviewed periodically', value: 25, riskLevel: 'medium' },
      { label: 'Board-approved AI risk appetite statement with quantified tolerances; operationalized through risk limits applied to AI deployment decisions; exceptions require board or risk committee approval; reviewed annually', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
  },
  {
    id: 'airisk-a-6',
    dimension: 'aiSpecificRisks',
    text: 'How is the effectiveness of human oversight mechanisms in your AI systems measured and reported, and what evidence supports their reliability?',
    helpText: 'EU AI Act Article 14 requires human oversight to be effective, not merely nominal; measuring and reporting on oversight effectiveness — rather than assuming it — is a Level 4+ governance maturity capability that regulators will test in examinations.',
    type: 'radio',
    options: [
      { label: 'Human oversight in place but effectiveness not measured or reported', value: 100, riskLevel: 'critical' },
      { label: 'Human oversight mechanisms exist; effectiveness assumed rather than measured', value: 75, riskLevel: 'high' },
      { label: 'Human oversight in place; effectiveness assessed through periodic review but not with defined metrics or independent validation', value: 50, riskLevel: 'high' },
      { label: 'Oversight effectiveness metrics defined and monitored; reported to governance function; independent review planned but not completed', value: 25, riskLevel: 'medium' },
      { label: 'Human oversight effectiveness measured through defined metrics (override rates, response times, escalation outcomes, fatigue indicators); results reported to governance committee; annual effectiveness review with independent validation', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'airisk-a-7',
    dimension: 'aiSpecificRisks',
    text: 'How are explainability requirements defined, enforced, and documented across AI use case tiers, and who is accountable for compliance?',
    helpText: 'Tiered explainability requirements — calibrated to the risk level of each AI use case — align with EU AI Act Article 13 and GDPR Article 22 right to explanation; documented requirements with named accountability satisfy auditor and regulator expectations.',
    type: 'radio',
    options: [
      { label: 'No formal explainability requirements or documentation', value: 100, riskLevel: 'critical' },
      { label: 'Explainability addressed on a case-by-case basis; no tiered framework or standardized documentation', value: 75, riskLevel: 'high' },
      { label: 'Explainability policy defined; applied to high-risk systems; documentation inconsistent; not yet fully tiered across risk levels', value: 50, riskLevel: 'high' },
      { label: 'Tiered explainability requirements defined; most production AI systems compliant; accountability assigned; documentation in model registry', value: 25, riskLevel: 'medium' },
      { label: 'Explainability requirements defined per use case risk tier with technical methods specified; compliance documented in model registry; accountable owner named; adherence verified by governance function; regulator-ready explanations tested', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'airisk-a-8',
    dimension: 'aiSpecificRisks',
    text: 'How is AI risk formally integrated into enterprise risk management (ERM), and what is the reporting relationship between the AI risk function and the Chief Risk Officer?',
    helpText: 'Integration of AI risk into ERM — rather than treating it as a standalone IT risk — is expected under ISO 42001, the Basel Committee AI guidance, and institutional investor governance frameworks that assess AI as a material enterprise risk.',
    type: 'radio',
    options: [
      { label: 'AI risk not formally integrated into enterprise risk management', value: 100, riskLevel: 'critical' },
      { label: 'AI risk managed independently by technology/AI function; limited integration with ERM', value: 75, riskLevel: 'high' },
      { label: 'AI risk captured in ERM at a high level; detailed AI risk management operated separately; limited CRO integration', value: 50, riskLevel: 'high' },
      { label: 'AI risk integrated into ERM; AI-specific risks in enterprise risk register; regular reporting to CRO; board receives AI risk summary in risk reports', value: 25, riskLevel: 'medium' },
      { label: 'AI risk formally integrated into ERM with AI risk as a named category in the enterprise risk taxonomy; AI risk owner reports to CRO; AI risks in enterprise risk register; AI risk presented alongside credit, market, and operational risk at board level', value: 0, riskLevel: 'low' },
      ],
    weight: 1.3,
  },
  {
    id: 'airisk-a-9',
    dimension: 'aiSpecificRisks',
    text: 'Does your organization proactively engage with regulators on AI risk identification, and how are regulatory engagement outcomes reflected in your governance program?',
    helpText: 'Proactive regulatory engagement on AI risk identification — distinct from reactive compliance — is a characteristic of the most mature AI governance programs and reduces the risk of enforcement by building regulatory trust and shared understanding.',
    type: 'radio',
    options: [
      { label: 'No proactive regulatory engagement on AI risk', value: 100, riskLevel: 'critical' },
      { label: 'Regulatory engagement managed by legal/compliance in response to inquiries; no proactive AI risk dialogue', value: 75, riskLevel: 'high' },
      { label: 'Regulatory dialogue on AI risk occurs but is informal or limited to mandatory reporting interactions', value: 50, riskLevel: 'high' },
      { label: 'Regular engagement with regulators on AI risk topics; outcomes inform governance program; documented in AI governance record', value: 25, riskLevel: 'medium' },
      { label: 'Structured regulatory engagement program on AI risk; participation in supervisory AI working groups or innovation hubs; engagement outcomes documented and fed back into governance program with tracked actions', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'airisk-a-10',
    dimension: 'aiSpecificRisks',
    text: 'What AI rights and redress mechanisms exist for individuals affected by your AI systems, and how accessible and effective are they?',
    helpText: 'GDPR Article 22, EU AI Act Article 86, and analogous frameworks require meaningful redress for individuals subject to AI-driven decisions; effectiveness of redress mechanisms — not just their existence — is increasingly tested by regulators and civil society.',
    type: 'radio',
    options: [
      { label: 'No specific AI rights or redress mechanisms for affected individuals', value: 100, riskLevel: 'critical' },
      { label: 'Complaints process covers AI decisions but is not AI-specific; redress mechanisms not designed for AI context', value: 75, riskLevel: 'high' },
      { label: 'Redress process exists but awareness among affected individuals is limited; effectiveness not formally measured', value: 50, riskLevel: 'high' },
      { label: 'Redress mechanisms in place for AI-impacted individuals; human review available; SLAs defined; effectiveness reported internally', value: 25, riskLevel: 'medium' },
      { label: 'Documented AI redress mechanisms accessible to affected individuals; process includes human review option; response SLAs defined and met; effectiveness measured (volume, resolution rate, escalation outcomes) and reported to governance committee', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk', 'us'],
  },

  // ---------------------------------------------------------------------------
  // ROI TRACKING — roi-a-1 to roi-a-10
  // ---------------------------------------------------------------------------
  {
    id: 'roi-a-1',
    dimension: 'roiTracking',
    text: 'How is your AI investment portfolio reported to the board, and are returns presented on a risk-adjusted basis?',
    helpText: 'Risk-adjusted ROI reporting for AI — accounting for governance, compliance, and reputational risk costs — is emerging as best practice among mature AI governance programs and is expected by institutional investors applying responsible AI investment criteria.',
    type: 'radio',
    options: [
      { label: 'No consolidated AI investment portfolio reporting to board', value: 100, riskLevel: 'critical' },
      { label: 'AI investment costs and benefits tracked but not consolidated as portfolio; no risk-adjusted reporting to board', value: 75, riskLevel: 'high' },
      { label: 'AI investment performance reported to senior leadership; board reporting annual or ad hoc; risk adjustment not yet systematically applied', value: 50, riskLevel: 'high' },
      { label: 'AI portfolio performance reported to board semi-annually; risk-adjusted framing applied to major investments; methodology defined', value: 25, riskLevel: 'medium' },
      { label: 'AI investment portfolio presented to board quarterly with risk-adjusted returns; methodology documented and validated by finance; portfolio view includes compliance costs, risk provisions, and governance overhead', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
  {
    id: 'roi-a-2',
    dimension: 'roiTracking',
    text: 'Is the value realization from AI investments independently audited by the finance function or an external party?',
    helpText: 'Independent audit of AI value realization prevents benefit inflation in business cases, improves forecast accuracy, and is a governance maturity indicator that supports credible board-level AI investment decisions.',
    type: 'radio',
    options: [
      { label: 'AI value realization not independently reviewed or audited', value: 100, riskLevel: 'critical' },
      { label: 'AI value tracked by business units; no independent finance or audit review of realization against business case', value: 75, riskLevel: 'high' },
      { label: 'Finance reviews AI investment performance but through standard financial reporting rather than AI-specific value realization audit', value: 50, riskLevel: 'high' },
      { label: 'Finance function reviews AI value realization for major investments; independent audit of select high-value initiatives; findings tracked', value: 25, riskLevel: 'medium' },
      { label: 'AI value realization independently audited by finance or external auditor annually; methodology consistent across business units; findings inform future business case standards', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
  },
  {
    id: 'roi-a-3',
    dimension: 'roiTracking',
    text: 'Does your organization benchmark AI ROI against industry peers, and how is the methodology validated?',
    helpText: 'Competitive benchmarking of AI ROI provides context for board investment decisions and signals governance confidence; absence of benchmarking leaves leadership unable to assess whether AI investments are generating competitive returns.',
    type: 'radio',
    options: [
      { label: 'No competitive benchmarking of AI ROI', value: 100, riskLevel: 'critical' },
      { label: 'Aware of industry benchmarks; not yet applied to own AI investment performance assessment', value: 75, riskLevel: 'high' },
      { label: 'Informal benchmarking through analyst reports or industry networks; not systematically applied to own ROI data', value: 50, riskLevel: 'high' },
      { label: 'Competitive benchmarking conducted for key AI investments using available external data; results inform board discussions periodically', value: 25, riskLevel: 'medium' },
      { label: 'Formal AI ROI benchmarking against industry peers using validated external data sources (e.g., industry surveys, analyst benchmarks); methodology reviewed by finance; results presented to board with interpretation', value: 0, riskLevel: 'low' },
      ],
    weight: 0.9,
  },
  {
    id: 'roi-a-4',
    dimension: 'roiTracking',
    text: 'Has your organization developed a Total Cost of Responsible AI Ownership (TCRAO) model that captures governance, compliance, ethics, and risk management costs?',
    helpText: 'The Total Cost of Responsible AI Ownership — encompassing governance infrastructure, compliance activities, ethics review, bias auditing, and risk management — is necessary for accurate AI business cases and is increasingly requested by CFOs and investors.',
    type: 'radio',
    options: [
      { label: 'Responsible AI overhead costs not captured or modeled', value: 100, riskLevel: 'critical' },
      { label: 'AI governance and compliance costs tracked at a high level; not systematically included in AI business cases', value: 75, riskLevel: 'high' },
      { label: 'Governance and compliance costs tracked separately; not yet integrated into a unified TCRAO model for business case analysis', value: 50, riskLevel: 'high' },
      { label: 'TCRAO model developed and applied to major AI investments; governance costs captured; reported to senior finance leadership', value: 25, riskLevel: 'medium' },
      { label: 'Formal TCRAO model with documented methodology capturing direct AI costs plus governance, compliance, ethics, and risk management overhead; applied to all material AI investments; reported to board', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'roi-a-5',
    dimension: 'roiTracking',
    text: 'Is AI governance cost tracked as a percentage of total AI investment, and how is this ratio used to inform governance funding decisions?',
    helpText: 'Tracking AI governance cost as a proportion of total AI investment enables right-sizing of governance budgets and prevents both under-investment (governance gaps) and over-investment (governance drag) — a financial discipline indicator for mature programs.',
    type: 'radio',
    options: [
      { label: 'AI governance cost not tracked or reported relative to AI investment', value: 100, riskLevel: 'critical' },
      { label: 'AI governance and AI investment costs tracked separately in different cost centers; ratio not computed', value: 75, riskLevel: 'high' },
      { label: 'AI governance costs tracked; ratio to total AI investment calculable but not routinely computed or reported', value: 50, riskLevel: 'high' },
      { label: 'Governance cost ratio tracked and reported to finance leadership; used to inform budget discussions; not yet benchmarked externally', value: 25, riskLevel: 'medium' },
      { label: 'AI governance cost as % of AI investment tracked, benchmarked against industry norms, and presented to board with trend analysis; ratio informs governance budget proposals and investment thresholds', value: 0, riskLevel: 'low' },
      ],
    weight: 0.9,
  },
  {
    id: 'roi-a-6',
    dimension: 'roiTracking',
    text: 'Are AI benefits realization outcomes linked to executive compensation, OKRs, or equivalent accountability mechanisms?',
    helpText: 'Linking AI value realization to executive compensation or OKRs creates the accountability for benefits delivery that distinguishes programs that generate measurable returns from those where governance is decoupled from financial performance.',
    type: 'radio',
    options: [
      { label: 'No accountability linkage between AI value realization and executive performance', value: 100, riskLevel: 'critical' },
      { label: 'AI investment success recognized informally in performance reviews; no formal OKR or compensation linkage', value: 75, riskLevel: 'high' },
      { label: 'AI delivery goals included in performance frameworks for some leaders; not systematically applied across AI portfolio', value: 50, riskLevel: 'high' },
      { label: 'AI value delivery included in OKRs for relevant leaders; outcomes tracked; compensation linkage indirect or informal', value: 25, riskLevel: 'medium' },
      { label: 'AI benefits realization explicitly linked to executive OKRs or compensation metrics for accountable leaders; linkage reviewed by remuneration committee; outcomes tracked and reported', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'roi-a-7',
    dimension: 'roiTracking',
    text: 'Does your organization participate in or commission external AI governance maturity benchmarking, and how are results used?',
    helpText: 'External benchmarking of AI governance maturity against industry peers provides an objective reference point for board investment decisions and can support ESG and responsible AI disclosures to investors and customers.',
    type: 'radio',
    options: [
      { label: 'No external AI governance maturity benchmarking', value: 100, riskLevel: 'critical' },
      { label: 'Internal maturity self-assessment conducted; no external benchmarking', value: 75, riskLevel: 'high' },
      { label: 'Industry maturity surveys or analyst assessments reviewed; own maturity not formally benchmarked against external reference', value: 50, riskLevel: 'high' },
      { label: 'External benchmarking completed; results used to inform governance priorities; not yet on a formal cycle', value: 25, riskLevel: 'medium' },
      { label: 'External AI maturity benchmarking conducted at least biennially using recognized framework (e.g., CMMI for AI, Stanford HAI, Gartner AI maturity); results presented to board; improvement trajectory tracked', value: 0, riskLevel: 'low' },
      ],
    weight: 0.9,
  },
  {
    id: 'roi-a-8',
    dimension: 'roiTracking',
    text: 'Are post-implementation reviews (post-mortems) of AI business cases completed, and are lessons learned shared across the organization?',
    helpText: 'AI business case post-mortems — comparing realized value against projected returns — improve future investment decisions and demonstrate the organizational learning expected by ISO 42001 clause 10 and mature investment governance frameworks.',
    type: 'radio',
    options: [
      { label: 'No post-implementation review of AI business cases', value: 100, riskLevel: 'critical' },
      { label: 'Post-launch reviews conducted but focused on technical performance rather than business case vs. actual return', value: 75, riskLevel: 'high' },
      { label: 'Post-mortems conducted informally for some investments; lessons not systematically captured or shared', value: 50, riskLevel: 'high' },
      { label: 'Post-mortems conducted for significant AI investments; lessons documented; shared with governance and finance leadership', value: 25, riskLevel: 'medium' },
      { label: 'Formal AI business case post-mortems completed for all material investments 12 months post-launch; lessons learned documented and shared with AI governance community and finance; inform future business case standards', value: 0, riskLevel: 'low' },
      ],
    weight: 1.0,
  },
  {
    id: 'roi-a-9',
    dimension: 'roiTracking',
    text: 'How is the ROI of your AI governance program itself quantified, including risk avoidance, incidents prevented, and compliance cost savings?',
    helpText: 'Quantifying AI governance ROI — capturing the value of risk prevented, incidents avoided, and regulatory penalties averted — enables evidence-based governance investment decisions and demonstrates responsible AI leadership to investors and boards.',
    type: 'radio',
    options: [
      { label: 'AI governance ROI not quantified or measured', value: 100, riskLevel: 'critical' },
      { label: 'Governance value acknowledged but not quantified; budget justification relies on compliance obligation rather than ROI case', value: 75, riskLevel: 'high' },
      { label: 'Qualitative assessment of governance value conducted; quantification in development; not yet presented to board', value: 50, riskLevel: 'high' },
      { label: 'Governance ROI estimated for major program areas; methodology defined; reported to senior leadership to support budget discussions', value: 25, riskLevel: 'medium' },
      { label: 'AI governance ROI quantified annually using documented methodology; includes risk avoidance value, incidents prevented (probability-adjusted cost), and compliance cost savings; presented to board as part of governance investment justification', value: 0, riskLevel: 'low' },
      ],
    weight: 1.1,
  },
  {
    id: 'roi-a-10',
    dimension: 'roiTracking',
    text: 'Does the board conduct an annual review of the AI investment thesis, including strategic alignment, risk tolerance, and return expectations?',
    helpText: 'Annual board review of the AI investment thesis — covering strategic rationale, risk appetite calibration, and return expectations — is expected by institutional investors applying AI governance criteria and signals that AI investment is subject to board-level discipline, not delegated entirely to management.',
    type: 'radio',
    options: [
      { label: 'No board-level review of AI investment thesis', value: 100, riskLevel: 'critical' },
      { label: 'Board receives AI updates periodically; no structured annual investment thesis review', value: 75, riskLevel: 'high' },
      { label: 'AI strategy presented to board annually but investment thesis, risk tolerance, and return expectations not explicitly reviewed as separate agenda items', value: 50, riskLevel: 'high' },
      { label: 'Board reviews AI strategy and investment performance annually; covers strategic alignment and broad risk appetite; return expectations compared to actuals', value: 25, riskLevel: 'medium' },
      { label: 'Formal annual board review of AI investment thesis; agenda includes strategic alignment, risk tolerance calibration, return expectations vs. actual, and competitive positioning; outcomes documented and inform next-year AI strategy', value: 0, riskLevel: 'low' },
      ],
    weight: 1.2,
  },
];
