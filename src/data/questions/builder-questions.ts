// =============================================================================
// BUILDER BANK — 60 questions
// For orgs actively integrating AI into workflows and building with AI.
// Focus: formalizing practices, identifying gaps, scaling responsibly.
// =============================================================================

import { AssessmentQuestion } from '@/types/assessment';

export const BUILDER_QUESTIONS: AssessmentQuestion[] = [
  // --- Shadow AI & Visibility (10) ---
  {
    id: 'shadow-b-1',
    dimension: 'shadowAI',
    text: 'Does your organization maintain a formal, up-to-date inventory of all AI tools in use?',
    helpText: 'A living inventory — covering tool name, owning team, data it accesses, and approval status — is the foundation of shadow AI governance. Without it, you cannot enforce policy or assess risk.',
    type: 'radio',
    options: [
      { label: 'No formal inventory exists', value: 100 },
      { label: 'Partial list, not consistently maintained', value: 65 },
      { label: 'Formal inventory exists but not current', value: 35 },
      { label: 'Formal, current inventory maintained and reviewed regularly', value: 0 },
    ],
  },
  {
    id: 'shadow-b-2',
    dimension: 'shadowAI',
    text: 'Is there a documented approval process employees must follow before adopting a new AI tool?',
    helpText: 'An approval workflow — even a simple intake form — gives IT and security visibility before tools enter the environment. Without it, adoption outpaces governance.',
    type: 'radio',
    options: [
      { label: 'No process exists', value: 100 },
      { label: 'Informal expectation, not documented', value: 65 },
      { label: 'Process exists but inconsistently followed', value: 35 },
      { label: 'Documented process consistently followed across the org', value: 0 },
    ],
  },
  {
    id: 'shadow-b-3',
    dimension: 'shadowAI',
    text: 'Do you have a written AI acceptable use policy that employees have formally acknowledged?',
    helpText: 'An acknowledged policy creates a shared standard and legal baseline. Verbal guidance or informal norms are insufficient as AI usage scales across the organization.',
    type: 'radio',
    options: [
      { label: 'No written policy exists', value: 100 },
      { label: 'Draft policy exists but not distributed or acknowledged', value: 65 },
      { label: 'Written policy distributed but not formally acknowledged', value: 35 },
      { label: 'Written policy in place with formal employee acknowledgment', value: 0 },
    ],
  },
  {
    id: 'shadow-b-4',
    dimension: 'shadowAI',
    text: 'How frequently do you review and update your AI tool inventory and approved tool list?',
    helpText: 'The AI tool landscape changes rapidly. A list that was accurate six months ago is likely already outdated. Quarterly reviews are a reasonable minimum for Builder-stage organizations.',
    type: 'radio',
    options: [
      { label: 'No review cadence established', value: 100 },
      { label: 'Reviewed ad hoc when issues arise', value: 65 },
      { label: 'Reviewed annually', value: 35 },
      { label: 'Reviewed quarterly or more frequently', value: 0 },
    ],
  },
  {
    id: 'shadow-b-5',
    dimension: 'shadowAI',
    text: 'Are AI tool sign-ups and access provisioning monitored through your identity provider or IT systems?',
    helpText: 'SSO enforcement and identity provider monitoring catch tools that employees adopt using company email addresses. This is one of the most effective technical controls for shadow AI.',
    type: 'radio',
    options: [
      { label: 'No monitoring in place', value: 100 },
      { label: 'Some tools are monitored, most are not', value: 65 },
      { label: 'Monitoring exists but not systematically reviewed', value: 35 },
      { label: 'Centralized monitoring via identity provider with regular review', value: 0 },
    ],
  },
  {
    id: 'shadow-b-6',
    dimension: 'shadowAI',
    text: 'Do employees need to request approval before using a new AI tool — including free tools?',
    helpText: 'Many shadow AI gaps start with free tools. An employee can sign up for ChatGPT, Perplexity, or Notion AI without any spending — yet those tools still process company data. An approval process that covers all AI tools, paid or free, gives you real visibility into your AI footprint.',
    type: 'radio',
    options: [
      { label: 'No approval process — employees adopt AI tools freely', value: 100 },
      { label: 'Approval only required for paid tools; free tools are unrestricted', value: 65 },
      { label: 'Approval is expected but not consistently enforced', value: 35 },
      { label: 'All AI tools require approval before use, regardless of cost', value: 0 },
    ],
  },
  {
    id: 'shadow-b-7',
    dimension: 'shadowAI',
    text: 'Do procurement and/or legal review AI vendor agreements before sign-off?',
    helpText: 'AI vendor contracts contain critical terms about data usage, model training, liability, and IP. Procurement and legal review catches problematic clauses before they become obligations.',
    type: 'radio',
    options: [
      { label: 'No review — employees accept terms directly', value: 100 },
      { label: 'Review happens for large contracts, not standard SaaS', value: 65 },
      { label: 'Procurement reviews, but legal rarely involved', value: 35 },
      { label: 'Both procurement and legal review all AI vendor agreements', value: 0 },
    ],
  },
  {
    id: 'shadow-b-8',
    dimension: 'shadowAI',
    text: 'Have you identified which departments carry the highest AI tool adoption and associated data risk?',
    helpText: 'Marketing, HR, legal, and finance teams often adopt AI tools faster and with more sensitive data than IT realizes. Knowing where concentration is highest lets you prioritize governance effort.',
    type: 'radio',
    options: [
      { label: 'No department-level analysis done', value: 100 },
      { label: 'Awareness based on anecdote, not data', value: 65 },
      { label: 'Some departments mapped, not comprehensive', value: 35 },
      { label: 'Full department-level inventory and risk assessment completed', value: 0 },
    ],
  },
  {
    id: 'shadow-b-9',
    dimension: 'shadowAI',
    text: 'Are there defined consequences for employees who use unapproved AI tools, and are they consistently enforced?',
    helpText: 'Policy without enforcement is decoration. Clearly communicated and consistently applied consequences — even minor ones — dramatically reduce shadow AI adoption.',
    type: 'radio',
    options: [
      { label: 'No policy or consequences defined', value: 100 },
      { label: 'Policy exists but no defined consequences', value: 65 },
      { label: 'Consequences defined but inconsistently enforced', value: 35 },
      { label: 'Defined consequences, consistently communicated and enforced', value: 0 },
    ],
  },
  {
    id: 'shadow-b-10',
    dimension: 'shadowAI',
    text: 'Do you track AI features being added to existing enterprise tools (Microsoft 365, Salesforce, Google Workspace) and review them before they activate?',
    helpText: 'Major enterprise vendors are embedding AI into existing products — often enabled by default on your existing license. These activations bypass your standard intake process and can silently expand your AI footprint.',
    type: 'radio',
    options: [
      { label: 'Not tracking embedded AI features in existing tools', value: 100 },
      { label: 'Aware this happens but no formal review process', value: 65 },
      { label: 'Review some vendors, not all', value: 35 },
      { label: 'Systematic review of all vendor AI feature releases before activation', value: 0 },
    ],
  },

  // --- Vendor Risk (10) ---
  {
    id: 'vendor-b-1',
    dimension: 'vendorRisk',
    text: 'Do you conduct a security review of AI vendors before onboarding them?',
    helpText: 'A security review — even a lightweight questionnaire — establishes a baseline of vendor security posture before your data enters their environment. Without it, you are accepting unknown risk.',
    type: 'radio',
    options: [
      { label: 'No security review before onboarding', value: 100 },
      { label: 'Informal check done for some vendors', value: 65 },
      { label: 'Structured review for high-risk vendors only', value: 35 },
      { label: 'Formal security review required for all AI vendors', value: 0 },
    ],
  },
  {
    id: 'vendor-b-2',
    dimension: 'vendorRisk',
    text: 'Do AI vendor contracts include data processing agreements (DPAs) specifying how your data is handled?',
    helpText: 'A DPA is legally required under GDPR and best practice everywhere. It specifies what the vendor can do with your data, how long they retain it, and their obligations if a breach occurs.',
    type: 'radio',
    options: [
      { label: 'No DPAs in place with AI vendors', value: 100 },
      { label: 'DPAs exist for some vendors, not all', value: 65 },
      { label: 'DPAs in place for regulated data vendors only', value: 35 },
      { label: 'DPAs required and in place for all AI vendors', value: 0 },
    ],
  },
  {
    id: 'vendor-b-3',
    dimension: 'vendorRisk',
    text: 'Have you determined whether your AI vendors use your organization\'s data to train their models?',
    helpText: 'Many AI vendors default to using customer data for model improvement unless explicitly opted out. This can expose confidential information and create IP or privacy obligations you are unaware of.',
    type: 'radio',
    options: [
      { label: 'Not reviewed — unknown for most vendors', value: 100 },
      { label: 'Checked for a few key vendors, not all', value: 65 },
      { label: 'Reviewed for all vendors but opt-outs not consistently applied', value: 35 },
      { label: 'Reviewed and opted out of model training for all relevant vendors', value: 0 },
    ],
  },
  {
    id: 'vendor-b-4',
    dimension: 'vendorRisk',
    text: 'Have you reviewed your AI vendors\' data retention and deletion policies?',
    helpText: 'Retention policies determine how long your data lives on vendor infrastructure after you stop using the service. Long retention windows increase breach exposure and may conflict with your own data lifecycle obligations.',
    type: 'radio',
    options: [
      { label: 'Not reviewed for any vendors', value: 100 },
      { label: 'Reviewed informally for a few vendors', value: 65 },
      { label: 'Formally reviewed for high-risk vendors', value: 35 },
      { label: 'Reviewed and documented for all AI vendors', value: 0 },
    ],
  },
  {
    id: 'vendor-b-5',
    dimension: 'vendorRisk',
    text: 'Do you review AI vendor security certifications (SOC 2, ISO 27001) as part of procurement?',
    helpText: 'Security certifications are third-party attestations of a vendor\'s security controls. A vendor unable to produce a recent SOC 2 or equivalent is a significant risk signal.',
    type: 'radio',
    options: [
      { label: 'Not reviewed as part of procurement', value: 100 },
      { label: 'Requested but not consistently required', value: 65 },
      { label: 'Required for some vendor tiers, not all', value: 35 },
      { label: 'Required for all AI vendors handling sensitive data', value: 0 },
    ],
  },
  {
    id: 'vendor-b-6',
    dimension: 'vendorRisk',
    text: 'Do you track which AI vendors have access to sensitive, regulated, or confidential data?',
    helpText: 'Not all AI vendor relationships carry equal risk. Knowing which vendors touch regulated data (PII, financial records, health data) lets you prioritize oversight and ensure appropriate contracts are in place.',
    type: 'radio',
    options: [
      { label: 'No tracking of data sensitivity by vendor', value: 100 },
      { label: 'Informally known for obvious cases', value: 65 },
      { label: 'Tracked for some vendors, not comprehensive', value: 35 },
      { label: 'Full vendor-data sensitivity mapping maintained', value: 0 },
    ],
  },
  {
    id: 'vendor-b-7',
    dimension: 'vendorRisk',
    text: 'Have you assessed operational continuity risk if a key AI vendor experiences an outage or shuts down?',
    helpText: 'AI tool dependency creates new operational risk. If a critical workflow now depends on a vendor API, an outage or service discontinuation can halt operations. Builder-stage orgs often discover this the hard way.',
    type: 'radio',
    options: [
      { label: 'Not considered', value: 100 },
      { label: 'Discussed but no formal assessment', value: 65 },
      { label: 'Assessed for highest-dependency vendors', value: 35 },
      { label: 'Continuity risk assessed and mitigations in place for all critical vendors', value: 0 },
    ],
  },
  {
    id: 'vendor-b-8',
    dimension: 'vendorRisk',
    text: 'Have you assessed concentration risk — how dependent your organization is on any single AI vendor?',
    helpText: 'Heavy reliance on a single AI platform (e.g., OpenAI, Microsoft, Google) creates strategic risk if pricing, terms, or access changes. Builder-stage organizations should understand their dependency profile.',
    type: 'radio',
    options: [
      { label: 'Concentration risk not considered', value: 100 },
      { label: 'Aware of dependencies but no formal assessment', value: 65 },
      { label: 'Assessed but no mitigation strategy developed', value: 35 },
      { label: 'Assessed and actively managing concentration risk', value: 0 },
    ],
  },
  {
    id: 'vendor-b-9',
    dimension: 'vendorRisk',
    text: 'Do you review AI vendors\' subprocessor lists to understand who else has access to your data?',
    helpText: 'Most AI vendors rely on subprocessors — cloud providers, analytics tools, support platforms — that also handle your data. Understanding this chain is critical for data residency, privacy, and compliance requirements.',
    type: 'radio',
    options: [
      { label: 'Not reviewed for any vendors', value: 100 },
      { label: 'Aware of the concept but not formally reviewed', value: 65 },
      { label: 'Reviewed for vendors handling regulated data', value: 35 },
      { label: 'Subprocessor lists reviewed and documented for all AI vendors', value: 0 },
    ],
  },
  {
    id: 'vendor-b-10',
    dimension: 'vendorRisk',
    text: 'Have you assessed which of your AI vendors fall under EU AI Act obligations and requested compliance documentation?',
    helpText: 'The EU AI Act (enforcement from August 2026) places obligations on both AI providers and deployers. If your vendors supply high-risk AI systems, you may have obligations as a deployer — regardless of where you are headquartered.',
    type: 'radio',
    jurisdictions: ['eu'],
    options: [
      { label: 'Not reviewed — unaware of EU AI Act vendor obligations', value: 100 },
      { label: 'Aware but no vendor assessment done', value: 65 },
      { label: 'Assessed some vendors, not all', value: 35 },
      { label: 'All vendors assessed and compliance documentation requested', value: 0 },
    ],
  },

  // --- Data Governance (10) ---
  {
    id: 'data-b-1',
    dimension: 'dataGovernance',
    text: 'Has your organization formally defined which data types cannot be shared with external AI tools?',
    helpText: 'A data classification policy that explicitly covers AI tools — listing what is prohibited (customer PII, financial records, trade secrets, regulated health data) — is the baseline control for data governance at Builder stage.',
    type: 'radio',
    options: [
      { label: 'No classification or prohibition defined', value: 100 },
      { label: 'General data classification exists but AI not addressed', value: 65 },
      { label: 'AI-specific guidance exists but informally communicated', value: 35 },
      { label: 'Formal AI data classification policy documented and communicated', value: 0 },
    ],
  },
  {
    id: 'data-b-2',
    dimension: 'dataGovernance',
    text: 'Do you have documented data handling procedures specific to AI tool usage?',
    helpText: 'Generic data handling policies often do not address AI-specific risks like model training data usage, prompt logging, or output storage. AI-specific procedures close this gap.',
    type: 'radio',
    options: [
      { label: 'No AI-specific procedures', value: 100 },
      { label: 'General data procedures apply but nothing AI-specific', value: 65 },
      { label: 'AI-specific procedures in draft or informally used', value: 35 },
      { label: 'Documented AI data handling procedures in active use', value: 0 },
    ],
  },
  {
    id: 'data-b-3',
    dimension: 'dataGovernance',
    text: 'Is access to AI tools restricted based on employee role or the sensitivity of data they handle?',
    helpText: 'Role-based access control for AI tools ensures that employees only use tools appropriate for their data access level. An HR team member should not have unrestricted access to an AI tool that can query the entire customer database.',
    type: 'radio',
    options: [
      { label: 'No role-based restrictions on AI tool access', value: 100 },
      { label: 'Some restrictions informally applied', value: 65 },
      { label: 'Restrictions defined for high-sensitivity roles only', value: 35 },
      { label: 'Role-based AI tool access controls formally implemented', value: 0 },
    ],
  },
  {
    id: 'data-b-4',
    dimension: 'dataGovernance',
    text: 'Is there a formal review process for AI tools that process personally identifiable information (PII)?',
    helpText: 'AI tools that process PII create privacy, security, and compliance obligations. A formal review — including a privacy impact assessment — before deployment is a key control at the Builder stage.',
    type: 'radio',
    options: [
      { label: 'No review process for PII-processing AI tools', value: 100 },
      { label: 'Informal awareness check done for some tools', value: 65 },
      { label: 'Review process exists but not consistently applied', value: 35 },
      { label: 'Formal privacy review required before deploying any PII-processing AI tool', value: 0 },
    ],
  },
  {
    id: 'data-b-5',
    dimension: 'dataGovernance',
    text: 'Have you established retention policies for AI-generated content and outputs stored in your systems?',
    helpText: 'AI-generated content — summaries, drafts, analysis outputs — accumulates rapidly in shared drives and collaboration tools. Without retention policies, this content may violate data minimization obligations or persist beyond its useful life.',
    type: 'radio',
    options: [
      { label: 'No retention policies for AI-generated content', value: 100 },
      { label: 'General content retention applies but AI not specifically addressed', value: 65 },
      { label: 'AI content retention policy in draft or informally applied', value: 35 },
      { label: 'Formal AI-generated content retention policy in place', value: 0 },
    ],
  },
  {
    id: 'data-b-6',
    dimension: 'dataGovernance',
    text: 'Do you have a process for handling data subject access or deletion requests that involve data processed by AI tools?',
    helpText: 'If a customer or employee exercises their right to access or erasure, you need to know which AI tools processed their data and be able to fulfill the request across those systems. This is a core GDPR and CCPA obligation.',
    type: 'radio',
    jurisdictions: ['eu', 'us'],
    options: [
      { label: 'No process — AI tools not considered in data subject requests', value: 100 },
      { label: 'Aware this is a gap but no process defined', value: 65 },
      { label: 'Process exists for some tools, not comprehensive', value: 35 },
      { label: 'Comprehensive process covering all AI-processed data', value: 0 },
    ],
  },
  {
    id: 'data-b-7',
    dimension: 'dataGovernance',
    text: 'Have you mapped data flows between your internal systems and AI tools?',
    helpText: 'Data flow mapping shows what data enters AI tools, what is returned, and where outputs are stored. Without this map, you cannot assess risk, enforce policy, or respond to incidents involving AI-processed data.',
    type: 'radio',
    options: [
      { label: 'No data flow mapping done', value: 100 },
      { label: 'Informal understanding but nothing documented', value: 65 },
      { label: 'Mapped for highest-risk tools only', value: 35 },
      { label: 'Comprehensive data flow map covering all AI tools', value: 0 },
    ],
  },
  {
    id: 'data-b-8',
    dimension: 'dataGovernance',
    text: 'Do you restrict employees from uploading customer data to external AI tools without explicit approval?',
    helpText: 'Customer data uploaded to external AI tools may be logged, stored, or used for training — creating breach, regulatory, and reputational risk. Explicit approval controls are a minimum control at Builder stage.',
    type: 'radio',
    options: [
      { label: 'No restriction in place', value: 100 },
      { label: 'Verbally discouraged but not formally restricted', value: 65 },
      { label: 'Restriction defined but inconsistently enforced', value: 35 },
      { label: 'Formal restriction with approval workflow and enforcement', value: 0 },
    ],
  },
  {
    id: 'data-b-9',
    dimension: 'dataGovernance',
    text: 'Do you know whether your AI tools share your data with third parties or subprocessors?',
    helpText: 'AI vendors routinely share data with infrastructure providers, analytics tools, and safety monitoring services. Understanding this chain is required for accurate privacy notices, contract terms, and regulatory compliance.',
    type: 'radio',
    options: [
      { label: 'Unknown — not reviewed', value: 100 },
      { label: 'Checked for some tools, not systematically', value: 65 },
      { label: 'Reviewed for regulated-data tools only', value: 35 },
      { label: 'Third-party data sharing understood and documented for all AI tools', value: 0 },
    ],
  },
  {
    id: 'data-b-10',
    dimension: 'dataGovernance',
    text: 'Do you apply data minimization principles — ensuring AI tools only access the data they need to function?',
    helpText: 'Data minimization is a core principle under GDPR and a security best practice universally. Configuring AI tools with least-privilege data access reduces breach impact and regulatory exposure.',
    type: 'radio',
    options: [
      { label: 'No data minimization applied to AI tool configuration', value: 100 },
      { label: 'Considered informally but not enforced', value: 65 },
      { label: 'Applied for highest-risk tools', value: 35 },
      { label: 'Data minimization enforced as standard for all AI tool deployments', value: 0 },
    ],
  },

  // --- Security & Compliance (10) ---
  {
    id: 'security-b-1',
    dimension: 'securityCompliance',
    text: 'Are all AI tool accounts protected with SSO and multi-factor authentication (MFA)?',
    helpText: 'AI tools accessed with personal credentials outside your identity provider create unmanaged access risk. SSO enforcement ensures centralized visibility and control, and MFA reduces credential-based compromise.',
    type: 'radio',
    options: [
      { label: 'Most AI tools use personal credentials outside SSO', value: 100 },
      { label: 'SSO applied to some tools, not systematically', value: 65 },
      { label: 'SSO enforced, MFA inconsistently applied', value: 35 },
      { label: 'SSO and MFA required for all AI tool access', value: 0 },
    ],
  },
  {
    id: 'security-b-2',
    dimension: 'securityCompliance',
    text: 'Do you have an incident response plan that explicitly covers AI-related data exposure or security events?',
    helpText: 'AI-related incidents — a vendor breach exposing your data, an employee sending sensitive data to an AI tool, a model producing harmful outputs — require specific response steps. Generic IT incident plans often do not cover these scenarios.',
    type: 'radio',
    options: [
      { label: 'No incident response plan covers AI scenarios', value: 100 },
      { label: 'General IT incident plan exists but AI not addressed', value: 65 },
      { label: 'AI scenarios partially covered in existing plan', value: 35 },
      { label: 'Incident response plan explicitly covers AI-related events and is tested', value: 0 },
    ],
  },
  {
    id: 'security-b-3',
    dimension: 'securityCompliance',
    text: 'Have you mapped your AI tool usage against applicable compliance frameworks (SOC 2, HIPAA, PCI-DSS, etc.)?',
    helpText: 'AI tool adoption can create unintended compliance scope expansion. A tool that processes payment card data brings it into PCI-DSS scope; one that processes health data may trigger HIPAA obligations. Mapping is required before you can assess gaps.',
    type: 'radio',
    options: [
      { label: 'No compliance mapping for AI tool usage', value: 100 },
      { label: 'Aware of potential issues but no formal mapping', value: 65 },
      { label: 'Mapped for primary compliance framework only', value: 35 },
      { label: 'AI tool usage mapped against all applicable frameworks', value: 0 },
    ],
  },
  {
    id: 'security-b-4',
    dimension: 'securityCompliance',
    text: 'Are access logs collected for AI tools handling sensitive data, and are they periodically reviewed?',
    helpText: 'Access logging creates the audit trail needed to detect unauthorized use, investigate incidents, and demonstrate compliance. Many SaaS AI tools provide audit logs — the question is whether you are collecting and reviewing them.',
    type: 'radio',
    options: [
      { label: 'No access logging for AI tools', value: 100 },
      { label: 'Logging available but not collected or reviewed', value: 65 },
      { label: 'Collected for some tools, reviewed infrequently', value: 35 },
      { label: 'Logs collected for all sensitive AI tools and reviewed on schedule', value: 0 },
    ],
  },
  {
    id: 'security-b-5',
    dimension: 'securityCompliance',
    text: 'Does your employee offboarding process include revoking access to all AI tools?',
    helpText: 'Former employees retaining AI tool access — especially to tools connected to company data sources — is a persistent risk. AI tool access is often missed in offboarding checklists because tools were adopted outside IT.',
    type: 'radio',
    options: [
      { label: 'AI tools not included in offboarding process', value: 100 },
      { label: 'Some AI tools included, most missed', value: 65 },
      { label: 'Process covers IT-provisioned AI tools, not shadow adoption', value: 35 },
      { label: 'All AI tools included in a comprehensive offboarding checklist', value: 0 },
    ],
  },
  {
    id: 'security-b-6',
    dimension: 'securityCompliance',
    text: 'Have you conducted security assessments of AI tools that handle sensitive or regulated data?',
    helpText: 'A security assessment — reviewing architecture, data handling, encryption, and access controls — is a standard control before deploying tools that handle sensitive data. For AI tools, this should also include model behavior and output risk.',
    type: 'radio',
    options: [
      { label: 'No security assessments conducted for AI tools', value: 100 },
      { label: 'Informal review done for some tools', value: 65 },
      { label: 'Formal assessment for highest-sensitivity tools only', value: 35 },
      { label: 'Security assessments required before deployment of any sensitive-data AI tool', value: 0 },
    ],
  },
  {
    id: 'security-b-7',
    dimension: 'securityCompliance',
    text: 'Do you communicate minimum security requirements to AI vendors during procurement?',
    helpText: 'Procurement is your highest leverage point for security. Vendors who know your requirements are more likely to meet them — and you have contractual recourse if they do not.',
    type: 'radio',
    options: [
      { label: 'No security requirements communicated to vendors', value: 100 },
      { label: 'Informally discussed but not documented in contracts', value: 65 },
      { label: 'Requirements shared for some vendors', value: 35 },
      { label: 'Documented security requirements included in all AI vendor contracts', value: 0 },
    ],
  },
  {
    id: 'security-b-8',
    dimension: 'securityCompliance',
    text: 'Have you assessed which AI systems you use fall under the EU AI Act\'s risk classification, and documented your obligations as a deployer?',
    helpText: 'The EU AI Act (enforcement from August 2, 2026) classifies AI systems by risk level. High-risk systems — including those used in hiring, credit scoring, education, and law enforcement — carry significant compliance obligations for deployers, not just providers.',
    type: 'radio',
    jurisdictions: ['eu'],
    options: [
      { label: 'Not assessed — unaware of EU AI Act deployer obligations', value: 100 },
      { label: 'Aware of the Act but no formal assessment done', value: 65 },
      { label: 'Initial assessment started, not complete', value: 35 },
      { label: 'AI systems classified under EU AI Act with deployer obligations documented', value: 0 },
    ],
  },
  {
    id: 'security-b-9',
    dimension: 'securityCompliance',
    text: 'Have you reviewed your AI tool usage for compliance with California Consumer Privacy Act (CCPA/CPRA) obligations?',
    helpText: 'CCPA/CPRA applies to businesses serving California residents and covers personal information processed by AI tools. Obligations include disclosure in privacy notices, honoring opt-outs of sale/sharing, and maintaining records of processing activities.',
    type: 'radio',
    jurisdictions: ['us'],
    options: [
      { label: 'Not reviewed — CCPA obligations not assessed for AI', value: 100 },
      { label: 'Aware of CCPA but AI tool usage not specifically reviewed', value: 65 },
      { label: 'Partial review conducted', value: 35 },
      { label: 'Comprehensive CCPA review of AI tool usage completed', value: 0 },
    ],
  },
  {
    id: 'security-b-10',
    dimension: 'securityCompliance',
    text: 'Is there a designated owner responsible for monitoring AI-related compliance obligations as regulations evolve?',
    helpText: 'AI regulation is moving faster than any other technology area. Without a designated owner actively tracking developments — EU AI Act, state privacy laws, sector-specific rules — your organization will inevitably fall behind.',
    type: 'radio',
    options: [
      { label: 'No defined owner for AI compliance monitoring', value: 100 },
      { label: 'Someone loosely responsible but not formally assigned', value: 65 },
      { label: 'Owner assigned with partial scope', value: 35 },
      { label: 'Designated owner with clear mandate and regular reporting cadence', value: 0 },
    ],
  },

  // --- AI-Specific Risks (10) ---
  {
    id: 'risk-b-1',
    dimension: 'aiSpecificRisks',
    text: 'Do employees have documented guidelines for reviewing and validating AI-generated outputs before acting on them?',
    helpText: 'AI hallucination rates are non-zero for every model. Without validation guidelines, employees may act on incorrect AI outputs — creating errors in customer communications, legal documents, financial analysis, and more.',
    type: 'radio',
    options: [
      { label: 'No validation guidelines exist', value: 100 },
      { label: 'Verbal expectation to check AI outputs, nothing written', value: 65 },
      { label: 'Written guidelines exist for some use cases', value: 35 },
      { label: 'Documented validation guidelines for all significant AI use cases', value: 0 },
    ],
  },
  {
    id: 'risk-b-2',
    dimension: 'aiSpecificRisks',
    text: 'Have you identified and documented AI use cases where errors could have material consequences?',
    helpText: 'An AI-generated typo in a draft blog post is minor. An error in an AI-assisted legal contract, financial report, or medical summary can have serious consequences. Knowing which use cases carry high error risk is prerequisite to managing them.',
    type: 'radio',
    options: [
      { label: 'No inventory of high-consequence AI use cases', value: 100 },
      { label: 'Informally understood but not documented', value: 65 },
      { label: 'Some high-risk use cases identified, not comprehensive', value: 35 },
      { label: 'Documented inventory of high-consequence AI use cases with controls', value: 0 },
    ],
  },
  {
    id: 'risk-b-3',
    dimension: 'aiSpecificRisks',
    text: 'Have you defined which decisions require human review before AI-generated recommendations are acted upon?',
    helpText: 'Human-in-the-loop requirements are a critical control for high-stakes AI use. Defining which decisions require human sign-off — and enforcing it — is the difference between AI as an assistant and AI as an unreviewed decision-maker.',
    type: 'radio',
    options: [
      { label: 'No human-in-the-loop requirements defined', value: 100 },
      { label: 'Expectation exists informally for some decisions', value: 65 },
      { label: 'Defined for some high-stakes decisions but not all', value: 35 },
      { label: 'Documented human-review requirements for all high-stakes AI decisions', value: 0 },
    ],
  },
  {
    id: 'risk-b-4',
    dimension: 'aiSpecificRisks',
    text: 'Is there a formal channel for employees to report AI errors, hallucinations, or unexpected behaviors?',
    helpText: 'Without a reporting channel, AI errors are invisible at the organizational level. A simple reporting mechanism — even a shared inbox or form — creates visibility into recurring issues and enables systematic improvement.',
    type: 'radio',
    options: [
      { label: 'No reporting channel exists', value: 100 },
      { label: 'Employees can report informally but no dedicated channel', value: 65 },
      { label: 'Reporting channel exists but infrequently used or reviewed', value: 35 },
      { label: 'Formal reporting channel with regular review and response process', value: 0 },
    ],
  },
  {
    id: 'risk-b-5',
    dimension: 'aiSpecificRisks',
    text: 'Have you assessed whether AI tools used in hiring, performance management, or customer-facing decisions could produce biased or discriminatory outcomes?',
    helpText: 'AI systems trained on historical data can perpetuate or amplify existing biases. This is both an ethical issue and a legal risk — particularly for decisions affecting employment, credit, and services.',
    type: 'radio',
    options: [
      { label: 'No bias assessment done for any AI tools', value: 100 },
      { label: 'Generally aware of bias risks but no formal assessment', value: 65 },
      { label: 'Assessment done for some high-risk use cases', value: 35 },
      { label: 'Formal bias assessment conducted for all people-facing AI applications', value: 0 },
    ],
  },
  {
    id: 'risk-b-6',
    dimension: 'aiSpecificRisks',
    text: 'Do you log or audit AI tool usage in workflows where AI outputs inform significant business decisions?',
    helpText: 'Audit trails for high-stakes AI usage are required for incident investigation, compliance demonstration, and accountability. Without logging, you cannot reconstruct what AI recommended or why a decision was made.',
    type: 'radio',
    options: [
      { label: 'No logging of AI usage in decision workflows', value: 100 },
      { label: 'Some logging exists but not specifically for decision contexts', value: 65 },
      { label: 'Logging in place for some high-stakes workflows', value: 35 },
      { label: 'Comprehensive audit logging for all AI-informed significant decisions', value: 0 },
    ],
  },
  {
    id: 'risk-b-7',
    dimension: 'aiSpecificRisks',
    text: 'Do you have a process to evaluate whether AI tools\' performance or output quality changes over time?',
    helpText: 'AI model updates, vendor changes, and data drift can cause tool behavior to shift without notice. A process to periodically evaluate performance catches degradation before it affects business outcomes.',
    type: 'radio',
    options: [
      { label: 'No evaluation process for AI tool performance over time', value: 100 },
      { label: 'Issues noticed reactively when problems arise', value: 65 },
      { label: 'Some tools evaluated periodically, not systematic', value: 35 },
      { label: 'Systematic performance evaluation process for all significant AI tools', value: 0 },
    ],
  },
  {
    id: 'risk-b-8',
    dimension: 'aiSpecificRisks',
    text: 'Has your organization defined who is accountable when an AI-assisted decision causes harm or a significant error?',
    helpText: '"The AI did it" is not a sufficient answer for regulators, customers, or courts. Accountability must be assigned to a person or role — and defined before an incident makes it urgent.',
    type: 'radio',
    options: [
      { label: 'Accountability not defined — unclear who is responsible', value: 100 },
      { label: 'Implicitly assumed but not formally assigned', value: 65 },
      { label: 'Accountability defined for some AI use cases', value: 35 },
      { label: 'Clear accountability assigned for all significant AI-assisted decisions', value: 0 },
    ],
  },
  {
    id: 'risk-b-9',
    dimension: 'aiSpecificRisks',
    text: 'Do you test AI tools before expanding their use into additional teams, workflows, or data types?',
    helpText: 'A tool that works well for one use case may perform poorly or unsafely in another context. Structured testing before expansion catches problems before they affect a broader population.',
    type: 'radio',
    options: [
      { label: 'No testing before expansion — tools are adopted and scaled freely', value: 100 },
      { label: 'Informal evaluation done sometimes', value: 65 },
      { label: 'Testing done for some expansions, not consistently', value: 35 },
      { label: 'Structured evaluation required before expanding any AI tool use case', value: 0 },
    ],
  },
  {
    id: 'risk-b-10',
    dimension: 'aiSpecificRisks',
    text: 'Have you addressed the risk of copyright infringement or IP ownership issues from AI-generated content used in your organization?',
    helpText: 'AI-generated content may incorporate copyrighted training data, and ownership of AI outputs varies by jurisdiction and tool. Organizations using AI-generated content in products, marketing, or client deliverables face potential infringement claims and IP ownership disputes.',
    type: 'radio',
    options: [
      { label: 'Not considered — no policy or guidance on AI-generated content IP', value: 100 },
      { label: 'Aware of the issue but no formal policy', value: 65 },
      { label: 'Policy exists for some content types or use cases', value: 35 },
      { label: 'Comprehensive IP and copyright policy covering all AI-generated content in use', value: 0 },
    ],
  },

  // --- ROI Tracking (10) ---
  {
    id: 'roi-b-1',
    dimension: 'roiTracking',
    text: 'Have you defined specific, measurable success metrics for your AI tool investments?',
    helpText: 'Without defined metrics, AI ROI is anecdotal. Success metrics should be specific (time saved per task, error rate reduction, cost per output) and agreed upon before deployment — not invented after the fact to justify the investment.',
    type: 'radio',
    options: [
      { label: 'No success metrics defined for AI investments', value: 100 },
      { label: 'Vague goals stated but no measurable metrics', value: 65 },
      { label: 'Metrics defined for some tools or teams', value: 35 },
      { label: 'Specific, measurable metrics defined for all significant AI investments', value: 0 },
    ],
  },
  {
    id: 'roi-b-2',
    dimension: 'roiTracking',
    text: 'Do you systematically track time savings from AI tool usage across teams?',
    helpText: 'Time savings is typically the largest and most measurable AI ROI driver. Systematic tracking — even through periodic surveys — gives you the data to demonstrate value, guide investment, and build the case for expanded AI use.',
    type: 'radio',
    options: [
      { label: 'No time savings tracking in place', value: 100 },
      { label: 'Anecdotal feedback collected informally', value: 65 },
      { label: 'Tracked in some teams through surveys or estimates', value: 35 },
      { label: 'Systematic time savings tracking across all teams using AI', value: 0 },
    ],
  },
  {
    id: 'roi-b-3',
    dimension: 'roiTracking',
    text: 'Do you have a centralized, accurate view of total AI tool spending across the organization?',
    helpText: 'AI spend is often fragmented across team budgets, credit cards, and departmental software contracts. Without a consolidated view, you cannot calculate ROI, identify redundancy, or negotiate volume discounts.',
    type: 'radio',
    options: [
      { label: 'No centralized view — AI spend is dispersed and untracked', value: 100 },
      { label: 'Some visibility through finance, but incomplete', value: 65 },
      { label: 'Most AI spend tracked centrally, some gaps', value: 35 },
      { label: 'Complete, centralized view of all AI tool spending', value: 0 },
    ],
  },
  {
    id: 'roi-b-4',
    dimension: 'roiTracking',
    text: 'Do you compare productivity or output quality across different AI tools to guide investment decisions?',
    helpText: 'Many organizations run multiple AI tools with overlapping capabilities. Comparing tools on consistent metrics lets you rationalize your portfolio, eliminate redundancy, and concentrate investment where it actually delivers value.',
    type: 'radio',
    options: [
      { label: 'No comparison done — tools adopted based on preference or vendor promotion', value: 100 },
      { label: 'Informal opinions shared but no structured comparison', value: 65 },
      { label: 'Some structured comparison done for major tool decisions', value: 35 },
      { label: 'Regular, data-driven comparison of AI tools informing investment decisions', value: 0 },
    ],
  },
  {
    id: 'roi-b-5',
    dimension: 'roiTracking',
    text: 'Have you calculated the cost per active user for your major AI tools?',
    helpText: 'Cost per active user reveals whether you are paying for unused seats. Many organizations find that 30–50% of AI tool licenses are inactive. Tracking this metric drives license rightsizing and improves ROI immediately.',
    type: 'radio',
    options: [
      { label: 'Not calculated — license costs not tracked by active usage', value: 100 },
      { label: 'Rough estimate available but not formally calculated', value: 65 },
      { label: 'Calculated for some tools', value: 35 },
      { label: 'Cost per active user calculated and monitored for all major AI tools', value: 0 },
    ],
  },
  {
    id: 'roi-b-6',
    dimension: 'roiTracking',
    text: 'Do you track the adoption rate of AI tools — what percentage of licensed employees actively use them?',
    helpText: 'Low adoption rates are the most common cause of poor AI ROI. Tracking adoption reveals where tools are underutilized, where change management is needed, and where training investment would have the highest impact.',
    type: 'radio',
    options: [
      { label: 'Adoption rate not tracked', value: 100 },
      { label: 'Anecdotal sense of who uses what, no data', value: 65 },
      { label: 'Tracked for some tools through vendor analytics', value: 35 },
      { label: 'Adoption rate tracked and reported for all licensed AI tools', value: 0 },
    ],
  },
  {
    id: 'roi-b-7',
    dimension: 'roiTracking',
    text: 'Is AI ROI or productivity data reported to any leadership stakeholder on a regular cadence?',
    helpText: 'Without leadership visibility, AI investments compete poorly against other priorities. Regular reporting — even a quarterly update — keeps AI on the agenda, builds the case for continued investment, and creates accountability for outcomes.',
    type: 'radio',
    options: [
      { label: 'No AI ROI reporting to leadership', value: 100 },
      { label: 'Mentioned in passing during general business reviews', value: 65 },
      { label: 'Informal reporting on some AI investments', value: 35 },
      { label: 'Regular, structured AI ROI reporting to leadership stakeholders', value: 0 },
    ],
  },
  {
    id: 'roi-b-8',
    dimension: 'roiTracking',
    text: 'Have you identified which AI use cases generate the most measurable business value?',
    helpText: 'AI ROI is highly uneven across use cases. Typically 20% of use cases drive 80% of the value. Identifying your highest-performing use cases lets you replicate success and concentrate future investment where it matters most.',
    type: 'radio',
    options: [
      { label: 'Not analyzed — no use case value ranking', value: 100 },
      { label: 'Informed opinions exist but no data backing them', value: 65 },
      { label: 'Some use cases analyzed and ranked', value: 35 },
      { label: 'Data-driven ranking of AI use cases by measurable business value', value: 0 },
    ],
  },
  {
    id: 'roi-b-9',
    dimension: 'roiTracking',
    text: 'Do you account for the cost of AI errors and rework — time spent correcting AI outputs — when calculating ROI?',
    helpText: 'Gross time savings from AI often look better than net savings once error correction is factored in. Tracking rework time gives you an honest ROI picture and identifies use cases where AI quality is not yet good enough to justify the cost.',
    type: 'radio',
    options: [
      { label: 'Rework cost not considered in ROI calculations', value: 100 },
      { label: 'Acknowledged as a factor but not measured', value: 65 },
      { label: 'Measured informally for some high-error use cases', value: 35 },
      { label: 'Rework costs systematically tracked and included in ROI calculations', value: 0 },
    ],
  },
  {
    id: 'roi-b-10',
    dimension: 'roiTracking',
    text: 'Have you conducted a cost-benefit analysis comparing building in-house AI capabilities versus using vendor tools?',
    helpText: 'As AI usage matures, the build-vs-buy decision becomes financially significant. Custom models offer more control and potential long-term cost efficiency; vendor tools offer speed and lower upfront cost. A structured analysis ensures you are making this decision deliberately rather than by default.',
    type: 'radio',
    options: [
      { label: 'Build vs. buy not formally analyzed — defaulting to vendors', value: 100 },
      { label: 'Discussed at a high level without financial analysis', value: 65 },
      { label: 'Analyzed for one or two specific use cases', value: 35 },
      { label: 'Structured build-vs-buy analysis conducted and informing strategy', value: 0 },
    ],
  },
];
