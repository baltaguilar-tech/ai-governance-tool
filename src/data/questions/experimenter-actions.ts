export const experimenterActions: Record<string, string> = {
  'shadow-e-1': 'Send a two-question email to every department head today: "What AI tools is your team using?" and "Does IT know about them?" Give them 48 hours to reply, then compile the answers into a shared spreadsheet — that is your AI inventory version one. You cannot govern what you have not found, and this survey typically surfaces five to ten tools that leadership never knew existed.',

  'shadow-e-2': 'Write and send one short all-hands email or Slack message this week stating the rule: "Before signing up for any new AI tool, check with your manager and IT first." Three sentences is enough — employees need to know the expectation exists before they can follow it. Without this signal, 65% of employees assume silence means permission, and shadow AI spreads unchecked.',

  'shadow-e-3': 'Open a shared document and list the AI tools IT or leadership already knows about — even if it\'s only two or three. Label it "Approved AI Tools" and share it with all managers today. A short approved list gives employees a default answer and removes the most common reason for shadow adoption: not knowing what they\'re allowed to use.',

  'shadow-e-4': 'Send a message to all employees stating: "When using AI tools for work, use your company email address to sign up, not your personal one." Follow it up with a note to IT asking them to flag any AI-tool signups that come through personal email addresses in the next 30 days. Personal account usage means your data is leaving the organization with zero visibility — if an employee pastes a customer contract into ChatGPT via their personal Gmail, you have no contractual protection whatsoever.',

  'shadow-e-5': 'Ask your finance team to pull every software expense coded as "SaaS," "subscription," or "software" from the last six months and flag anything AI-related. Then ask IT to search your email gateway logs or SaaS management tool (if you have one) for domains belonging to known AI vendors. Even this rough sweep gives you a first picture — most organizations discover three to five AI tools IT never approved.',

  'shadow-e-6': 'Put "AI tool adoption — what\'s happening in our org" on the agenda for the next leadership or management team meeting. Bring the informal list from your inventory efforts and present it as a ten-minute update — no formal deck required. Without leadership awareness, there is no budget, no mandate, and no accountability for AI governance — it simply won\'t happen.',

  'shadow-e-7': 'At your next all-hands or team meeting, ask employees directly: "Has anyone used an AI tool with customer names, financial data, or internal documents?" Treat the first honest answer as a win, not a violation. Then follow up with a brief written reminder: "Assume anything you type into a public AI tool can be read by the vendor — act accordingly." Real incidents have involved employees pasting client contracts and salary data into ChatGPT; finding out informally is better than finding out through a breach.',

  'shadow-e-8': 'Pick one person today — an IT manager, an operations lead, or a department head who is already interested in AI — and assign them a part-time role as your AI point person. Write it down in a short email: "You are now responsible for keeping a list of AI tools we use and flagging concerns." Without a named owner, every AI governance problem becomes someone else\'s problem, and nothing gets done.',

  'shadow-e-9': 'Log into the admin settings of Microsoft 365 (Copilot), Zoom (AI Companion), Slack (AI features), and Salesforce (Einstein) this week and check what AI features are enabled by default. Disable any that are processing data in ways you haven\'t reviewed yet. These embedded features are often turned on without IT\'s knowledge during a general software update, and they can start sending your internal communications and documents to vendor AI pipelines overnight.',

  'shadow-e-10': 'Draft a five-bullet "AI at work — what to know" message and send it to all employees this week. Cover: what AI tools are approved, what data you should never enter, who to ask before trying a new tool, that AI outputs need human review, and who to contact with concerns. One communication sent to all employees is worth more than ten informal conversations — only 18.5% of employees report knowing their company has any AI policy at all.',

  'vendor-e-1': 'Pull up your top ten software vendor list and spend 30 minutes searching each vendor\'s website for "AI features," "AI assistant," or "Copilot." Create a simple spreadsheet with three columns: Vendor | AI Feature Name | Currently Enabled (Yes/No/Unknown). Your AI footprint from embedded vendor features is almost certainly larger than you think — Salesforce, Microsoft, and Zoom alone have added dozens of AI capabilities in the past 18 months.',

  'vendor-e-2': 'Open the terms of service or data processing agreement for your two highest-risk vendors — the ones handling your customer data, financial records, or employee information. Search the document for the words "AI," "machine learning," "model training," and "improve our services." Highlight every clause that mentions how your data may be used. Many standard vendor contracts grant broad rights to use customer data for AI model training, and most organizations have signed these without reading them.',

  'vendor-e-3': 'Email your top three AI vendors this week with one direct question: "Is our data used to train your AI models, and if so, how do we opt out?" Log their responses in a spreadsheet. Most major vendors offer an opt-out — OpenAI, Google, and Microsoft all have enterprise data protection settings — but they require you to ask. If you have customer PII flowing through these tools and opt-out is available, failing to use it is a missed privacy control.',

  'vendor-e-4': 'Log into the admin console of your top two AI tools and navigate to Privacy Settings or Data Controls. Look for toggles labeled "use data to improve the product," "share data with third parties," or "AI training." Turn off any settings you did not consciously choose to enable. These settings default to maximum data sharing because it benefits the vendor, not you — a 20-minute review can meaningfully reduce your data exposure.',

  'vendor-e-5': 'Look up each of your top five vendors and find their data residency documentation — usually in their privacy policy or trust center. Note the country where your data is stored and processed and add it to your vendor spreadsheet. If you serve EU customers, any vendor storing data outside the EU without a valid transfer mechanism (like Standard Contractual Clauses) could expose you to GDPR liability regardless of where your company is based.',

  'vendor-e-6': 'Take your current vendor list and add a column: "What sensitive data does this vendor have access to?" Mark each vendor as High (PII, financial, health, IP), Medium (internal communications, HR), or Low (public content only). Focus your next 30 days of vendor review on your High tier. You cannot manage vendor AI risk evenly across 50 vendors — knowing which three carry 80% of the risk is what lets you prioritize.',

  'vendor-e-7': 'Go to each of your key AI vendors\' websites and search their Trust Center or Security page for "SOC 2" or "ISO 27001." Note whether they have a current certification and when it expires. A vendor without SOC 2 certification is a vendor that has not subjected their security practices to independent audit — this matters especially when they are handling your customer or employee data.',

  'vendor-e-8': 'Before your next software purchase, add one step to the process: have whoever is buying the tool spend 15 minutes answering three questions — "What data will this vendor access?", "Is their data handling policy acceptable?", and "Does IT approve?" Write these questions on a sticky note and put them in the purchase approval chain. Buying AI tools without any risk check is how organizations end up with vendors processing sensitive data under terms they never reviewed.',

  'vendor-e-9': 'Create a simple contact card for your top three vendors containing: the vendor\'s security breach notification contact (usually in their terms of service), your internal IT contact, and your legal or compliance contact. Store it somewhere everyone can find it — a shared drive folder or a pinned Slack message. The average time to identify a data breach is 194 days; knowing who to call on day one of a suspected incident cuts your response time and your liability significantly.',

  'vendor-e-10': 'Read the "data deletion" or "termination" section of your top two AI vendor contracts. Look for language about how long they retain your data after you cancel, whether they delete it on request, and whether deletion is guaranteed or just "reasonable efforts." Many vendors retain customer data for 30 to 90 days after offboarding — some longer — and some use aggregate data for model training indefinitely. Knowing this before you cancel protects you.',

  'data-e-1': 'Write a single sentence rule for employees: "Do not type customer names, customer contact details, financial figures, employee personal information, or confidential business documents into any AI tool." Send this in an email or Slack message to all staff today — it takes five minutes to write and can prevent a privacy breach worth tens of thousands in regulatory fines or lost contracts. Employees aren\'t deliberately misusing AI; they simply don\'t know what "sensitive" means in this context until you tell them.',

  'data-e-2': 'At your next all-staff meeting or via email, share one concrete example of how AI tools store and can expose data: "When you type something into ChatGPT on a free plan, OpenAI may use that text to train future models — your prompt becomes part of their dataset." One real example does more than a generic warning. Employees who understand the mechanism change their behavior; employees who hear only "be careful" do not.',

  'data-e-3': 'Send an email to all employees with this simple three-tier framework: "Public — anyone can see it (marketing copy, published prices). Internal — for employees only (meeting notes, internal reports). Confidential — must be protected (customer data, financials, employee records, legal documents)." Instruct them: "Only Public information can safely go into an AI tool without a review." A three-tier system takes 30 minutes to document and immediately gives employees a decision rule they can apply.',

  'data-e-4': 'Ask three or four employees directly this week: "Have you ever typed a customer\'s name or contact information into an AI tool for a work task?" If the answer is yes, treat it as an opportunity to set a clear rule, not a reason to punish. Follow up by clarifying the guideline in writing. In most organizations, this is already happening — the only question is whether you know about it and whether employees know it is a problem.',

  'data-e-5': 'Write down your current data retention practice in one paragraph: what data you keep, for how long, and who is responsible for deleting it when the time comes. Even if the practice is informal, writing it down is the first step to making it a policy. Organizations that cannot state their retention practices are also the organizations that struggle most with AI data governance — AI systems inherit data environment weaknesses, and the most common weakness is retaining sensitive data far longer than necessary.',

  'data-e-6': 'Pick your most-used AI tool and spend 30 minutes reading its privacy policy. Search specifically for: "how data is stored," "model training," "data sharing with third parties," and "data deletion." Write a three-sentence summary of what you found and share it with IT and your department heads. Most organizations that do this exercise discover at least one data handling practice they were not aware of — and that awareness drives better vendor selection and configuration decisions.',

  'data-e-7': 'Add one sentence to any AI guidance you send to employees, specifically targeting HR: "Do not use AI tools to process performance reviews, compensation data, employee medical information, or disciplinary records." If you have an HR team or an HR manager, brief them directly with a five-minute conversation. HR data exposed through AI tools is a double liability — it triggers both employment law and privacy law risk, and the individuals affected are your own employees.',

  'data-e-8': 'Send one message to all employees stating: "If you think AI has been used to handle data incorrectly, or if an AI tool gave you something that seems like it shouldn\'t be in that output, report it to [IT contact or manager name]." That is the entirety of the process you need right now. Employees who notice problems but have no reporting channel will do nothing — those who know where to go will act, and early reports prevent small data handling errors from compounding.',

  'data-e-9': 'Add one sentence to any AI communication you send to employees: "Before sending any AI-generated email, report, or document to anyone outside the company, read it yourself and confirm it\'s accurate and appropriate." Frame it as a quality step, not just a risk step. The most common AI governance failures happen when organizations skip the review step entirely — AI outputs have been sent to customers containing fabricated facts, competitor names, and internal pricing data.',

  'data-e-10': 'Ask your legal counsel or a trusted advisor one question this week: "Do we have any customers, employees, or business partners in the EU or California?" If the answer is yes, request a 30-minute overview of what GDPR or CCPA requires of your organization. You do not need to achieve full compliance this week, but you do need to know whether these laws apply — because "we didn\'t know" is not a defense regulators accept, and GDPR fines start at €10 million.',

  'security-e-1': 'Schedule a 30-minute call between IT and the team or person who manages your AI tool subscriptions. Ask IT to review the two highest-risk AI tools by checking: Does the vendor have SOC 2? What data does it access? How is access controlled? Write down the answers. This conversation doesn\'t require a formal audit framework — it requires IT to be in the room before the next tool gets purchased, not after.',

  'security-e-2': 'Spend 20 minutes with your IT or security lead discussing these three AI-specific risks: data leakage through prompts, prompt injection (where malicious content in AI inputs hijacks the AI\'s behavior), and training data exposure. Ask them: "Which of these could realistically happen with the tools we use today?" This conversation doesn\'t require technical expertise from you — it requires you to ask the question so your security team can give you an honest answer.',

  'security-e-3': 'Identify your industry and look up one sentence about the primary regulation that applies to your data: HIPAA for healthcare, PCI DSS for payment card handling, SOC 2 for SaaS companies, GDPR for EU data, CCPA for California consumer data. Then ask your legal or compliance contact: "Does our AI tool usage create any new risks under [that regulation]?" One question asked to the right person this week beats six months of unasked uncertainty.',

  'security-e-4': 'Send a short message to all managers: "Remind your team — use your company email address, not personal email, for any AI tool you use for work." Then ask IT to configure your email domain so that any AI tool sign-ups using a company email address trigger a notification to IT. Personal account usage is the most common way AI data controls are bypassed — the fix costs nothing and takes one email to implement.',

  'security-e-5': 'Send your leadership team a brief two-paragraph summary of three AI security risks: data leakage (employees pasting sensitive info into AI tools), deepfake phishing (AI-generated voice or video impersonating executives or vendors), and prompt injection (malicious content manipulating AI tool behavior). This doesn\'t need to be a formal presentation — a short email with three bullet points is enough. Leadership cannot approve security investment for risks they\'ve never heard of.',

  'security-e-6': 'If you do not have a written cybersecurity policy, write a one-page document this week covering: acceptable use of company systems, password requirements, how to report a security incident, and what data employees must protect. Dozens of free templates exist from organizations like SANS Institute (sans.org/information-security-policy) — you can adapt one in under two hours. AI security guidance has nothing to anchor to in an organization that has no baseline security policy.',

  'security-e-7': 'At your next team or management meeting, ask: "Has anyone had an experience in the last six months where an AI tool did something unexpected with data, or where you got a phishing attempt that seemed AI-generated?" Treat any yes answers as learning opportunities and write down what happened. Organizations that don\'t track incidents can\'t learn from them — and the ones that do get better at identifying threats before they become expensive.',

  'security-e-8': 'Add one scenario to your next all-staff communication or meeting: "Be aware that scammers are now using AI to create realistic voice recordings and video of executives asking for urgent wire transfers or login credentials. If you get a call or video message from a senior leader asking you to take an unusual financial or security action, call them back on a known number to verify." This warning takes two minutes to deliver and has prevented six-figure fraud at companies similar to yours.',

  'security-e-9': 'Email your cyber insurance broker this week with one question: "Does our current policy cover incidents caused by AI tool misuse — including data exposed through an employee using a third-party AI service?" Ask them to confirm in writing. Cyber insurance exclusions for AI-related incidents are increasing; knowing your coverage gap now costs nothing, while discovering it after an incident can mean an uncovered claim.',

  'security-e-10': 'Send one message to all employees: "If you think an AI tool has accessed, shared, or generated something it shouldn\'t have, contact [name] at [email or phone] immediately." Pin this contact information somewhere visible — the intranet, a Slack channel, or a physical poster in a common area. The first hour after an AI security incident is the most important — employees who don\'t know who to call will wait, and waiting makes every incident worse.',

  'airisks-e-1': 'Send a short message to all employees this week explaining AI hallucinations in plain language: "AI tools sometimes confidently state things that are completely false — they don\'t know they\'re wrong, and they won\'t warn you. Always verify important facts from an AI before acting on them or sharing them." Use a concrete example they\'ll recognize: "If you ask ChatGPT for a statistic or a legal reference, look it up independently before using it." Employees who don\'t know hallucinations exist will trust AI outputs they should be checking — this has led to organizations filing court documents with fabricated case citations.',

  'airisks-e-2': 'Add one rule to your next all-staff AI communication: "Before you send, publish, or submit anything written or researched with AI assistance, read it yourself from start to finish and confirm it\'s accurate." Make it practical: "This applies to emails, reports, proposals, and anything else going to someone outside your immediate team." Without this expectation in writing, the default behavior is to use AI output directly, and the first time that fails in a visible way it will be embarrassing — or worse.',

  'airisks-e-3': 'At your next management meeting, ask each manager: "Has anyone on your team had an AI tool give them information that turned out to be wrong, or generate content that caused an issue?" Collect the answers and write up a one-paragraph summary of what happened and what you learned. Even if the current answer is "not that we know of," creating this conversation means you\'ll hear about the next incident — and you won\'t be learning about AI errors through a customer complaint.',

  'airisks-e-4': 'Send a clear written rule to all employees today: "Any content generated by an AI tool that will be seen by customers — emails, proposals, support responses, marketing content — must be reviewed and approved by a human before it goes out." Name who is responsible for that review in each team. Unreviewed AI-generated customer communications have caused documented incidents including factually wrong product information, tone-deaf crisis responses, and leaked internal pricing — all of which became public.',

  'airisks-e-5': 'Send a targeted message to anyone in your organization involved in hiring, lending decisions, customer scoring, or support triage: "AI tools can produce biased recommendations, especially in decisions that affect people differently based on their background. Do not use AI to make final decisions in these areas — use it for research or drafting only, and have a human make the final call." Bias in AI-assisted hiring decisions has already resulted in employment discrimination lawsuits; the defense that "the AI decided" has not held up in court.',

  'airisks-e-6': 'Write a short guideline — one paragraph — and share it with all managers: "AI tools are for helping with research, drafting, and analysis. They are not for making final decisions on hiring, firing, pricing, legal matters, financial commitments, or anything that significantly affects a person or a major business outcome. A human must own every decision that matters." This doesn\'t require a formal policy document right now — it requires a clear statement that employees can refer back to.',

  'airisks-e-7': 'Send your legal counsel or IP advisor one question this week: "If an employee uses an AI tool to write code, marketing copy, or creative content, do we own it, and could it infringe on someone else\'s copyright?" Ask for a brief written answer. AI coding assistants have been shown to reproduce licensed code verbatim, and AI writing tools have reproduced copyrighted text — if that content ends up in a client deliverable or a published product, your organization may be liable.',

  'airisks-e-8': 'At your next all-staff meeting or via email, name a specific person employees can go to when an AI output seems wrong, suspicious, or potentially harmful: "If you get an AI output that looks factually wrong, seems biased, or gives you something unexpected — tell [name] or your manager immediately." Also tell them what qualifies: "Wrong facts, outputs that seem discriminatory, or anything you wouldn\'t feel comfortable sharing with a customer." Without a named escalation contact, employees either self-censor concerns or act on bad outputs without telling anyone.',

  'airisks-e-9': 'Ask your CEO or a senior leader to spend five minutes reviewing one real AI reputational incident from the past two years — Air Canada\'s chatbot inventing a refund policy and a court accepting it as binding is a widely reported example. Frame this as a business risk question, not a technology question: "Could something like this happen to us, and what would it cost us in customer trust?" Leadership engagement with a concrete example moves this from abstract concern to risk management action.',

  'airisks-e-10': 'Schedule a five-minute standing item in your monthly manager meeting: "Has anyone noticed AI tool misuse or an AI output that caused a problem this month?" That\'s it. No dashboard required, no technical monitoring infrastructure needed. A regular verbal check-in creates accountability and means problems surface through conversation instead of through a customer complaint or a legal notice.',

  'roi-e-1': 'Send a three-question survey to all employees who use AI tools at work: "What AI tool do you use most? Roughly how many hours per week does it save you? What task does it help most with?" Use a free Google Form or a Typeform. Compile the answers and you have your first time-savings dataset. Measuring "hours saved" is the single fastest way to build a business case for AI investment, and you can do it this week at zero cost.',

  'roi-e-2': 'Ask your finance team to pull all software subscription charges from the last three months and flag anything with "AI," "GPT," "Copilot," "Claude," or similar terms in the vendor name. Then ask department heads to send you a list of AI subscriptions they pay for from their own budgets or expense accounts. Adding both lists together gives you your first AI spend inventory. Most organizations doing this exercise for the first time find they are spending 30–50% more on AI tools than leadership thought.',

  'roi-e-3': 'Log into your organization\'s identity management or SaaS management tool (or ask IT to do this) and check the last-login date for every seat on your top two or three AI subscriptions. Flag any license where no one has logged in for 30 days or more — those are candidates for cancellation. If you don\'t have a SaaS management tool, ask each department head to confirm who on their team actually uses each AI tool they pay for. Unused AI licenses are one of the most common and easiest-to-eliminate cost items in a technology budget.',

  'roi-e-4': 'Schedule a 30-minute leadership conversation with one agenda item: "What specific business outcomes do we expect from our AI tools in the next 12 months?" Write down three to five specific outcomes — not "become more innovative" but "reduce time spent on first-draft content by 20%" or "cut vendor onboarding time from two weeks to one week." Expectations you have not named are expectations you cannot measure — and you cannot make investment decisions about AI without knowing what you expected to get.',

  'roi-e-5': 'Ask every manager to send you one sentence from a team member about how AI has helped their work this month. Compile the responses into a one-page internal memo titled "AI productivity wins this month" and circulate it to leadership. This takes one email and two hours to compile. These anecdotes are your evidence base before you have formal metrics — they also build employee motivation to keep using AI tools well, and they give leadership concrete examples to reference when making investment decisions.',

  'roi-e-6': 'Identify one repetitive process in your organization where AI is already being used — email drafting, data entry, report generation, customer response — and ask the team lead: "Before we used AI for this, how often did errors occur? Have you noticed a difference?" Write down the answer. Even one data point on error reduction is more useful than no data, and identifying the right process to measure is step one. Error reduction often delivers more value than time savings, but it\'s almost never measured.',

  'roi-e-7': 'Send a short email to your IT manager or CFO today: "Starting now, all AI tool subscriptions and purchases need to come through you for approval and tracking." This does not require a new system or a new budget — it requires one sentence establishing that AI spend has a home. Without a budget owner, AI subscriptions accumulate in individual expense reports, you have no aggregate view, and ROI accountability does not exist for anyone.',

  'roi-e-8': 'Ask your leadership team to spend 10 minutes answering one question: "Name one thing a competitor could do with AI in the next 12 months that would hurt our business." Use the answers to drive a brief discussion about your current pace of AI adoption. You don\'t need a formal competitive analysis framework — you need your leadership team to understand that AI adoption is not just an internal efficiency question but a competitive positioning question, and that doing nothing is also a choice with consequences.',

  'roi-e-9': 'Pick one AI use case in your organization and calculate a simple number: (hours saved per week) × (average hourly cost of that employee) × 52 weeks. If three employees each save two hours per week using AI for drafting, and their average cost is $40 per hour, that\'s $12,480 per year from one use case. Write that number down. Even a rough estimate creates a reference point — organizations with no ROI number make AI investment decisions based on gut feel, and gut feel rarely wins budget arguments.',

  'roi-e-10': 'In your next customer satisfaction survey or customer conversation, add one question: "Has the speed or quality of our responses improved recently?" If you have a support team using AI, track their response time before and after AI adoption — most ticketing systems can produce this report in minutes. Customer-facing AI is often where the ROI is clearest, but it requires you to track the right metric before and after the change, not just after.',
};
