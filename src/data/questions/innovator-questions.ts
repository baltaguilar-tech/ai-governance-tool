// =============================================================================
// INNOVATOR BANK — 60 questions
// For orgs with formal AI governance programs and dedicated AI teams.
// Focus: program maturity, compliance depth, systematic risk management.
// =============================================================================

import { AssessmentQuestion } from '@/types/assessment';

export const INNOVATOR_QUESTIONS: AssessmentQuestion[] = [
  // ---------------------------------------------------------------------------
  // SHADOW AI — shadow-i-1 to shadow-i-10
  // ---------------------------------------------------------------------------
  {
    id: 'shadow-i-1',
    dimension: 'shadowAI',
    text: 'How does your organization discover AI tools deployed outside of formal procurement channels?',
    helpText: 'Automated discovery significantly reduces the lag between tool adoption and governance visibility. Manual-only approaches create blind spots even in mature programs.',
    type: 'radio',
    options: [
      { label: 'Automated scanning tools continuously monitor our environment for new AI services and alert governance teams in real time', value: 0, riskLevel: 'low' },
      { label: 'Regular manual audits combined with employee self-reporting on a defined quarterly schedule', value: 25, riskLevel: 'medium' },
      { label: 'Periodic audits exist but cadence is inconsistent and results are not systematically tracked', value: 50, riskLevel: 'high' },
      { label: 'We rely mainly on IT tickets or manager escalations to surface unauthorized AI tools', value: 75, riskLevel: 'high' },
      { label: 'No systematic discovery process exists despite having a formal AI governance program', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'shadow-i-2',
    dimension: 'shadowAI',
    text: 'What percentage of AI tools in active use across your organization are captured in your official AI inventory?',
    helpText: 'Inventory completeness is a direct indicator of governance program reach. Mature programs use automated tooling to close coverage gaps.',
    type: 'radio',
    options: [
      { label: 'Greater than 95% — automated discovery and mandatory registration keep our inventory current', value: 0, riskLevel: 'low' },
      { label: '80–95% — regular audits catch most tools, with a defined reconciliation process', value: 25, riskLevel: 'medium' },
      { label: '60–79% — known gaps exist but are tracked and remediation is in progress', value: 50, riskLevel: 'high' },
      { label: 'Less than 60% — significant portions of AI usage are unaccounted for', value: 75, riskLevel: 'high' },
      { label: 'We do not measure inventory completeness or have no reliable inventory', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'shadow-i-3',
    dimension: 'shadowAI',
    text: 'How does your organization enforce AI procurement and approval policy for new tool adoption?',
    helpText: 'Policy enforcement mechanisms — not just policy existence — determine whether governance programs have real teeth. Technical controls outperform awareness campaigns.',
    type: 'radio',
    options: [
      { label: 'Technical controls (network-level blocks, browser policies, purchasing system gates) enforce approval requirements automatically', value: 0, riskLevel: 'low' },
      { label: 'Mandatory approval workflow with documented exceptions process and periodic compliance checks', value: 25, riskLevel: 'medium' },
      { label: 'Approval process exists but technical controls are absent — enforcement depends on employee compliance', value: 50, riskLevel: 'high' },
      { label: 'Policy is documented but enforcement is inconsistent and exceptions are rarely reviewed', value: 75, riskLevel: 'high' },
      { label: 'No enforcement mechanism exists — policy is aspirational only', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
  },
  {
    id: 'shadow-i-4',
    dimension: 'shadowAI',
    text: 'How does your organization track and analyze incidents involving unauthorized AI tool usage?',
    helpText: 'Shadow AI incident tracking feeds risk pattern analysis and informs policy updates. Mature programs use incidents as governance learning opportunities, not just disciplinary triggers.',
    type: 'radio',
    options: [
      { label: 'Dedicated incident log with root cause analysis, trend reporting, and quarterly governance review', value: 0, riskLevel: 'low' },
      { label: 'Incidents are logged and reviewed but trend analysis is informal or infrequent', value: 25, riskLevel: 'medium' },
      { label: 'Some incidents are recorded but tracking is inconsistent and no trend analysis occurs', value: 50, riskLevel: 'high' },
      { label: 'Incidents are handled individually with no systematic tracking or follow-up', value: 75, riskLevel: 'high' },
      { label: 'No shadow AI incident tracking exists despite a formal governance program', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'shadow-i-5',
    dimension: 'shadowAI',
    text: 'How deeply is AI governance integrated into your organization\'s procurement workflow for new software and services?',
    helpText: 'Upstream procurement integration is far more effective than after-the-fact auditing. Governance gates at contract or purchase-order stage prevent shadow AI before it starts.',
    type: 'radio',
    options: [
      { label: 'AI governance review is a mandatory gate in all software procurement workflows, with automated flagging of AI-enabled tools', value: 0, riskLevel: 'low' },
      { label: 'AI governance review is required for known AI tools but relies on requester disclosure for embedded AI features', value: 25, riskLevel: 'medium' },
      { label: 'AI review is recommended but not mandatory — approval path does not block purchase', value: 50, riskLevel: 'high' },
      { label: 'AI governance team is occasionally consulted but has no formal procurement role', value: 75, riskLevel: 'high' },
      { label: 'Procurement has no AI governance integration despite a formal governance program', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'shadow-i-6',
    dimension: 'shadowAI',
    text: 'How are compliance exceptions for unapproved AI tools managed within your governance program?',
    helpText: 'A well-defined exceptions process allows legitimate business needs while maintaining audit trails. Programs without exceptions governance often see policy erosion over time.',
    type: 'radio',
    options: [
      { label: 'Formal exceptions process with documented risk acceptance, time-limited approvals, compensating controls, and periodic renewal reviews', value: 0, riskLevel: 'low' },
      { label: 'Exceptions are approved and logged, with defined owners, but renewal reviews are inconsistent', value: 25, riskLevel: 'medium' },
      { label: 'Exceptions are granted informally with limited documentation and no expiration tracking', value: 50, riskLevel: 'high' },
      { label: 'Exceptions are handled ad hoc with no consistent process or audit trail', value: 75, riskLevel: 'high' },
      { label: 'No exceptions process exists — unapproved tools are either blocked outright or ignored', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'shadow-i-7',
    dimension: 'shadowAI',
    text: 'What channels does your organization provide for employees to report AI tools they are using or have discovered?',
    helpText: 'Accessible, non-punitive reporting channels increase voluntary disclosure and reduce the gap between actual and inventoried AI usage.',
    type: 'radio',
    options: [
      { label: 'Multiple reporting channels available (portal, Slack bot, manager path); non-punitive reporting policy documented and communicated to employees', value: 0, riskLevel: 'low' },
      { label: 'A designated reporting channel exists and is included in AI policy communications', value: 25, riskLevel: 'medium' },
      { label: 'Employees can report through general IT channels but no AI-specific path is promoted', value: 50, riskLevel: 'high' },
      { label: 'Reporting is theoretically possible but no channel is clearly designated or communicated', value: 75, riskLevel: 'high' },
      { label: 'No reporting mechanism exists — employees have no clear path to disclose AI tool usage', value: 100, riskLevel: 'critical' },
    ],
    weight: 0.9,
  },
  {
    id: 'shadow-i-8',
    dimension: 'shadowAI',
    text: 'How does your organization use usage analytics to monitor AI tool adoption patterns across teams and departments?',
    helpText: 'Usage analytics dashboards surface adoption concentration risks, detect policy circumvention patterns, and support data-driven governance decisions.',
    type: 'radio',
    options: [
      { label: 'Automated dashboards track AI tool usage by team, volume, data types accessed, and flag anomalies for governance review', value: 0, riskLevel: 'low' },
      { label: 'Usage data is collected and reviewed regularly, with manual analysis to identify outliers', value: 25, riskLevel: 'medium' },
      { label: 'Some usage data is available but is not consistently analyzed or reported to governance stakeholders', value: 50, riskLevel: 'high' },
      { label: 'Usage analytics exist for approved tools only — shadow tools generate no governance-visible signals', value: 75, riskLevel: 'high' },
      { label: 'No usage analytics are used for AI governance monitoring', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'shadow-i-9',
    dimension: 'shadowAI',
    text: 'What process governs the decommissioning of AI tools that are no longer approved or in active use?',
    helpText: 'Decommissioning procedures prevent data persistence risks and ensure data processed by discontinued tools is properly handled. Zombie AI deployments are a common audit finding.',
    type: 'radio',
    options: [
      { label: 'Defined decommissioning checklist including data deletion, access revocation, vendor notification, and inventory update — tracked to completion', value: 0, riskLevel: 'low' },
      { label: 'Decommissioning steps are documented and followed for most tools, with sign-off required from data and security teams', value: 25, riskLevel: 'medium' },
      { label: 'Decommissioning is handled informally — some steps may be skipped and no formal sign-off is required', value: 50, riskLevel: 'high' },
      { label: 'Tools are abandoned rather than formally decommissioned — data and access cleanup is unreliable', value: 75, riskLevel: 'high' },
      { label: 'No decommissioning process exists for AI tools', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'shadow-i-10',
    dimension: 'shadowAI',
    text: 'How does your AI governance program maintain visibility across all business units and teams using AI tools?',
    helpText: 'Cross-team AI visibility requires both technical mechanisms and organizational coordination. Siloed governance programs often have blind spots in business units with high technical autonomy.',
    type: 'radio',
    options: [
      { label: 'Embedded AI governance liaisons in each major business unit, supported by centralized tooling and a unified inventory', value: 0, riskLevel: 'low' },
      { label: 'Regular cross-functional reporting from business units to a central governance function, with defined escalation paths', value: 25, riskLevel: 'medium' },
      { label: 'Central governance team periodically surveys business units but coverage and frequency are inconsistent', value: 50, riskLevel: 'high' },
      { label: 'Governance visibility is primarily limited to centrally managed tools — business unit deployments are largely untracked', value: 75, riskLevel: 'high' },
      { label: 'No cross-team visibility mechanism exists — governance operates in silos', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },

  // ---------------------------------------------------------------------------
  // VENDOR RISK — vendor-i-1 to vendor-i-10
  // ---------------------------------------------------------------------------
  {
    id: 'vendor-i-1',
    dimension: 'vendorRisk',
    text: 'How comprehensive are the AI-specific due diligence questionnaires your organization uses when evaluating new AI vendors?',
    helpText: 'Generic vendor questionnaires often miss AI-specific risks such as training data provenance, model update processes, and algorithmic accountability. AI-specific questionnaires are a governance maturity marker.',
    type: 'radio',
    options: [
      { label: 'AI-specific questionnaire covering model provenance, training data, bias testing, update notification, and regulatory compliance — reviewed and updated annually', value: 0, riskLevel: 'low' },
      { label: 'AI-specific questionnaire exists and is used consistently, though it may not cover all risk dimensions', value: 25, riskLevel: 'medium' },
      { label: 'AI questions are appended to a generic vendor questionnaire but are not systematically scored or tracked', value: 50, riskLevel: 'high' },
      { label: 'AI vendors complete the same questionnaire as all other vendors with no AI-specific questions', value: 75, riskLevel: 'high' },
      { label: 'No structured due diligence questionnaire is used for AI vendors', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'vendor-i-2',
    dimension: 'vendorRisk',
    text: 'How comprehensively do your AI vendor contracts address data ownership, audit rights, and model update notification obligations?',
    helpText: 'Contractual AI provisions establish enforceable governance rights. Model update notifications are particularly important as silent model changes can invalidate prior risk assessments.',
    type: 'radio',
    options: [
      { label: 'All AI vendor contracts include explicit clauses for data ownership, right-to-audit, model change notification windows, and liability for AI-caused harm', value: 0, riskLevel: 'low' },
      { label: 'Most AI contracts include these provisions; a contract review process ensures new agreements meet the standard', value: 25, riskLevel: 'medium' },
      { label: 'Some AI contracts include these provisions but coverage is inconsistent and legacy contracts may lack them', value: 50, riskLevel: 'high' },
      { label: 'Data ownership is typically addressed but audit rights and model change notification are rarely negotiated', value: 75, riskLevel: 'high' },
      { label: 'AI-specific contractual provisions are absent from vendor agreements', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'vendor-i-3',
    dimension: 'vendorRisk',
    text: 'How does your organization conduct security assessments specifically for AI vendor components?',
    helpText: 'AI systems introduce attack surfaces — model inversion, adversarial inputs, training data extraction — that standard vendor security assessments may not cover.',
    type: 'radio',
    options: [
      { label: 'Dedicated AI security assessment framework applied to all AI vendors, including model-specific threat vectors, reviewed at least annually', value: 0, riskLevel: 'low' },
      { label: 'AI vendors receive enhanced security assessments that include some AI-specific risk areas beyond standard controls', value: 25, riskLevel: 'medium' },
      { label: 'AI vendors complete standard security questionnaires with no AI-specific additions', value: 50, riskLevel: 'high' },
      { label: 'Security assessments are conducted only for vendors above a certain spend threshold — smaller AI vendors are not assessed', value: 75, riskLevel: 'high' },
      { label: 'No security assessment process exists for AI vendors', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'vendor-i-4',
    dimension: 'vendorRisk',
    text: 'How does your organization track the provenance and lineage of third-party AI models used in production systems?',
    helpText: 'Third-party model lineage tracking is increasingly required for regulatory compliance under the EU AI Act and supports incident investigation when model behavior is unexpected.',
    type: 'radio',
    options: [
      { label: 'Model registry captures provenance (source, version, training data summary, known limitations) for all third-party models in production', value: 0, riskLevel: 'low' },
      { label: 'Key third-party models are documented with provenance information, though coverage of all models is incomplete', value: 25, riskLevel: 'medium' },
      { label: 'Model provenance is captured informally in documentation that is not consistently maintained or centralized', value: 50, riskLevel: 'high' },
      { label: 'Vendor-provided documentation is retained but no internal lineage tracking is maintained', value: 75, riskLevel: 'high' },
      { label: 'No third-party model lineage tracking exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
    jurisdictions: ['eu'],
  },
  {
    id: 'vendor-i-5',
    dimension: 'vendorRisk',
    text: 'How does your organization monitor SLA compliance for AI-specific performance commitments from vendors?',
    helpText: 'AI system SLAs should cover not just uptime but model accuracy degradation, bias drift, and latency — metrics that standard IT SLAs rarely capture.',
    type: 'radio',
    options: [
      { label: 'Automated monitoring tracks AI-specific SLA metrics (accuracy, latency, fairness thresholds) with escalation triggers and vendor accountability reporting', value: 0, riskLevel: 'low' },
      { label: 'SLA compliance is reviewed regularly with vendors, though monitoring focuses primarily on availability rather than model performance', value: 25, riskLevel: 'medium' },
      { label: 'SLA compliance is reviewed reactively — vendor performance is assessed when issues arise rather than proactively', value: 50, riskLevel: 'high' },
      { label: 'AI vendor SLAs are defined but monitoring tools are absent — compliance is self-reported by vendors', value: 75, riskLevel: 'high' },
      { label: 'AI-specific SLA monitoring does not exist', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'vendor-i-6',
    dimension: 'vendorRisk',
    text: 'How precisely do your AI vendor contracts specify incident notification obligations — including timelines, required content, and escalation paths?',
    helpText: 'Precise contractual notification requirements ensure vendor SLAs align with your own regulatory reporting deadlines. Vague or absent notification clauses leave you dependent on vendor discretion when AI-related incidents occur and regulators ask what you knew and when.',
    type: 'radio',
    options: [
      { label: 'Contracts specify maximum notification windows (e.g., 24–72 hours), required content, communication channels, and post-incident reporting obligations', value: 0, riskLevel: 'low' },
      { label: 'Incident notification requirements are included in contracts but timelines or content requirements may be imprecise', value: 25, riskLevel: 'medium' },
      { label: 'Contracts reference incident notification obligations but specifics are left to vendor discretion', value: 50, riskLevel: 'high' },
      { label: 'Incident notification is addressed only in general vendor terms, not in AI-specific contract provisions', value: 75, riskLevel: 'high' },
      { label: 'No incident notification requirements are contractually specified for AI vendors', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'vendor-i-7',
    dimension: 'vendorRisk',
    text: 'How does your organization assess and manage AI vendor concentration risk?',
    helpText: 'Reliance on a single AI vendor or model family for critical functions creates operational and competitive risk. Mature programs maintain a diversification strategy for key AI capabilities.',
    type: 'radio',
    options: [
      { label: 'Vendor concentration risk is formally assessed, with documented thresholds, diversification targets, and contingency plans for critical AI capabilities', value: 0, riskLevel: 'low' },
      { label: 'Concentration risk is reviewed periodically and single-vendor dependencies for critical functions are flagged for mitigation', value: 25, riskLevel: 'medium' },
      { label: 'Concentration risk is acknowledged but no formal assessment or mitigation planning exists', value: 50, riskLevel: 'high' },
      { label: 'Vendor diversity is considered during procurement but concentration risk is not tracked after initial purchase', value: 75, riskLevel: 'high' },
      { label: 'AI vendor concentration risk is not assessed or managed', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'vendor-i-8',
    dimension: 'vendorRisk',
    text: 'How does your organization oversee sub-processors and fourth-party AI components used by your AI vendors?',
    helpText: 'AI vendors frequently use sub-processors for infrastructure, data labeling, or model components. Sub-processor risk can flow directly into your regulatory and reputational exposure.',
    type: 'radio',
    options: [
      { label: 'Contracts require vendor disclosure of all AI sub-processors, with change notification obligations and the right to object — regularly reviewed', value: 0, riskLevel: 'low' },
      { label: 'Sub-processor lists are requested and reviewed at contract initiation, with change notification required', value: 25, riskLevel: 'medium' },
      { label: 'Sub-processor disclosure is requested but not consistently obtained or tracked over time', value: 50, riskLevel: 'high' },
      { label: 'Sub-processors are not specifically addressed in AI vendor contracts', value: 75, riskLevel: 'high' },
      { label: 'No sub-processor oversight exists for AI vendors', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'vendor-i-9',
    dimension: 'vendorRisk',
    text: 'How does your organization conduct ongoing performance monitoring for AI vendors beyond initial due diligence?',
    helpText: 'AI vendor performance monitoring must be continuous — model quality, compliance posture, and vendor financial health can all change materially after initial onboarding.',
    type: 'radio',
    options: [
      { label: 'Structured annual reviews plus continuous automated monitoring for performance, compliance, and security indicators with defined escalation thresholds', value: 0, riskLevel: 'low' },
      { label: 'Annual vendor reviews are conducted with defined scoring criteria and improvement tracking', value: 25, riskLevel: 'medium' },
      { label: 'Vendor reviews occur but are inconsistent in frequency and depth — not all AI vendors are covered', value: 50, riskLevel: 'high' },
      { label: 'Ongoing monitoring is reactive — vendors are reviewed only when issues arise or contracts come up for renewal', value: 75, riskLevel: 'high' },
      { label: 'No ongoing AI vendor performance monitoring program exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'vendor-i-10',
    dimension: 'vendorRisk',
    text: 'How mature is your organization\'s AI vendor exit planning for critical AI capabilities?',
    helpText: 'Exit planning ensures business continuity if a vendor relationship ends unexpectedly. For AI, this includes data portability, model replication feasibility, and transition timelines.',
    type: 'radio',
    options: [
      { label: 'Documented exit plans for all critical AI vendors including data portability requirements, transition timelines, interim alternatives, and tested runbooks', value: 0, riskLevel: 'low' },
      { label: 'Exit plans exist for the most critical AI vendors, with data portability addressed contractually', value: 25, riskLevel: 'medium' },
      { label: 'Exit planning is considered at contract negotiation but plans are not maintained or tested', value: 50, riskLevel: 'high' },
      { label: 'Exit planning is acknowledged as a gap but no action has been taken', value: 75, riskLevel: 'high' },
      { label: 'No AI vendor exit planning exists despite reliance on critical AI vendors', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },

  // ---------------------------------------------------------------------------
  // DATA GOVERNANCE — data-i-1 to data-i-10
  // ---------------------------------------------------------------------------
  {
    id: 'data-i-1',
    dimension: 'dataGovernance',
    text: 'How does your organization classify training data used in AI models relative to your broader data classification policy?',
    helpText: 'AI training data classification determines permissible use, retention requirements, and cross-border transfer restrictions. Many organizations apply general data classification but miss AI-specific risk dimensions like re-identification risk in aggregated datasets.',
    type: 'radio',
    options: [
      { label: 'AI training data classification policy extends general data classification with AI-specific categories (e.g., model-sensitive, re-identification risk) and is enforced at data pipeline entry points', value: 0, riskLevel: 'low' },
      { label: 'Training data is classified using the organization\'s general data classification framework with documented AI use case applicability', value: 25, riskLevel: 'medium' },
      { label: 'Training data classification is inconsistent — some datasets are classified, others are not, with no systematic enforcement', value: 50, riskLevel: 'high' },
      { label: 'Training data is classified on an ad hoc basis without a formal policy or governance review', value: 75, riskLevel: 'high' },
      { label: 'No training data classification policy or practice exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk', 'us'],
  },
  {
    id: 'data-i-2',
    dimension: 'dataGovernance',
    text: 'How does your organization maintain and audit consent records for personal data used in AI processing?',
    helpText: 'Consent audit trails for AI use cases must capture not just initial consent but the specific AI processing purposes authorized. Repurposing data for AI training without valid consent is a leading GDPR enforcement risk.',
    type: 'radio',
    options: [
      { label: 'Automated consent management system maintains audit trails by data subject, purpose, and AI use case — regularly audited for validity and coverage', value: 0, riskLevel: 'low' },
      { label: 'Consent records are maintained and periodically reviewed, with AI processing purposes documented in consent frameworks', value: 25, riskLevel: 'medium' },
      { label: 'Consent is obtained for primary data collection but AI-specific processing purposes may not be explicitly captured', value: 50, riskLevel: 'high' },
      { label: 'Consent management is handled by legal team but AI processing purposes are not separately tracked or audited', value: 75, riskLevel: 'high' },
      { label: 'No consent audit trail exists for AI use cases despite processing personal data', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'data-i-3',
    dimension: 'dataGovernance',
    text: 'How does your organization enforce data minimization principles for AI training and inference workloads?',
    helpText: 'Data minimization for AI is particularly challenging because richer datasets often improve model performance. Mature programs balance model quality objectives against data minimization requirements through documented policy.',
    type: 'radio',
    options: [
      { label: 'Data minimization requirements are applied at AI project inception with documented justification for each data element, reviewed by data governance team before model training begins', value: 0, riskLevel: 'low' },
      { label: 'Data minimization is considered during AI project design and documented in data processing impact assessments', value: 25, riskLevel: 'medium' },
      { label: 'Data minimization is referenced in policy but not consistently applied or verified at the AI project level', value: 50, riskLevel: 'high' },
      { label: 'Data minimization is left to individual team judgment with no governance review or documentation requirement', value: 75, riskLevel: 'high' },
      { label: 'No data minimization controls or requirements apply to AI workloads', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'data-i-4',
    dimension: 'dataGovernance',
    text: 'How does your organization manage cross-border data transfers specifically for AI processing workloads?',
    helpText: 'AI processing often involves cross-border data flows to cloud providers, model vendors, and third-party APIs. These flows require specific transfer mechanisms under GDPR and equivalent frameworks.',
    type: 'radio',
    options: [
      { label: 'All cross-border AI data flows are mapped, covered by appropriate transfer mechanisms (SCCs, adequacy decisions, BCRs), and reviewed at least annually', value: 0, riskLevel: 'low' },
      { label: 'Cross-border transfers for primary AI workloads are covered, with periodic review of transfer mechanisms', value: 25, riskLevel: 'medium' },
      { label: 'Major AI data flows are addressed but coverage of third-party APIs and sub-processors is incomplete', value: 50, riskLevel: 'high' },
      { label: 'Cross-border transfer controls exist for general IT systems but AI-specific data flows are not systematically reviewed', value: 75, riskLevel: 'high' },
      { label: 'Cross-border data transfers for AI processing are not managed or assessed', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
    jurisdictions: ['eu', 'uk'],
  },
  {
    id: 'data-i-5',
    dimension: 'dataGovernance',
    text: 'How does your organization govern the use of synthetic data in AI model development?',
    helpText: 'Synthetic data offers privacy benefits but introduces its own governance questions around generation methodology, bias inheritance from source data, and validation requirements before use in production models.',
    type: 'radio',
    options: [
      { label: 'Synthetic data governance policy covers generation methodology approval, source data lineage, bias inheritance assessment, and validation requirements before use in training', value: 0, riskLevel: 'low' },
      { label: 'Guidelines exist for synthetic data use in AI, with documentation of generation methods and validation testing required', value: 25, riskLevel: 'medium' },
      { label: 'Synthetic data is used in AI projects but without formal governance — quality and bias risks are managed informally', value: 50, riskLevel: 'high' },
      { label: 'Synthetic data use is not distinguished from other training data in governance processes', value: 75, riskLevel: 'high' },
      { label: 'No synthetic data governance policy exists for AI development', value: 100, riskLevel: 'critical' },
    ],
    weight: 0.9,
  },
  {
    id: 'data-i-6',
    dimension: 'dataGovernance',
    text: 'How comprehensive is your organization\'s data lineage documentation for AI model training datasets?',
    helpText: 'Data lineage for AI training enables root-cause analysis when model behavior is unexpected, supports regulatory audits, and is increasingly required under the EU AI Act for high-risk systems.',
    type: 'radio',
    options: [
      { label: 'Automated data lineage tracking captures source systems, transformations, and versions for all production model training datasets, queryable on demand', value: 0, riskLevel: 'low' },
      { label: 'Data lineage is documented for key AI models, with source systems and major transformations captured in a maintained catalog', value: 25, riskLevel: 'medium' },
      { label: 'Data lineage documentation exists for some models but is incomplete and not consistently maintained', value: 50, riskLevel: 'high' },
      { label: 'Data lineage is captured informally in project documentation with no centralized catalog or automated tracking', value: 75, riskLevel: 'high' },
      { label: 'No data lineage documentation exists for AI training datasets', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
    jurisdictions: ['eu'],
  },
  {
    id: 'data-i-7',
    dimension: 'dataGovernance',
    text: 'How does your organization manage retention and deletion obligations for data generated by AI system outputs?',
    helpText: 'AI output data — model predictions, generated content, inference logs — carries distinct retention and deletion obligations, particularly when outputs involve personal data or high-stakes decisions.',
    type: 'radio',
    options: [
      { label: 'AI output retention policy defines retention periods by output type and risk level, with automated deletion workflows and audit trails for compliance', value: 0, riskLevel: 'low' },
      { label: 'Retention periods for AI outputs are defined in data management policy and applied to primary production systems', value: 25, riskLevel: 'medium' },
      { label: 'Retention guidelines exist but AI outputs are not consistently classified or subject to automated deletion enforcement', value: 50, riskLevel: 'high' },
      { label: 'AI outputs are subject to general data retention policies without AI-specific treatment', value: 75, riskLevel: 'high' },
      { label: 'No specific retention or deletion policy applies to AI output data', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'data-i-8',
    dimension: 'dataGovernance',
    text: 'How does your organization monitor and manage data quality for AI input data in production systems?',
    helpText: 'Data quality degradation in production inputs is a leading cause of model performance drift. Monitoring input distributions against training baselines enables early detection before downstream harm occurs.',
    type: 'radio',
    options: [
      { label: 'Automated data quality monitoring tracks input distributions against training baselines, with alerting thresholds that trigger governance review before model outputs are affected', value: 0, riskLevel: 'low' },
      { label: 'Data quality checks run on AI inputs on a defined schedule, with manual review of flagged anomalies', value: 25, riskLevel: 'medium' },
      { label: 'Data quality checks exist for source systems but are not specifically tuned to AI input requirements or drift detection', value: 50, riskLevel: 'high' },
      { label: 'Data quality is monitored informally — issues surface through model performance degradation rather than input monitoring', value: 75, riskLevel: 'high' },
      { label: 'No data quality monitoring exists specifically for AI input data', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'data-i-9',
    dimension: 'dataGovernance',
    text: 'How does your organization govern the feature engineering process in AI model development to ensure consistency and auditability?',
    helpText: 'Feature engineering governance prevents undocumented transformations from introducing bias or compliance gaps. A feature store with documented lineage supports both model reproducibility and regulatory audit readiness.',
    type: 'radio',
    options: [
      { label: 'Feature store with documented transformations, version control, bias screening requirements, and mandatory review before features enter production training pipelines', value: 0, riskLevel: 'low' },
      { label: 'Feature engineering is documented and version-controlled, with review requirements for features derived from sensitive data', value: 25, riskLevel: 'medium' },
      { label: 'Feature engineering documentation exists but is inconsistently maintained and not subject to formal review', value: 50, riskLevel: 'high' },
      { label: 'Feature engineering is tracked within individual project documentation with no centralized governance', value: 75, riskLevel: 'high' },
      { label: 'No feature engineering governance exists — transformations are undocumented and unreviewed', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'data-i-10',
    dimension: 'dataGovernance',
    text: 'How does your organization enforce access controls for data used in AI training and inference pipelines?',
    helpText: 'AI pipelines often aggregate data from multiple sources, creating elevated access risk. Least-privilege access controls specifically designed for AI pipeline service accounts are a governance maturity indicator.',
    type: 'radio',
    options: [
      { label: 'Least-privilege access controls are enforced for all AI pipeline service accounts, reviewed quarterly, with access logging enabling forensic investigation', value: 0, riskLevel: 'low' },
      { label: 'Role-based access controls are applied to AI pipelines, with documented access justification and periodic reviews', value: 25, riskLevel: 'medium' },
      { label: 'Access controls exist for underlying data stores but are not specifically designed or reviewed for AI pipeline access patterns', value: 50, riskLevel: 'high' },
      { label: 'AI pipelines use broadly permissioned service accounts without role-specific access controls or regular reviews', value: 75, riskLevel: 'high' },
      { label: 'No specific access controls or monitoring exist for AI pipeline data access', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },

  // ---------------------------------------------------------------------------
  // SECURITY & COMPLIANCE — security-i-1 to security-i-10
  // ---------------------------------------------------------------------------
  {
    id: 'security-i-1',
    dimension: 'securityCompliance',
    text: 'How does your organization conduct AI-specific threat modeling for AI systems before and during production deployment?',
    helpText: 'AI systems introduce threat vectors — model extraction, membership inference, adversarial inputs — that standard IT threat modeling frameworks do not adequately cover. AI-specific threat modeling is a prerequisite for meaningful security controls.',
    type: 'radio',
    options: [
      { label: 'Mandatory AI-specific threat modeling (covering model-layer attacks, data pipeline threats, and inference risks) is completed before production deployment and updated at major model changes', value: 0, riskLevel: 'low' },
      { label: 'AI threat modeling is conducted for high-risk AI systems, with documented findings and mitigations tracked to closure', value: 25, riskLevel: 'medium' },
      { label: 'Standard IT threat modeling is applied to AI systems but AI-specific attack vectors are not systematically addressed', value: 50, riskLevel: 'high' },
      { label: 'Threat modeling is conducted for AI systems but on an ad hoc basis without a structured AI-specific methodology', value: 75, riskLevel: 'high' },
      { label: 'No AI-specific threat modeling exists for production AI systems', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
  },
  {
    id: 'security-i-2',
    dimension: 'securityCompliance',
    text: 'How does your organization test AI models for adversarial inputs, prompt injection, and model-specific security vulnerabilities?',
    helpText: 'Model security testing requires dedicated techniques beyond standard application penetration testing. Prompt injection in LLM-based systems and adversarial robustness testing for ML models are now considered baseline security hygiene for AI governance programs.',
    type: 'radio',
    options: [
      { label: 'Structured model security testing program includes adversarial robustness evaluation, prompt injection testing, and model extraction resistance — conducted before production and after significant updates', value: 0, riskLevel: 'low' },
      { label: 'Model security testing is conducted for high-risk AI applications, with findings documented and remediation tracked', value: 25, riskLevel: 'medium' },
      { label: 'Some ad hoc testing for known attack types is performed but no structured program or coverage standards exist', value: 50, riskLevel: 'high' },
      { label: 'Security testing of AI models is limited to the underlying infrastructure — model-layer vulnerabilities are not specifically tested', value: 75, riskLevel: 'high' },
      { label: 'No model-specific security testing exists for production AI systems', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'security-i-3',
    dimension: 'securityCompliance',
    text: 'How frequently and thoroughly does your organization conduct regulatory compliance gap assessments for AI systems?',
    helpText: 'As AI regulations evolve rapidly — EU AI Act, national implementations, sector-specific rules — point-in-time gap assessments quickly become outdated. Mature programs conduct structured gap reviews on a defined cadence.',
    type: 'radio',
    options: [
      { label: 'Formal gap assessments conducted semi-annually, with continuous monitoring for regulatory changes and a tracked remediation roadmap reviewed by executive stakeholders', value: 0, riskLevel: 'low' },
      { label: 'Annual compliance gap assessments with documented findings and a remediation plan reviewed by the governance committee', value: 25, riskLevel: 'medium' },
      { label: 'Compliance gap assessments are conducted but on an irregular basis or only when triggered by a specific regulatory event', value: 50, riskLevel: 'high' },
      { label: 'Compliance posture is assessed informally by legal counsel without a structured gap analysis methodology', value: 75, riskLevel: 'high' },
      { label: 'No formal AI regulatory compliance gap assessments are conducted', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
    jurisdictions: ['eu'],
  },
  {
    id: 'security-i-4',
    dimension: 'securityCompliance',
    text: 'How prepared is your organization to produce audit evidence for AI governance controls on short notice?',
    helpText: 'Audit evidence readiness determines how efficiently your organization can respond to regulatory inquiries, customer due diligence requests, and certification audits. Programs that assemble evidence reactively face significant operational risk.',
    type: 'radio',
    options: [
      { label: 'Continuous evidence collection is automated — governance controls generate audit artifacts in real time, with an evidence repository maintained and tested through regular dry runs', value: 0, riskLevel: 'low' },
      { label: 'Evidence collection procedures are documented and tested at least annually, with clear ownership for each control area', value: 25, riskLevel: 'medium' },
      { label: 'Evidence can be assembled but requires significant manual effort — no pre-positioned evidence repository exists', value: 50, riskLevel: 'high' },
      { label: 'Evidence assembly is reactive and depends on individual team members with no standardized procedures', value: 75, riskLevel: 'high' },
      { label: 'No audit evidence readiness program exists for AI governance controls', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'security-i-5',
    dimension: 'securityCompliance',
    text: 'How explicitly are your AI governance controls mapped to EU AI Act requirements and ISO 42001 controls?',
    helpText: 'Explicit control mapping to EU AI Act and ISO 42001 enables gap identification, audit efficiency, and demonstrates systematic compliance intent to regulators and customers.',
    type: 'radio',
    options: [
      { label: 'Complete control mapping exists in a maintained matrix linking each internal control to EU AI Act articles and ISO 42001 clauses, with control owners and evidence references', value: 0, riskLevel: 'low' },
      { label: 'Control mapping covers the primary requirements of EU AI Act and ISO 42001 with documented gaps and a remediation timeline', value: 25, riskLevel: 'medium' },
      { label: 'Partial mapping exists for the most significant requirements but coverage is incomplete and not regularly maintained', value: 50, riskLevel: 'high' },
      { label: 'Mapping has been started but is not maintained and does not cover all relevant requirements', value: 75, riskLevel: 'high' },
      { label: 'No control mapping to EU AI Act or ISO 42001 exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
    jurisdictions: ['eu'],
  },
  {
    id: 'security-i-6',
    dimension: 'securityCompliance',
    text: 'How comprehensive is security logging for AI inference operations in production systems?',
    helpText: 'AI inference logging captures the inputs and outputs that feed forensic investigation, bias auditing, and regulatory inquiry responses. Insufficient logging is a frequent finding in AI security assessments.',
    type: 'radio',
    options: [
      { label: 'Comprehensive inference logging captures inputs, outputs, timestamps, user context, and model version — with tamper-evident storage, defined retention, and regular access review', value: 0, riskLevel: 'low' },
      { label: 'Inference logging is implemented for production AI systems with defined retention periods and access controls', value: 25, riskLevel: 'medium' },
      { label: 'Some inference logging exists but coverage, retention, and access controls are inconsistent across AI systems', value: 50, riskLevel: 'high' },
      { label: 'Standard application logging captures AI system activity but is not specifically designed for AI inference forensic requirements', value: 75, riskLevel: 'high' },
      { label: 'No dedicated security logging exists for AI inference operations', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'security-i-7',
    dimension: 'securityCompliance',
    text: 'How does your organization manage privileged access to AI systems, models, and training infrastructure?',
    helpText: 'Privileged access to AI systems — including model weights, training pipelines, and inference infrastructure — represents a high-value attack surface. Just-in-time access and comprehensive audit trails are governance best practices.',
    type: 'radio',
    options: [
      { label: 'Just-in-time privileged access with full session recording, mandatory justification, automated expiration, and quarterly access reviews for all AI infrastructure', value: 0, riskLevel: 'low' },
      { label: 'Privileged access to AI systems is controlled through a PAM solution with access logging and periodic reviews', value: 25, riskLevel: 'medium' },
      { label: 'Role-based access controls are in place but privileged access is not specifically managed through a PAM solution for AI systems', value: 50, riskLevel: 'high' },
      { label: 'AI infrastructure uses shared administrative accounts or infrequently reviewed access rights', value: 75, riskLevel: 'high' },
      { label: 'No privileged access management specific to AI systems exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'security-i-8',
    dimension: 'securityCompliance',
    text: 'How does your organization manage a compliance calendar for AI-specific regulatory obligations and certification deadlines?',
    helpText: 'AI compliance timelines — EU AI Act conformity assessments, ISO 42001 surveillance audits, sector-specific deadlines — require proactive calendar management to avoid last-minute remediation efforts.',
    type: 'radio',
    options: [
      { label: 'Dedicated AI compliance calendar with all regulatory deadlines, certification renewals, and internal review cycles — owned by a named role with executive visibility', value: 0, riskLevel: 'low' },
      { label: 'AI compliance obligations are tracked in an organizational compliance calendar with defined owners and advance notice triggers', value: 25, riskLevel: 'medium' },
      { label: 'AI compliance deadlines are tracked informally or included in a general IT compliance calendar without AI-specific ownership', value: 50, riskLevel: 'high' },
      { label: 'Compliance deadlines are managed reactively — tracked only when a specific regulatory event prompts action', value: 75, riskLevel: 'high' },
      { label: 'No compliance calendar or tracking process exists for AI-specific regulatory obligations', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
    jurisdictions: ['eu'],
  },
  {
    id: 'security-i-9',
    dimension: 'securityCompliance',
    text: 'How does your organization monitor for regulatory changes that may affect your AI governance obligations?',
    helpText: 'The AI regulatory landscape is changing faster than most compliance programs can track. Systematic regulatory change monitoring prevents surprise compliance gaps and enables proactive program adjustment.',
    type: 'radio',
    options: [
      { label: 'Dedicated regulatory monitoring process with assigned owners, curated sources covering relevant jurisdictions, and a defined workflow for assessing and routing regulatory changes to governance owners', value: 0, riskLevel: 'low' },
      { label: 'Regulatory monitoring is conducted through legal counsel and governance team subscriptions with quarterly briefings to the AI governance committee', value: 25, riskLevel: 'medium' },
      { label: 'Regulatory changes are monitored informally — awareness depends on individual team members following news and publications', value: 50, riskLevel: 'high' },
      { label: 'Regulatory monitoring is outsourced entirely to external counsel with no internal ownership or systematic tracking', value: 75, riskLevel: 'high' },
      { label: 'No regulatory change monitoring process exists for AI-related obligations', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'security-i-10',
    dimension: 'securityCompliance',
    text: 'How developed are your AI-specific security incident response playbooks?',
    helpText: 'AI security incidents — model poisoning, data extraction, adversarial attacks — require specialized response procedures that differ from standard cybersecurity incident response. Generic IR playbooks leave AI-specific scenarios unaddressed.',
    type: 'radio',
    options: [
      { label: 'Dedicated AI security incident response playbooks cover major AI attack scenarios, are tested through tabletop exercises at least annually, and are integrated with the broader IR program', value: 0, riskLevel: 'low' },
      { label: 'AI-specific sections exist within the broader IR playbook, with defined escalation paths and response procedures for common AI security incidents', value: 25, riskLevel: 'medium' },
      { label: 'AI incidents are addressed through general IR procedures with informal adaptations for AI-specific scenarios', value: 50, riskLevel: 'high' },
      { label: 'Standard IR playbooks apply to AI systems with no AI-specific modifications or testing', value: 75, riskLevel: 'high' },
      { label: 'No IR playbooks address AI-specific security incidents', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },

  // ---------------------------------------------------------------------------
  // AI RISKS — airisk-i-1 to airisk-i-10
  // ---------------------------------------------------------------------------
  {
    id: 'airisk-i-1',
    dimension: 'aiSpecificRisks',
    text: 'How systematic and frequent is your organization\'s bias auditing process for production AI models?',
    helpText: 'Bias auditing cadence must match deployment risk — high-stakes models (hiring, lending, healthcare) require more frequent audits. Methodology should cover both statistical measures and real-world outcome disparities.',
    type: 'radio',
    options: [
      { label: 'Structured bias auditing program with documented methodology, risk-tiered cadence (quarterly for high-risk, semi-annually for medium-risk), independent review, and findings reported to executive stakeholders', value: 0, riskLevel: 'low' },
      { label: 'Bias audits are conducted on a defined schedule for production models, with documented methodology and remediation tracking', value: 25, riskLevel: 'medium' },
      { label: 'Bias testing occurs at model launch but ongoing auditing cadence is undefined or inconsistently applied', value: 50, riskLevel: 'high' },
      { label: 'Bias is evaluated informally at model development with no post-deployment auditing program', value: 75, riskLevel: 'high' },
      { label: 'No bias auditing program exists despite operating AI systems that make or influence consequential decisions', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
    jurisdictions: ['eu', 'us'],
  },
  {
    id: 'airisk-i-2',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization detect and respond to model performance drift in production AI systems?',
    helpText: 'Model drift — where model performance degrades as real-world data distributions diverge from training data — is one of the most common causes of AI governance failures. Automated alerting before drift reaches a harmful threshold is the governance standard.',
    type: 'radio',
    options: [
      { label: 'Automated drift detection monitors performance and input distribution metrics continuously, with threshold-based alerts triggering governance review before model outputs reach a harmful error rate', value: 0, riskLevel: 'low' },
      { label: 'Drift monitoring runs on a defined schedule with manual review of metrics and a documented process for triggering model retraining or retirement', value: 25, riskLevel: 'medium' },
      { label: 'Model performance is monitored but drift-specific metrics and alert thresholds are not formally defined', value: 50, riskLevel: 'high' },
      { label: 'Drift is detected reactively through downstream performance complaints rather than proactive monitoring', value: 75, riskLevel: 'high' },
      { label: 'No model drift detection or monitoring program exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'airisk-i-3',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization define and enforce explainability requirements for AI systems based on their use case and risk level?',
    helpText: 'Explainability requirements should be calibrated to the stakes of the decision — a content recommendation engine requires less rigorous explainability than a credit decisioning model. Mature programs define explainability standards by risk tier.',
    type: 'radio',
    options: [
      { label: 'Explainability requirements are defined by use case risk tier, technically implemented (e.g., SHAP, LIME, or attention mechanisms), and tested against regulatory standards before deployment', value: 0, riskLevel: 'low' },
      { label: 'Explainability standards are documented by risk tier and implemented for high-risk use cases with periodic validation', value: 25, riskLevel: 'medium' },
      { label: 'Explainability is considered during model selection but requirements are not formalized by risk tier or consistently validated', value: 50, riskLevel: 'high' },
      { label: 'Explainability is addressed informally — model developers make individual judgments without governance standards', value: 75, riskLevel: 'high' },
      { label: 'No explainability requirements or standards exist for AI systems', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
    jurisdictions: ['eu'],
  },
  {
    id: 'airisk-i-4',
    dimension: 'aiSpecificRisks',
    text: 'How mature are your organization\'s AI-specific incident response procedures?',
    helpText: 'AI incidents — including model errors causing harm, unexpected model behavior, and AI-enabled fraud — require specialized response procedures covering impact assessment, rollback decisions, and regulatory notification obligations.',
    type: 'radio',
    options: [
      { label: 'Documented AI incident response procedures cover detection, containment, impact assessment, rollback decision criteria, regulatory notification thresholds, and post-incident review — tested annually', value: 0, riskLevel: 'low' },
      { label: 'AI incident response procedures are documented and integrated with the broader IR program, with defined ownership for AI-specific response steps', value: 25, riskLevel: 'medium' },
      { label: 'AI incidents are handled through general IT incident response procedures with informal adaptations — AI-specific steps are not formally documented', value: 50, riskLevel: 'high' },
      { label: 'Incident response procedures exist but do not address AI-specific scenarios or regulatory notification requirements for AI-caused harm', value: 75, riskLevel: 'high' },
      { label: 'No AI-specific incident response procedures exist', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
  },
  {
    id: 'airisk-i-5',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization maintain and govern a registry of high-risk AI use cases?',
    helpText: 'A high-risk AI use case registry enables risk-tiered governance — ensuring that the highest-stakes AI deployments receive proportionate oversight, testing, and documentation requirements.',
    type: 'radio',
    options: [
      { label: 'Maintained high-risk AI use case registry with risk classification criteria, documented rationale, governance requirements by tier, and mandatory review before new high-risk deployments', value: 0, riskLevel: 'low' },
      { label: 'High-risk AI use cases are identified and documented, with enhanced governance requirements applied and periodic registry reviews', value: 25, riskLevel: 'medium' },
      { label: 'High-risk use cases are recognized informally but no formal registry or consistent risk classification criteria exist', value: 50, riskLevel: 'high' },
      { label: 'Risk classification is applied at project initiation but the registry is not maintained as systems evolve', value: 75, riskLevel: 'high' },
      { label: 'No high-risk AI use case registry or classification program exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
    jurisdictions: ['eu'],
  },
  {
    id: 'airisk-i-6',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization implement and verify human oversight mechanisms for consequential AI decisions?',
    helpText: 'Human oversight requirements under EU AI Act and ethical AI frameworks must be technically implemented — not just policy-stated. Oversight mechanisms should be verified to be functioning and not routinely bypassed under operational pressure.',
    type: 'radio',
    options: [
      { label: 'Human oversight is technically enforced for high-risk decisions, with documentation of override rates, mandatory reviewer qualification, and audit logging of all human review actions', value: 0, riskLevel: 'low' },
      { label: 'Human review is required for high-risk AI decisions, with defined reviewer roles, documented processes, and periodic audits of override rates', value: 25, riskLevel: 'medium' },
      { label: 'Human review processes exist but are inconsistently applied — override rates and reviewer qualifications are not monitored', value: 50, riskLevel: 'high' },
      { label: 'Human oversight is policy-mandated but not technically enforced — compliance depends on individual reviewer behavior', value: 75, riskLevel: 'high' },
      { label: 'Human oversight mechanisms for consequential AI decisions do not exist or have not been implemented', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.4,
    jurisdictions: ['eu'],
  },
  {
    id: 'airisk-i-7',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization conduct ethics reviews for new AI use cases before deployment?',
    helpText: 'An AI ethics review process provides a structured check against organizational values and ethical frameworks before deployment, separate from legal compliance review. Mature programs involve cross-functional reviewers, not just technical or legal teams.',
    type: 'radio',
    options: [
      { label: 'Formal AI ethics review process with a cross-functional review panel, documented criteria, mandatory pre-deployment gate for high-risk use cases, and a public-facing ethics statement', value: 0, riskLevel: 'low' },
      { label: 'AI ethics review is conducted for new use cases with documented findings and a defined escalation path for borderline cases', value: 25, riskLevel: 'medium' },
      { label: 'Ethics considerations are part of AI project documentation but no formal review process or panel exists', value: 50, riskLevel: 'high' },
      { label: 'Ethics review is conducted informally by individual team members or delegated entirely to legal counsel', value: 75, riskLevel: 'high' },
      { label: 'No ethics review process exists for AI use cases', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'airisk-i-8',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization define, measure, and track fairness metrics for AI systems that affect people?',
    helpText: 'Fairness metrics must be explicitly defined — demographic parity, equalized odds, and calibration can produce conflicting results. Mature programs select and justify fairness metrics based on the specific use case and affected populations.',
    type: 'radio',
    options: [
      { label: 'Fairness metrics are explicitly selected with documented justification for each use case, measured in production, reported to governance stakeholders, and reviewed when metrics approach threshold values', value: 0, riskLevel: 'low' },
      { label: 'Fairness metrics are defined for high-risk AI use cases and monitored in production with a documented review process', value: 25, riskLevel: 'medium' },
      { label: 'Fairness metrics are used during model development but are not tracked consistently in production systems', value: 50, riskLevel: 'high' },
      { label: 'Fairness is evaluated informally without defined metrics or thresholds', value: 75, riskLevel: 'high' },
      { label: 'No fairness metrics are defined or tracked for AI systems affecting individuals', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.3,
    jurisdictions: ['eu', 'us'],
  },
  {
    id: 'airisk-i-9',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization manage model versioning and rollback capabilities for production AI systems?',
    helpText: 'Model versioning and rollback capability is a critical operational risk control. Without it, responding to a model failure, bias discovery, or security incident may require extended downtime or continued harm during remediation.',
    type: 'radio',
    options: [
      { label: 'All production models are versioned with immutable artifacts, rollback is tested as part of deployment procedures, and rollback criteria are pre-defined before each deployment', value: 0, riskLevel: 'low' },
      { label: 'Model versioning is implemented and rollback capability is documented, with rollback tested periodically', value: 25, riskLevel: 'medium' },
      { label: 'Model versioning exists but rollback procedures are not consistently documented or tested', value: 50, riskLevel: 'high' },
      { label: 'Versioning is used for code but model artifacts and weights are not systematically versioned or retained for rollback', value: 75, riskLevel: 'high' },
      { label: 'No model versioning or rollback capability exists for production AI systems', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'airisk-i-10',
    dimension: 'aiSpecificRisks',
    text: 'How does your organization commission and act on third-party AI risk assessments for high-risk AI systems?',
    helpText: 'Third-party AI risk assessments provide independent validation of internal risk management and are increasingly required for regulatory compliance, customer trust, and high-stakes AI procurement. Acting on findings distinguishes mature programs from checkbox exercises.',
    type: 'radio',
    options: [
      { label: 'Third-party assessments are conducted on a defined cadence for high-risk systems, findings are tracked to remediation, and assessment results are reported to the board or executive AI governance body', value: 0, riskLevel: 'low' },
      { label: 'Third-party assessments are conducted for major AI systems, with documented findings and a remediation tracking process', value: 25, riskLevel: 'medium' },
      { label: 'Third-party assessments have been conducted but are not on a regular cadence and findings tracking is informal', value: 50, riskLevel: 'high' },
      { label: 'Third-party assessments have been discussed but not yet commissioned for current production AI systems', value: 75, riskLevel: 'high' },
      { label: 'No third-party AI risk assessments have been conducted', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },

  // ---------------------------------------------------------------------------
  // ROI TRACKING — roi-i-1 to roi-i-10
  // ---------------------------------------------------------------------------
  {
    id: 'roi-i-1',
    dimension: 'roiTracking',
    text: 'How formalized is your organization\'s framework for measuring ROI across AI projects?',
    helpText: 'A structured ROI measurement framework ensures consistent evaluation across projects and enables portfolio-level investment decisions. Ad hoc ROI claims without a framework are difficult to validate or compare.',
    type: 'radio',
    options: [
      { label: 'Documented ROI measurement framework with standardized benefit categories, calculation methodology, measurement periods, and approval by finance and AI governance — applied consistently to all AI investments', value: 0, riskLevel: 'low' },
      { label: 'ROI framework exists and is applied to most AI projects, with defined benefit categories and a standard calculation approach reviewed by finance', value: 25, riskLevel: 'medium' },
      { label: 'ROI is measured for major AI projects using informal or project-specific approaches that are not consistent across the portfolio', value: 50, riskLevel: 'high' },
      { label: 'ROI is reported for AI projects but calculations are not standardized and are rarely independently validated', value: 75, riskLevel: 'high' },
      { label: 'No formal ROI measurement framework exists for AI investments despite operating a formal AI program', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.2,
  },
  {
    id: 'roi-i-2',
    dimension: 'roiTracking',
    text: 'How does your organization track whether anticipated benefits from AI projects are actually realized over time?',
    helpText: 'Benefit realization tracking closes the loop between AI investment decisions and actual outcomes. Without it, organizations cannot distinguish high-performing AI investments from underperformers, leading to poor capital allocation.',
    type: 'radio',
    options: [
      { label: 'Structured benefit realization process with pre-defined benefit metrics, baseline measurements, milestone reviews at 3, 6, and 12 months post-deployment, and variance analysis reported to investment governance', value: 0, riskLevel: 'low' },
      { label: 'Benefit realization is tracked for major AI projects with defined metrics and periodic check-ins against projected benefits', value: 25, riskLevel: 'medium' },
      { label: 'Benefits are measured post-deployment but tracking cadence and methodology are inconsistent across projects', value: 50, riskLevel: 'high' },
      { label: 'Post-deployment benefit measurement is attempted for some projects but is not systematically tied to the original business case', value: 75, riskLevel: 'high' },
      { label: 'No benefit realization tracking exists — AI project outcomes are not systematically measured after deployment', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'roi-i-3',
    dimension: 'roiTracking',
    text: 'How comprehensively does your organization attribute costs to AI projects, including compute, licensing, and human oversight expenses?',
    helpText: 'Accurate AI cost attribution must capture not just direct costs (compute, licenses) but often-ignored indirect costs such as human oversight labor, data preparation, and model maintenance. Without comprehensive cost tracking, ROI calculations systematically overstate returns.',
    type: 'radio',
    options: [
      { label: 'Total cost of ownership model captures compute, licensing, data costs, human oversight labor, model maintenance, and governance overhead — allocated by project with finance validation', value: 0, riskLevel: 'low' },
      { label: 'Cost attribution covers direct costs and major indirect costs for AI projects, with a defined overhead allocation methodology', value: 25, riskLevel: 'medium' },
      { label: 'Direct costs are tracked but indirect costs (human oversight, data preparation, governance) are not consistently attributed to AI projects', value: 50, riskLevel: 'high' },
      { label: 'Cost tracking focuses on infrastructure and licensing — human and operational costs are absorbed in department budgets without attribution', value: 75, riskLevel: 'high' },
      { label: 'No structured cost attribution methodology exists for AI projects', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'roi-i-4',
    dimension: 'roiTracking',
    text: 'How systematically does your organization collect productivity metrics attributable to AI deployments?',
    helpText: 'Productivity metrics must be collected with methodology rigor — isolating AI contribution from other changes requires baseline periods, control groups or comparable reference points, and consistent measurement instruments.',
    type: 'radio',
    options: [
      { label: 'Pre/post productivity measurement with defined metrics, baseline period, control group or reference comparators, and statistical confidence methodology — validated by an independent function', value: 0, riskLevel: 'low' },
      { label: 'Productivity metrics are collected before and after AI deployment, with documented methodology and review by business stakeholders', value: 25, riskLevel: 'medium' },
      { label: 'Productivity impact is measured but methodology lacks rigor — baselines or control comparators are absent', value: 50, riskLevel: 'high' },
      { label: 'Productivity gains are self-reported by teams without standardized measurement instruments', value: 75, riskLevel: 'high' },
      { label: 'No systematic productivity metrics collection exists for AI deployments', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'roi-i-5',
    dimension: 'roiTracking',
    text: 'How does your organization incorporate risk-adjusted ROI calculations for AI projects?',
    helpText: 'Risk-adjusted ROI accounts for the probability and cost of AI governance failures, regulatory penalties, reputational damage, and model performance risks. Unadjusted ROI calculations systematically underestimate the true cost of AI investment.',
    type: 'radio',
    options: [
      { label: 'Risk-adjusted ROI methodology incorporates governance risk factors (regulatory penalty probability, bias incident costs, model failure scenarios) with actuarial inputs reviewed by risk management', value: 0, riskLevel: 'low' },
      { label: 'Risk-adjusted calculations are used for high-risk AI projects, with documented risk factors and adjustment methodology', value: 25, riskLevel: 'medium' },
      { label: 'Risk is qualitatively referenced in ROI discussions but not quantitatively incorporated into return calculations', value: 50, riskLevel: 'high' },
      { label: 'ROI calculations are unadjusted — risk factors are considered separately in risk documentation but not reflected in investment metrics', value: 75, riskLevel: 'high' },
      { label: 'No risk adjustment exists in AI ROI calculations', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.1,
  },
  {
    id: 'roi-i-6',
    dimension: 'roiTracking',
    text: 'How does your organization maintain a portfolio view of AI investments and returns?',
    helpText: 'Portfolio-level visibility enables cross-project comparisons, resource reallocation decisions, and governance of cumulative AI risk exposure. Project-level ROI without portfolio synthesis limits strategic decision-making.',
    type: 'radio',
    options: [
      { label: 'Active AI investment portfolio dashboard maintained by finance and AI governance, with cross-project ROI comparisons, risk concentrations, and investment reallocation recommendations reviewed quarterly', value: 0, riskLevel: 'low' },
      { label: 'Portfolio view of AI investments is maintained and reviewed at defined intervals, with cross-project ROI comparisons available to executive stakeholders', value: 25, riskLevel: 'medium' },
      { label: 'AI investment data exists but portfolio-level synthesis is done on an ad hoc basis, not maintained as a live management tool', value: 50, riskLevel: 'high' },
      { label: 'Individual project ROI is tracked but no portfolio view is assembled or reviewed by leadership', value: 75, riskLevel: 'high' },
      { label: 'No portfolio view of AI investments and returns exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'roi-i-7',
    dimension: 'roiTracking',
    text: 'How consistently does your organization conduct post-implementation reviews for completed AI projects?',
    helpText: 'Post-implementation reviews generate institutional learning about AI investment outcomes, closing the feedback loop between expected and actual results. Programs without PIRs repeat the same estimation errors across projects.',
    type: 'radio',
    options: [
      { label: 'Mandatory post-implementation reviews are conducted at 6 and 12 months for all significant AI projects, with findings shared across the AI program and incorporated into future investment criteria', value: 0, riskLevel: 'low' },
      { label: 'Post-implementation reviews are conducted for major AI projects, with findings documented and shared with the AI governance function', value: 25, riskLevel: 'medium' },
      { label: 'Post-implementation reviews occur for some projects but are not mandatory and coverage is inconsistent', value: 50, riskLevel: 'high' },
      { label: 'Lessons learned are collected informally without a structured review process or systematic sharing', value: 75, riskLevel: 'high' },
      { label: 'No post-implementation reviews are conducted for AI projects', value: 100, riskLevel: 'critical' },
    ],
    weight: 0.9,
  },
  {
    id: 'roi-i-8',
    dimension: 'roiTracking',
    text: 'How regularly and formally does your organization report AI value realization to executive and board-level stakeholders?',
    helpText: 'Value realization reporting cadence signals how seriously AI ROI is treated as a governance accountability measure. Regular executive reporting creates accountability for AI investment performance and supports informed resource allocation.',
    type: 'radio',
    options: [
      { label: 'Quarterly AI ROI reporting to executive leadership and annual board-level reporting, with standardized formats, trend analysis, and explicit governance risk metrics alongside financial returns', value: 0, riskLevel: 'low' },
      { label: 'Regular (at least semi-annual) AI value reporting to executive stakeholders with documented ROI metrics and progress against targets', value: 25, riskLevel: 'medium' },
      { label: 'AI ROI reporting occurs annually or when specifically requested — not on a proactive, defined cadence', value: 50, riskLevel: 'high' },
      { label: 'AI value reporting is bundled with general IT reporting without AI-specific metrics or governance accountability', value: 75, riskLevel: 'high' },
      { label: 'No formal AI value realization reporting exists for executive or board stakeholders', value: 100, riskLevel: 'critical' },
    ],
    weight: 1.0,
  },
  {
    id: 'roi-i-9',
    dimension: 'roiTracking',
    text: 'How does your organization track the opportunity cost of AI governance investments versus foregone AI deployment speed?',
    helpText: 'Governance overhead represents a real cost that must be weighed against risk reduction value. Mature programs quantify governance cost as a percentage of AI project cost and benchmark it against the value of prevented incidents.',
    type: 'radio',
    options: [
      { label: 'Governance overhead is measured as a percentage of project cost, benchmarked against industry data, and reported alongside ROI metrics to demonstrate governance investment value', value: 0, riskLevel: 'low' },
      { label: 'Governance costs are tracked and qualitatively referenced in investment cases, with periodic review of governance process efficiency', value: 25, riskLevel: 'medium' },
      { label: 'Governance costs are tracked in budget but are not specifically analyzed in relation to risk reduction or deployment speed trade-offs', value: 50, riskLevel: 'high' },
      { label: 'Governance costs are absorbed in overhead budgets with no specific tracking or opportunity cost analysis', value: 75, riskLevel: 'high' },
      { label: 'No tracking of governance investment costs or opportunity cost analysis exists', value: 100, riskLevel: 'critical' },
    ],
    weight: 0.8,
  },
  {
    id: 'roi-i-10',
    dimension: 'roiTracking',
    text: 'How does your organization benchmark its AI ROI metrics against industry peers and published benchmarks?',
    helpText: 'Industry benchmarking contextualizes internal AI ROI data and enables identification of underperforming investments that may appear successful in isolation. It also provides board-level credibility for AI investment cases.',
    type: 'radio',
    options: [
      { label: 'Systematic benchmarking using industry research (Gartner, McKinsey, IDC AI benchmarks) incorporated into annual AI portfolio reviews, with explicit comparison to sector-specific productivity and ROI norms', value: 0, riskLevel: 'low' },
      { label: 'External benchmarks are referenced periodically in AI investment reviews, with documented comparison to peer organizations where data is available', value: 25, riskLevel: 'medium' },
      { label: 'Benchmarking is conducted informally — individual team members reference industry reports without a systematic process', value: 50, riskLevel: 'high' },
      { label: 'External benchmarks are not specifically applied to AI ROI — general IT benchmarking may be used without AI-specific calibration', value: 75, riskLevel: 'high' },
      { label: 'No AI ROI benchmarking against industry peers or published standards is conducted', value: 100, riskLevel: 'critical' },
    ],
    weight: 0.9,
  },
];
