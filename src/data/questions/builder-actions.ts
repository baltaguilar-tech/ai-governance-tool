export const builderActions: Record<string, string> = {
  'shadow-b-1': 'Formalize your AI tool inventory by converting whatever list you have — spreadsheet, wiki page, or mental model — into a structured registry with four required fields: tool name, owning team, data it accesses, and approval status. Assign one owner to maintain it and set a calendar reminder to review it every quarter. An incomplete or informal inventory gives leadership false confidence that AI adoption is under control when it almost certainly is not.',

  'shadow-b-2': 'Document your approval process as a one-page workflow with a simple intake form — tool name, intended use, data it will access, requester, approving manager — and publish it on your intranet. Send a company-wide message linking to it and specifying that it applies to all AI tools immediately. Without a written, findable process, adoption will always outpace governance because employees default to "it is probably fine" when no process is visible.',

  'shadow-b-3': 'Take your draft or informally distributed AI acceptable use policy and finalize it with three specific additions: a list of prohibited data types, a clear statement covering free tools, and an acknowledgment signature block. Distribute it through your HR system or onboarding platform to collect dated acknowledgments. A policy employees have not formally acknowledged is not a policy — it is a document that provides no legal baseline and no basis for enforcement.',

  'shadow-b-4': 'Assign a quarterly AI inventory review to one named owner and put the first review on the calendar today — block 90 minutes and use a checklist: tool name, data accessed, last reviewed date, risk tier, and whether the vendor has changed its terms in the past 90 days. Add a step to your standard IT governance cycle so it is not treated as optional. AI vendors add new data-sharing features and model training defaults at update time; a registry that is reviewed once a year or never will miss changes that could expose your organization.',

  'shadow-b-5': 'Work with IT to configure your identity provider (Okta, Azure AD, Google Workspace) to flag new OAuth authorizations and SaaS sign-ups using company email addresses, and set up a weekly alert review for any new AI tool registrations. For tools not yet in your SSO, create a 30-day remediation plan to bring them under centralized access control or remove them. Employees adopting AI tools outside your identity provider create access you cannot revoke during offboarding, cannot audit, and cannot inventory without manual discovery.',

  'shadow-b-6': 'Update your AI approval policy to explicitly state that ALL AI tools require approval before use, regardless of cost, and add "free AI tools" as a specific example in the policy text so there is no ambiguity. Send a targeted communication to the teams most likely to be using free tools — marketing, customer success, operations — with a link to the intake form and a 30-day amnesty period for self-reporting. Free tools are where shadow AI starts: an employee can put customer data into ChatGPT, Perplexity, or Notion AI without spending a cent, and that data may be logged, retained, and used for model training.',

  'shadow-b-7': 'Add a mandatory procurement step requiring both legal and IT security to sign off on all AI vendor agreements before access is provisioned, and create a one-page "AI vendor review checklist" covering data training opt-out, DPA requirement, liability cap, and IP ownership. Apply this retroactively to your top five AI vendors this quarter. AI vendor contracts regularly include clauses allowing vendors to use your data for model training and limiting their liability for breaches to a fraction of annual contract value — clauses that are trivial to negotiate before signing but binding after.',

  'shadow-b-8': 'Run a 2-week discovery exercise: send a short survey to department heads asking them to list every AI tool their team uses and what data it accesses, then compare responses against your existing inventory to find gaps. Prioritize marketing, HR, legal, and finance — the four departments where AI adoption and data sensitivity tend to be highest. You cannot govern what you cannot see, and concentration of sensitive-data AI usage in one department can become your highest-risk exposure point overnight.',

  'shadow-b-9': 'Add a consequences clause to your AI acceptable use policy — at minimum, a written warning for first offense and manager escalation for repeat violations — and communicate it explicitly in your next all-hands or team meeting rather than burying it in policy text. Work with HR to confirm the consequences are consistent with your existing disciplinary framework. Policy without stated, communicated consequences is routinely ignored; the act of communicating that consequences exist changes behavior even when enforcement is rare.',

  'shadow-b-10': 'Assign someone in IT to subscribe to the release notes and admin newsletters for Microsoft 365, Google Workspace, and Salesforce — your three highest-probability sources of embedded AI activations — and create a simple review gate: any new AI feature that touches company data must be reviewed before it is enabled tenant-wide. Block 30 minutes monthly to check pending feature releases. Microsoft Copilot, Google Duet AI, and Salesforce Einstein features are being activated on existing licenses with minimal notice; without a review gate, these activations bypass your entire AI intake process.',

  'vendor-b-1': 'Formalize your security review by creating a standard AI vendor security questionnaire — 10 to 15 questions covering encryption in transit and at rest, access controls, breach notification timelines, third-party audit certifications, and data deletion processes — and make completion a required step before any new AI vendor is provisioned. Start by running your current top five vendors through the questionnaire retroactively. Onboarding a vendor without a security review means you have accepted unknown risk; when a breach occurs, you will have no documented basis for due diligence.',

  'vendor-b-2': 'Audit your current AI vendor contracts this month: for each vendor that touches any company or customer data, confirm whether a data processing agreement is in place and request one immediately from any vendor that does not have one. Make DPA execution a non-negotiable gate in your procurement workflow going forward. Under GDPR, operating without a DPA for any vendor processing personal data is a direct regulatory violation — not just a best-practice gap — with fines up to €20M or 4% of global turnover.',

  'vendor-b-3': 'Pull the privacy policy and terms of service for every AI vendor you use and search for the sections covering model training, data improvement, and feedback use — then document for each vendor whether they use your data for training and whether you have opted out. For any vendor that defaults to using your data for training, follow their opt-out process immediately and document it. Many vendors — including major platforms — default to using customer prompts and outputs to train future model versions; without an explicit opt-out, your confidential business context is becoming training data right now.',

  'vendor-b-4': 'Document the data retention period for each AI vendor in your inventory — how long they keep your data after sessions, after account termination, and after deletion requests — and flag any vendor whose retention window exceeds your own internal data lifecycle policy. Add data retention review as a required field in your vendor security questionnaire going forward. A vendor retaining your data for 12 months after you cancel your subscription is 12 months of additional breach exposure after you have stopped receiving any business value from the relationship.',

  'vendor-b-5': 'Add security certification review to your procurement checklist as a required step, and specify minimum acceptable certifications by tier: SOC 2 Type II or ISO 27001 for any vendor handling sensitive or regulated data, SOC 2 Type I as a minimum for all others. Request the most recent certification report — not just a badge — and note the report date. A vendor whose last SOC 2 audit was three years ago and whose certification has lapsed is not demonstrating the continuous security controls the certification is supposed to attest to.',

  'vendor-b-6': 'Build a vendor-data sensitivity map by adding a "data accessed" column to your AI vendor inventory and tagging each vendor with the data categories it touches: PII, financial records, health data, intellectual property, employee data, or public-only. Then rank vendors by sensitivity tier and confirm that your highest-sensitivity vendors have the strongest contractual protections and review cadences in place. Without this map, you are applying governance effort uniformly across high- and low-risk vendors, which means you are simultaneously over-governing low-risk tools and under-governing the ones that could trigger regulatory action.',

  'vendor-b-7': 'Identify your top three most operationally critical AI tools — the ones where a 24-hour outage would visibly impact your team\'s output — and for each one, document a contingency plan: what manual process substitutes, what alternative tool could step in, and what the recovery timeline would look like. Add a "continuity risk" field to your vendor risk register. Builder-stage organizations are actively integrating AI into workflows, which means they are also actively building single points of failure; discovering a critical dependency during an outage is far more expensive than mapping it in advance.',

  'vendor-b-8': 'Audit your current AI vendor portfolio and calculate what percentage of your AI spend and critical AI workflows run through a single vendor. If any one vendor accounts for more than 50% of either, document that as a concentration risk and begin a mitigation plan — whether that is identifying a secondary vendor, maintaining a manual fallback process, or negotiating contractual protections against service termination. When a dominant vendor changes pricing, restricts API access, or exits the market, organizations with no alternatives face operational disruption at exactly the moment when switching cost is highest.',

  'vendor-b-9': 'Request the current subprocessor list from each of your AI vendors that handles sensitive data — most vendors publish these in their privacy notices or data processing agreements — and review the list for any subprocessors operating in jurisdictions that conflict with your data residency requirements or customer contracts. Add subprocessor list review to your annual vendor review cycle. Your DPA with the AI vendor is only as protective as the weakest subprocessor in their chain; a breach at a subprocessor you did not know existed is still a breach of your data.',

  'vendor-b-10': 'Run your AI vendor list against the EU AI Act\'s high-risk use case categories — which include AI used in hiring, credit, education, law enforcement support, and critical infrastructure — and for any vendor falling into high-risk categories, request their conformity assessment documentation, CE marking status, and technical documentation. Do this now, before August 2026 enforcement begins. As a deployer of high-risk AI systems under the EU AI Act, you carry compliance obligations including maintaining use logs, conducting fundamental rights impact assessments, and providing human oversight — obligations that attach regardless of whether your vendor is compliant.',

  'data-b-1': 'Amend your existing data classification policy — or create a one-page addendum if none exists — to explicitly list what is prohibited from AI tools: customer PII, employee records, financial data, health information, legal communications, and trade secrets. Distribute this addendum to all employees with AI tool access and include it in your AI intake approval form so reviewers check it for every new tool request. Without an explicit, AI-specific prohibition list, employees make their own risk judgments, which tend to underestimate sensitivity.',

  'data-b-2': 'Draft a one-page AI data handling procedure covering four scenarios your general policy almost certainly does not address: what to do if you accidentally input sensitive data into an AI tool, how to store AI-generated outputs containing confidential information, when AI outputs can be shared externally, and how to handle AI tool session data. Circulate it to your five most active AI-using teams for a two-week pilot, then finalize and publish. Generic data policies were written before AI tools existed; the gaps they leave are exactly where incidents happen.',

  'data-b-3': 'Map your employee roles to AI tool access tiers: define which roles can use which tools with which data types, and translate that into your IT provisioning workflow so access is role-gated by default rather than open by default. Start with your most sensitive tools — those touching customer data, financial records, or employee information — and implement access restrictions there first. An HR administrator who can query employee salary data should not have unrestricted access to an AI tool that can generate, export, or summarize that data without oversight.',

  'data-b-4': 'Create a privacy review gate in your AI tool intake process: before any tool that processes PII is deployed, require a privacy impact assessment covering what personal data the tool accesses, the legal basis for processing, the vendor\'s data retention and deletion practices, and whether a DPA is in place. Use a one-page checklist rather than a lengthy PIA template to keep the review practical. Deploying a PII-processing AI tool without a privacy review creates undisclosed processing activities that violate GDPR\'s accountability principle and expose you to enforcement action independent of whether a breach ever occurs.',

  'data-b-5': 'Extend your data retention schedule to explicitly cover AI-generated content: define maximum retention periods for AI-generated documents, summaries, and analysis outputs stored in shared drives and collaboration tools, and configure automated deletion or archival rules in your primary platforms (SharePoint, Google Drive, Confluence) to enforce them. Run a one-time cleanup of AI-generated content older than your retention threshold. AI tools generate content at a rate that manual retention management cannot keep up with; without automated rules, your shared drives become an uncontrolled accumulation of potentially sensitive AI outputs.',

  'data-b-6': 'Add AI tool coverage to your existing data subject request process: create a lookup step that identifies which AI tools processed the requesting individual\'s data, and document for each vendor whether deletion requests can be fulfilled and what the SLA is. Test this process end-to-end with a synthetic request before you receive a real one. Fulfilling a GDPR erasure request or CCPA deletion request incorrectly — because you did not know an AI tool had processed the individual\'s data — is a compliance failure with the same consequences as refusing the request outright.',

  'data-b-7': 'Create a data flow diagram for your top five AI tools showing what data enters each tool, what the tool returns, where outputs are stored, and whether any of that data is shared with the vendor\'s subprocessors. Use a simple table format if a diagram tool is not available. Without this map, you cannot accurately describe your AI data processing activities to regulators, cannot identify which systems are in scope for a breach response, and cannot tell vendors what data to delete when you terminate a contract.',

  'data-b-8': 'Formalize your customer data restriction by adding it to your AI acceptable use policy with explicit language: "Customer data — including names, contact information, account details, and any personal information — must not be uploaded to external AI tools without written approval from [named role]." Add an approval request step to your intake form specifically for customer-data use cases. Customer data in an external AI tool is data that may be logged, retained, and used for model training without your customer\'s knowledge — a breach of trust and potentially of your customer contracts.',

  'data-b-9': 'For each AI vendor in your inventory, locate their privacy policy\'s subprocessor or third-party section and document the answer to two questions: who else receives your data, and where are those parties located geographically. Flag any third-party data sharing that conflicts with your customer-facing privacy notice or your contractual data residency commitments. If your privacy notice says you share data with "selected technology partners" but does not name the AI vendors and their subprocessors, you have an inaccurate privacy notice — which is itself a regulatory violation under GDPR Article 13.',

  'data-b-10': 'For each AI tool in your inventory, review the configuration options available and apply the principle of least privilege: connect only the data sources the tool needs to perform its stated function, disable access to all others, and document the access scope for each tool in your vendor registry. Do this during your next quarterly inventory review. AI tools frequently request broad data access by default because broad access increases their utility — but it also increases your breach surface and makes it harder to scope the impact of a security incident.',

  'security-b-1': 'Work with IT to create a 60-day plan to bring all AI tools under SSO: identify which tools support SAML or OIDC integration, configure SSO for those immediately, and for tools that do not support SSO, add them to a watch list for replacement at renewal. Simultaneously, audit which AI tool accounts are protected by MFA and require it for any that are not. AI tools accessed with personal credentials outside your identity provider are accounts you cannot disable during offboarding, cannot audit for unauthorized access, and cannot include in your access review cycle.',

  'security-b-2': 'Schedule a 2-hour workshop with your IT and operations leads to walk through your existing incident response plan and explicitly add three AI-specific scenarios: an employee sending sensitive data to an AI tool, an AI vendor notifying you of a data breach, and an AI output causing a significant business error. Document the response steps for each scenario in your IRP and assign owners. When an AI-related incident occurs — and it will — your team will default to existing procedures; if those procedures do not address AI scenarios, the response will be improvised under pressure.',

  'security-b-3': 'Pull your list of active AI tools and map each one against your compliance frameworks: for SOC 2, identify which trust service criteria are affected; for HIPAA, flag any tool that touches PHI; for PCI-DSS, identify any tool in the payment card data flow. Document the result in a simple matrix. AI tool adoption expands your compliance scope in ways that are invisible until an auditor asks — and discovering that an AI tool you have been using for a year brought you into PCI-DSS scope after the fact is far more disruptive than mapping it in advance.',

  'security-b-4': 'For every AI tool in your inventory that handles sensitive data, log into the admin console and confirm whether audit logging is available, then enable it if it is not already active and configure log export to your SIEM or a centralized logging tool. Set a monthly calendar reminder to review logs for anomalous access patterns. Without access logs, you cannot detect an employee exfiltrating data through an AI tool, cannot investigate a vendor\'s claim that they did not receive certain data, and cannot demonstrate due diligence to an auditor or regulator.',

  'security-b-5': 'Update your IT offboarding checklist to include a dedicated AI tool section listing every tool in your approved inventory, with a checkbox for confirming access revocation for each one. For tools outside SSO, create a manual step where IT contacts each vendor\'s admin panel to remove the departing employee. Test this by running through the checklist for your next departure before it becomes live. Former employees with active AI tool access can continue accessing company data, interacting with your AI integrations, and — in adversarial cases — exfiltrating information through tools that were never in your offboarding scope.',

  'security-b-6': 'Create a tiered security assessment requirement: any AI tool that processes regulated data (PII, health, financial, legal) requires a formal security assessment before deployment, covering encryption standards, access control architecture, breach notification commitment, and model behavior risk. Use a vendor security questionnaire as the structured input, and require a SOC 2 Type II report or equivalent third-party attestation. Deploying a sensitive-data AI tool without a security assessment means the first time you learn about its security weaknesses may be during a breach notification.',

  'security-b-7': 'Create a one-page "AI vendor minimum security requirements" document covering: encryption in transit and at rest, MFA requirement for admin access, SOC 2 or ISO 27001 certification, data processing agreement availability, and breach notification within 72 hours. Include this document as an attachment in all AI vendor procurement conversations and reference it in contract negotiation. Security requirements communicated only during sales calls and not written into contracts are unenforceable; putting them in the contract gives you recourse and signals to vendors that you take security seriously.',

  'security-b-8': 'Map your current AI tool portfolio against the EU AI Act\'s Annex III high-risk use case list — which covers AI in hiring, credit, education, biometric identification, law enforcement support, and critical infrastructure — and for each tool that falls into a high-risk category, document your obligations as a deployer: logging requirements, human oversight requirements, fundamental rights impact assessment, and registration in the EU database. Start this mapping now so you have six months before August 2026 enforcement to address gaps. Deployer obligations under the EU AI Act apply to you even if the AI system was built and supplied by a third-party vendor; "we bought it, we didn\'t build it" is not a defense.',

  'security-b-9': 'Conduct a CCPA compliance review of your AI tool usage by answering three questions for each tool: does it process personal information of California residents, does your privacy notice disclose it, and have you implemented the opt-out of sale or sharing where applicable. Engage your legal counsel or privacy officer to review AI-specific disclosures in your privacy notice and update them to include your AI data processing activities. CCPA violations carry statutory penalties of $100 to $750 per consumer per incident — and if you process data on thousands of California residents, an undisclosed AI tool creates significant aggregate liability.',

  'security-b-10': 'Formally assign AI compliance monitoring to a named individual — your privacy officer, legal counsel, IT security lead, or a designated AI governance lead — with a written scope of responsibility and a quarterly reporting obligation to leadership. Subscribe that person to the key regulatory monitoring sources: IAPP, Future of Privacy Forum, EU AI Office updates, and your relevant sector regulator. Without a designated monitor, regulatory changes arrive as surprises: you learn about a new obligation when a customer asks about it, a vendor mentions it, or an auditor cites it.',

  'risk-b-1': 'Write a one-page AI output validation guide tailored to your three most common AI use cases — for example, AI-drafted customer emails, AI-generated financial summaries, and AI-assisted contract review — specifying what to check in each case: factual claims, cited sources, numbers and calculations, legal language accuracy. Publish it in the same place employees access the AI tools. Employees know they should check AI outputs, but without specific guidance on what to check and how, "review" often means a quick read that misses the exact errors most likely to cause problems.',

  'risk-b-2': 'Run a 90-minute workshop with your department heads to inventory AI use cases and tag each one with a consequence level: Minor (internal draft with no external impact), Significant (customer-facing or financial), and Critical (legal, regulatory, clinical, or safety-relevant). For every Critical use case, document the specific consequences of an error and the controls currently in place. You cannot prioritize validation effort, assign human oversight, or justify governance investment without knowing which AI use cases carry material risk — and that knowledge must be documented, not just held in people\'s heads.',

  'risk-b-3': 'Draft a human-in-the-loop (HITL) policy that lists the specific decision types requiring human sign-off before an AI-generated recommendation is acted on — start with hiring decisions, legal document approval, financial commitments above a defined threshold, and any customer-facing AI-generated content. Assign a named reviewer role for each decision type and add HITL requirements to the relevant workflow documentation or process guides. An AI model that recommends a hiring decision, contract term, or financial action without a human review checkpoint is not an AI assistant — it is an autonomous decision-maker, with your organization bearing accountability for every outcome.',

  'risk-b-4': 'Create an AI error reporting channel today — a dedicated Slack channel, a shared inbox, or a simple Google Form — and announce it to all employees with a clear message: "If an AI tool gives you wrong, harmful, or unexpected output, report it here." Assign one person to review submissions weekly and identify patterns. Without a reporting channel, you have no organizational visibility into AI errors; each incident is a one-off that someone handles privately, and you never learn that the same tool is failing repeatedly across different teams.',

  'risk-b-5': 'Identify every AI tool your organization uses that influences decisions about people — hiring screening tools, performance monitoring, customer credit or service tier decisions, content moderation — and for each one, request the vendor\'s bias testing documentation or fairness assessment, then review it against the demographics most represented in the affected population. If the vendor cannot provide bias testing documentation, treat that as a red flag requiring escalation before continued use. Bias in AI hiring or credit tools is not an abstract ethical concern — it creates direct exposure under anti-discrimination law in the US, UK, and EU, and enforcement activity in this area is accelerating.',

  'risk-b-6': 'Identify the three to five AI-informed workflows in your organization that drive the most significant business decisions — for example, AI-assisted pricing, AI-generated customer proposals, AI-analyzed contract terms — and implement audit logging for those specific workflows: record what input was provided, what the AI recommended, who reviewed it, and what decision was made. Most AI tools provide this via API logs or conversation history export; start there before investing in custom logging. When a high-stakes AI-informed decision is challenged — by a customer, regulator, or in litigation — you need to be able to reconstruct exactly what the AI recommended and whether a human reviewed it.',

  'risk-b-7': 'Pick your two most critical AI tools and set up a quarterly performance spot-check: pull 20 representative inputs from recent usage, run them through the tool, and compare outputs against your quality baseline or a human expert review. Document the results and flag any meaningful degradation. AI model behavior can change without notice: vendors update models, change system prompts, and adjust safety filters — all of which can alter output quality in ways that are invisible without active testing. A tool that was performing well six months ago may not be performing well today.',

  'risk-b-8': 'Create an AI accountability matrix: list every significant AI-assisted workflow and decision type in your organization and assign a named accountable role for each one — the person responsible when an error causes harm, not just the person who approved the tool. Share this matrix with leadership and the relevant teams. Accountability defined after an incident is accountability that does not exist when it is needed; regulators and courts assign liability based on what was in place at the time of the incident, not what you put in place afterward.',

  'risk-b-9': 'Create a two-step gate for any AI tool expansion: first, a structured test covering at least 20 representative use cases in the new context; second, a sign-off from the team lead and IT before the tool is provisioned for new users or connected to new data sources. Document both steps. An AI coding assistant that works well for Python may behave poorly with your legacy COBOL codebase; an AI summarization tool trained on English text may perform unreliably on multilingual customer feedback. Testing is the only way to find out before the problem affects real work.',

  'risk-b-10': 'Write a one-page AI-generated content IP policy covering three questions: can AI-generated content be used in client deliverables (and under what disclosure conditions), what is your position on copyright ownership of AI-generated outputs, and how should employees handle AI tool claims about content being original. Have legal review it before publishing. Organizations using AI-generated content in products, marketing, or client deliverables without a clear IP policy are making implicit risk decisions that their legal team has not reviewed — and the legal landscape on AI copyright is actively shifting in ways that can change your exposure overnight.',

  'roi-b-1': 'For each significant AI tool investment, define three to five specific, measurable metrics before the next renewal date: for example, average time saved per task (measured in minutes, not "faster"), error rate before and after AI assistance, output volume per person per week, or cost per unit of output. Write these metrics into a one-page investment brief and share it with the team lead and finance. Without pre-defined metrics, AI ROI assessments are written after the fact to justify a decision already made — which means you learn nothing about what is actually working and cannot guide future investment accordingly.',

  'roi-b-2': 'Implement a lightweight time savings tracking method for your three most-used AI tools: a monthly 5-minute survey sent to active users asking for two numbers — estimated hours per week the tool saves, and confidence in that estimate. Run it for three months to establish a baseline, then use the data in your next AI investment review. Anecdotal time savings stories are compelling in a presentation but cannot survive budget scrutiny; survey-based estimates, while imperfect, give you a defensible number and reveal where adoption is shallow despite high license cost.',

  'roi-b-3': 'Pull AI-related spend from every budget owner this quarter — ask each department head to list all AI tool subscriptions charged to their budget, including tools purchased on corporate cards — and consolidate it into a single spreadsheet. Then share the consolidated view with your finance team and set up a tagged cost center or GL code for AI tool spending going forward. Fragmented AI spend across team budgets makes redundancy invisible: two departments may be paying for different tools that do the same thing, and nobody can see it until someone builds the consolidated view.',

  'roi-b-4': 'Pick one pair of overlapping AI tools your organization currently runs — the two most likely candidates for redundancy — and run a 30-day structured comparison: give the same representative tasks to both tools, score outputs on accuracy, speed, and relevance, and have the primary users rate each. Present the results to the tool owners with a recommendation. Many organizations accumulate AI tools through individual team decisions, ending up with three summarization tools, two writing assistants, and two code completion tools; a structured comparison is the only way to make a defensible rationalization decision.',

  'roi-b-5': 'Pull the license count and last-30-days active user data from your vendor admin console for each major AI tool this week — most enterprise AI tools provide this in their usage dashboard. Divide monthly cost by active users to get cost per active user, and flag any tool where fewer than 60% of licensed seats are active. For tools with low utilization, schedule a conversation with the team lead before the next renewal to understand whether adoption is a training gap, a tool fit issue, or a redundancy. Paying for AI licenses that are not being used is the most immediate and correctable source of poor AI ROI.',

  'roi-b-6': 'Export adoption data from your vendor admin consoles for your top three AI tools — most SaaS AI platforms report monthly active users in their admin dashboard — and calculate the adoption rate: active users divided by licensed seats. Set a minimum adoption threshold (70% is a reasonable starting point) and treat anything below it as requiring action before the next renewal. Low adoption is the most common and least discussed driver of poor AI ROI; a tool with 40% adoption is delivering half the potential value at full cost, and the problem is almost always fixable through training or workflow integration.',

  'roi-b-7': 'Build a one-page AI ROI summary template — covering total AI spend, active users, time savings estimate, adoption rate by tool, and top-performing use cases — and present it to your executive sponsor or leadership team at the next quarterly business review. Commit to a regular cadence, whether quarterly or bi-annual. Leadership that does not receive regular AI ROI data cannot make informed budget decisions: AI investments continue by inertia rather than by merit, and initiatives that are delivering real value cannot build the internal case for expansion.',

  'roi-b-8': 'Rank your current AI use cases by a simple value score: estimated time saved per week multiplied by the number of active users, then identify the top three. For each top performer, document what makes it successful — the workflow it fits into, the training employees received, the quality of outputs — so you can replicate those conditions when deploying AI into new use cases. AI value concentration is predictable: the use cases that work best tend to have clear task boundaries, measurable outputs, and strong user adoption. Understanding why your best use cases work lets you stop guessing and start engineering success.',

  'roi-b-9': 'Add a rework cost line to your AI ROI tracking: for each significant AI use case, ask the team lead to estimate the average percentage of AI outputs that require substantial correction, and multiply that by the time spent correcting them. Include this rework cost in your ROI calculation alongside gross time savings. A use case where AI saves 2 hours but generates 1.5 hours of rework is delivering a 0.5-hour net benefit, not a 2-hour benefit; without tracking rework you are overstating ROI by a factor that can make a net-negative use case look like a success.',

  'roi-b-10': 'For any AI capability your organization is considering building internally — custom models, fine-tuned tools, proprietary AI pipelines — run a structured build-vs-buy analysis before the decision is made: document the total cost of building (development time, ongoing maintenance, infrastructure, ML talent), the total cost of buying (licensing, integration, vendor risk), and the strategic value of control or differentiation. Present both options to leadership with a recommendation. Defaulting to vendor tools is common and often correct, but it should be a deliberate choice; organizations that drift into heavy vendor dependency without evaluating build options may face significant switching costs if vendor pricing or terms change.',
};
